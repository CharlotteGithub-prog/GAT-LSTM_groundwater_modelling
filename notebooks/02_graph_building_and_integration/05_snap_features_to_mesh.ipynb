{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config, deep_format, expanduser_tree\n",
    "from src.utils.config_loader import load_project_config\n",
    "from src.graph_building.graph_construction import build_mesh, \\\n",
    "    define_catchment_polygon, build_main_df\n",
    "from src.graph_building.data_merging import snap_stations_to_mesh, \\\n",
    "    merge_timeseries_data_to_df, load_gwl_data_for_merge, reorder_static_columns\n",
    "    # aggregate_resolution\n",
    "from src.visualisation.mapped_visualisations import plot_interactive_mesh_with_stations\n",
    "from src.preprocessing.hydroclimatic_feature_engineering import transform_aet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up root directory paths in config\n",
    "raw_data_root = config[\"global\"][\"paths\"][\"raw_data_root\"]\n",
    "results_root = config[\"global\"][\"paths\"][\"results_root\"]\n",
    "\n",
    "# Reformat config roots\n",
    "config = deep_format(\n",
    "    config,\n",
    "    raw_data_root=raw_data_root,\n",
    "    results_root=results_root\n",
    ")\n",
    "config = expanduser_tree(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "pred_frequency = config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Catchment area from country wide gdf\n",
    "define_catchment_polygon(\n",
    "    england_catchment_gdf_path=config[catchment]['paths']['gis_catchment_boundary'],\n",
    "    target_mncat=config[catchment]['target_mncat'],\n",
    "    catchment=catchment,\n",
    "    polygon_output_path=config[catchment]['paths']['gis_catchment_dir']\n",
    ")\n",
    "\n",
    "# Build catchment mesh\n",
    "mesh_nodes_table, mesh_nodes_gdf, mesh_cells_gdf_polygons, catchment_polygon = build_mesh(\n",
    "    shape_filepath=config[catchment]['paths']['gis_catchment_dir'],\n",
    "    output_path=config[catchment]['paths']['mesh_nodes_output'],\n",
    "    catchment=catchment,\n",
    "    grid_resolution=config[catchment]['preprocessing']['graph_construction']['grid_resolution']\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Build Mesh' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Load in centroid node csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path=config[catchment]['paths']['mesh_nodes_output']\n",
    "# grid_resolution=config[catchment]['preprocessing']['graph_construction']['grid_resolution']\n",
    "\n",
    "# mesh_input_path = input_path + '_' + str(grid_resolution) + '.csv'\n",
    "# mesh_nodes = pd.read_csv(mesh_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Merge station list with polygon geometry using spatial join to snap stations to mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_node_mapping = snap_stations_to_mesh(\n",
    "    station_list_path=config[catchment][\"paths\"][\"gwl_station_list_output\"],\n",
    "    polygon_geometry_path=config[catchment]['paths']['output_polygon_dir'],\n",
    "    output_path=config[catchment][\"paths\"][\"snapped_station_node_mapping\"],\n",
    "    mesh_nodes_gdf=mesh_nodes_gdf,\n",
    "    catchment=catchment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_node_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_map = plot_interactive_mesh_with_stations(\n",
    "    mesh_nodes_gdf=mesh_nodes_gdf,\n",
    "    catchment_polygon=catchment_polygon,\n",
    "    map_blue=config['global']['visualisations']['maps']['map_blue'],\n",
    "    esri=config['global']['visualisations']['maps']['esri'],\n",
    "    esri_attr=config['global']['visualisations']['maps']['esri_attr'],\n",
    "    static_output_path=config[catchment]['visualisations']['maps']['static_mesh_map_output'],\n",
    "    interactive_output_path=config[catchment]['visualisations']['maps']['interactive_station_map_output'],\n",
    "    catchment=catchment,\n",
    "    grid_resolution=config[catchment]['preprocessing']['graph_construction']['grid_resolution'],\n",
    "    interactive=config['global']['visualisations']['maps']['display_interactive_map'],\n",
    "    stations_gdf=station_node_mapping\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Interactive Mesh Mapping' complete for {catchment} catchment.\")\n",
    "\n",
    "# Display map in notebook\n",
    "mesh_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Create a main df for merging all features in to model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = build_main_df(\n",
    "    start_date=config[\"global\"][\"data_ingestion\"][\"model_start_date\"],\n",
    "    end_date=config[\"global\"][\"data_ingestion\"][\"model_end_date\"],\n",
    "    mesh_nodes_gdf=mesh_nodes_gdf,\n",
    "    catchment=catchment,\n",
    "    pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Merge all timeseries data into main df by station (from {station}_trimmed.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Merge in Timeseries Features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snap Precipitation, Lags and Averages to timestep\n",
    "\n",
    "merged_ts_precipitation = merge_timeseries_data_to_df(\n",
    "    model_start_date=config[\"global\"][\"data_ingestion\"][\"model_start_date\"],\n",
    "    model_end_date=config[\"global\"][\"data_ingestion\"][\"model_end_date\"],\n",
    "    feature_csv=config[catchment][\"paths\"][\"rainfall_csv_path\"],\n",
    "    csv_name=f'rainfall_{pred_frequency}_catchment_sum_log_transform.csv',\n",
    "    feature='all_precipitation',\n",
    "    pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Precipitation and derived data snapped to graph timesteps ({pred_frequency} aggregates).\\n\")\n",
    "\n",
    "# Snap 2m Temperature to timestep\n",
    "\n",
    "merged_ts_tsm = merge_timeseries_data_to_df(\n",
    "    model_start_date=config[\"global\"][\"data_ingestion\"][\"model_start_date\"],\n",
    "    model_end_date=config[\"global\"][\"data_ingestion\"][\"model_end_date\"],\n",
    "    feature_csv=config[catchment][\"paths\"][\"2t_csv_path\"],\n",
    "    csv_name=f'2m_temp_{pred_frequency}_catchment_mean.csv',\n",
    "    feature='2m_temperature',\n",
    "    pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"],\n",
    "    timeseries_df=merged_ts_precipitation\n",
    ")\n",
    "\n",
    "logger.info(f\"2m Temperature Data snapped to graph timesteps ({pred_frequency} aggregate).\\n\")\n",
    "\n",
    "# Snap AET to timestep\n",
    "\n",
    "merged_ts_aet = merge_timeseries_data_to_df(\n",
    "    model_start_date=config[\"global\"][\"data_ingestion\"][\"model_start_date\"],\n",
    "    model_end_date=config[\"global\"][\"data_ingestion\"][\"model_end_date\"],\n",
    "    feature_csv=config[catchment][\"paths\"][\"aet_csv_path\"],\n",
    "    csv_name=f'aet_{pred_frequency}_catchment_sum.csv',\n",
    "    feature='aet',\n",
    "    pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"],\n",
    "    timeseries_df=merged_ts_tsm\n",
    ")\n",
    "        \n",
    "merged_ts_aet = transform_aet_data(merged_ts_aet, catchment)\n",
    "\n",
    "logger.info(f\"Actual evapotranspiration data snapped to graph timesteps ({pred_frequency} aggregate).\\n\")\n",
    "\n",
    "        # Snap Surface Pressure to timestep\n",
    "\n",
    "merged_ts_sp = merge_timeseries_data_to_df(\n",
    "    model_start_date=config[\"global\"][\"data_ingestion\"][\"model_start_date\"],\n",
    "    model_end_date=config[\"global\"][\"data_ingestion\"][\"model_end_date\"],\n",
    "    feature_csv=config[catchment][\"paths\"][\"sp_csv_path\"],\n",
    "    csv_name=f'surface_pressure_{pred_frequency}_catchment_mean.csv',\n",
    "    feature='surface_pressure',\n",
    "    pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"],\n",
    "    timeseries_df=merged_ts_aet\n",
    ")\n",
    "\n",
    "logger.info(f\"Surface pressure data snapped to graph timesteps ({pred_frequency} aggregate).\\n\")\n",
    "\n",
    "# Snap Streamflow to timestep\n",
    "\n",
    "final_merged_ts_df = merge_timeseries_data_to_df(\n",
    "    model_start_date=config[\"global\"][\"data_ingestion\"][\"model_start_date\"],\n",
    "    model_end_date=config[\"global\"][\"data_ingestion\"][\"model_end_date\"],\n",
    "    feature_csv=config[catchment][\"paths\"][\"stream_flow_csv\"],\n",
    "    csv_name=f'{pred_frequency}_streamflow.csv',\n",
    "    feature='streamflow',\n",
    "    pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"],\n",
    "    timeseries_df=merged_ts_sp\n",
    ")\n",
    "\n",
    "logger.info(f\"Streamflow data snapped to graph timesteps ({pred_frequency} aggregate).\\n\")\n",
    "\n",
    "save_path = config[catchment][\"paths\"][\"final_df_path\"] + 'final_timeseries_df.csv'\n",
    "final_merged_ts_df.to_csv(save_path)\n",
    "\n",
    "logger.info(f\"Final merged time series dataframe saved to {save_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Merge in Static Features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snap Land Cover to Mesh\n",
    "\n",
    "agg_land_cover_df = pd.read_csv(config[catchment][\"paths\"][\"land_cover_csv_path\"])\n",
    "merged_gdf_nodes_landuse = mesh_nodes_gdf.merge(\n",
    "    agg_land_cover_df[['easting', 'northing', 'land_cover_code']],\n",
    "    on=['easting', 'northing'],\n",
    "    how='left'  # left join to keep all centroids, even NaN\n",
    ")\n",
    "\n",
    "logger.info(f\"Land cover data snapped to mesh nodes (centroids).\\n\")\n",
    "\n",
    "# Snap Elevation to Mesh\n",
    "\n",
    "elevation_gdf_polygon = gpd.read_file(config[catchment][\"paths\"][\"elevation_geojson_path\"])\n",
    "merged_gdf_nodes_elevation = merged_gdf_nodes_landuse.merge(\n",
    "    elevation_gdf_polygon[['node_id', 'mean_elevation', 'geometry']],\n",
    "    on='node_id',\n",
    "    how='left'  # left join to keep all centroids, even NaN\n",
    ")\n",
    "\n",
    "logger.info(f\"Elevation data snapped to mesh nodes (centroids).\\n\")\n",
    "\n",
    "# Snap Geology Maps to Mesh\n",
    "\n",
    "geology_path = os.path.join(config[catchment][\"paths\"][\"geology_df\"], 'geology_df.csv')\n",
    "mesh_geology_df = pd.read_csv(geology_path)\n",
    "merged_gdf_nodes_geology = merged_gdf_nodes_elevation.merge(\n",
    "    mesh_geology_df[['geo_bedrock_type', 'geo_superficial_type', 'bedrock_flow_type',\n",
    "                        'bedrock_perm_avg', 'superficial_flow_type', 'superficial_perm_avg',\n",
    "                        'node_id']],\n",
    "    on='node_id',\n",
    "    how='left'  # left join to keep all centroids, even if NaN\n",
    ")\n",
    "\n",
    "logger.info(f\"Geology data snapped to mesh nodes (centroids).\\n\")\n",
    "\n",
    "# Snap Slope to Mesh\n",
    "\n",
    "slope_gdf = pd.read_csv(config[catchment][\"paths\"][\"slope_path\"])\n",
    "merged_gdf_nodes_slope = merged_gdf_nodes_geology.merge(\n",
    "    slope_gdf[['node_id', 'mean_slope_degrees', 'mean_aspect_sin', 'mean_aspect_cos']],\n",
    "    on='node_id',\n",
    "    how='left'  # left join to keep all centroids, even if NaN\n",
    ")\n",
    "\n",
    "logger.info(f\"Slope degrees and sinusoidal aspect data snapped to mesh nodes (centroids).\\n\")\n",
    "\n",
    "# Snap Soil Hydrology to Mesh\n",
    "\n",
    "soil_hydrology_path = os.path.join(config[catchment][\"paths\"][\"soil_csv_path\"], 'soil_hydrology.csv')\n",
    "soil_hydrology_df = pd.read_csv(soil_hydrology_path)\n",
    "merged_gdf_nodes_soil = merged_gdf_nodes_slope.merge(\n",
    "    soil_hydrology_df[['node_id', 'HOST_soil_class']],\n",
    "    on='node_id',\n",
    "    how='left'  # left join to keep all centroids, even if NaN\n",
    ")\n",
    "\n",
    "logger.info(f\"Soil Hydrology data snapped to mesh nodes (centroids).\\n\")\n",
    "\n",
    "# Snap Aquifer Productivity to Mesh\n",
    "\n",
    "productivity_path = os.path.join(config[catchment][\"paths\"][\"productivity_csv_path\"], 'productivity_data.csv')\n",
    "productivity_gdf = pd.read_csv(productivity_path)\n",
    "merged_gdf_nodes_productivity = merged_gdf_nodes_soil.merge(\n",
    "    productivity_gdf[['node_id', 'aquifer_productivity']],\n",
    "    on='node_id',\n",
    "    how='left'  # left join to keep all centroids, even if NaN\n",
    ")\n",
    "\n",
    "logger.info(f\"Aquifer Productivity data snapped to mesh nodes (centroids).\\n\")\n",
    "\n",
    "# Snap Distance from River to Mesh\n",
    "\n",
    "dist_to_river_path = os.path.join(config[catchment][\"paths\"][\"rivers_csv_path\"], 'distance_to_river.csv')\n",
    "dist_to_river_gdf = pd.read_csv(dist_to_river_path)\n",
    "static_features = merged_gdf_nodes_productivity.merge(\n",
    "    dist_to_river_gdf[['node_id', 'distance_to_river']],\n",
    "    on='node_id',\n",
    "    how='left'  # left join to keep all centroids, even if NaN\n",
    ")\n",
    "\n",
    "logger.info(f\"Distance from river data snapped to mesh nodes (centroids).\\n\")\n",
    "\n",
    "# [FUTURE] Snap Depth to Groundwater to Mesh - Awaiting Licensing\n",
    "# [FUTURE] Snap Infiltration Rate to Mesh\n",
    "# [FUTURE] Snap Soil Type Maps to Mesh\n",
    "\n",
    "# Finalise final_static_df for merge\n",
    "\n",
    "final_static_df = reorder_static_columns(static_features)  # TODO: Update as more features added\n",
    "static_data_ingestion.save_final_static_data(\n",
    "    static_features=final_static_df,\n",
    "    dir_path=config[catchment][\"paths\"][\"final_df_path\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Full static feature dataframe finalised and ready to merge into main model dataframe.\\n\")\n",
    "\n",
    "static_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Final Merge into main_df ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge static data into main_df\n",
    "static_df = pd.read_csv(os.path.join(config[catchment][\"paths\"][\"final_df_path\"], 'final_static_df.csv'))\n",
    "main_df_static = main_df.merge(\n",
    "    static_df,\n",
    "    left_on='node_id',\n",
    "    right_on='node_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "logger.info(f\"Static data successfully merged into main_df for {catchment} catchment.\\n\")\n",
    "\n",
    "# Merge timeseries data into main_df\n",
    "timeseries_df = pd.read_csv(os.path.join(config[catchment][\"paths\"][\"final_df_path\"], 'final_timeseries_df.csv'))\n",
    "timeseries_df['time'] = pd.to_datetime(timeseries_df['time'])\n",
    "main_df_timeseries = main_df_static.merge(\n",
    "    timeseries_df,\n",
    "    left_on='timestep',\n",
    "    right_on='time',\n",
    "    how='left'\n",
    ").drop(columns='time')\n",
    "\n",
    "logger.info(f\"Timeseries data successfully merged into main_df for {catchment} catchment.\\n\")\n",
    "    \n",
    "# Load GWL station data and merge data into main_df\n",
    "station_dir = config[catchment][\"paths\"][\"trimmed_output_dir\"]\n",
    "node_mapping_dir = config[catchment][\"paths\"][\"snapped_station_node_mapping\"]\n",
    "gwl_data = load_gwl_data_for_merge(station_dir, node_mapping_dir)\n",
    "\n",
    "seasonal_df = gwl_data[['timestep', 'season_sin', 'season_cos']].drop_duplicates('timestep')\n",
    "main_df_full = (\n",
    "    main_df_timeseries\n",
    "    .merge(seasonal_df, on='timestep', how='left')\n",
    "    .merge(gwl_data.drop(columns=['season_sin', 'season_cos']), on=['node_id', 'timestep'], how='left')\n",
    ")\n",
    "\n",
    "logger.info(f\"Groundwater Level data successfully merged into main_df for {catchment} catchment.\\n\")\n",
    "\n",
    "# Save final dataframe to file - NB: TIME TO SAVE APPROX. 3.5 MINS (total block 4 mins)\n",
    "final_save_path = config[catchment][\"paths\"][\"final_df_path\"] + 'final_df.csv'\n",
    "main_df_full.to_csv(final_save_path)\n",
    "logger.info(f\"Final merged dataframe saved to {final_save_path}\")\n",
    "\n",
    "# Display in notebook\n",
    "main_df_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ncc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
