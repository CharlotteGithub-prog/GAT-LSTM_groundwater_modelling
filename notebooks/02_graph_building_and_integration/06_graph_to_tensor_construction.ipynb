{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config, deep_format, expanduser_tree\n",
    "from src.graph_building.graph_construction import build_mesh, \\\n",
    "    define_catchment_polygon, define_graph_adjacency\n",
    "from src.preprocessing.data_partitioning import define_station_id_splits, \\\n",
    "    load_graph_tensors, build_pyg_object\n",
    "from src.preprocessing.model_feature_engineering import preprocess_gwl_features, \\\n",
    "    preprocess_shared_features\n",
    "from src.utils.run_manifest import save_run_manifest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up root directory paths in config\n",
    "raw_data_root = config[\"global\"][\"paths\"][\"raw_data_root\"]\n",
    "results_root = config[\"global\"][\"paths\"][\"results_root\"]\n",
    "\n",
    "# Reformat config roots\n",
    "config = deep_format(\n",
    "    config,\n",
    "    raw_data_root=raw_data_root,\n",
    "    results_root=results_root\n",
    ")\n",
    "config = expanduser_tree(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Catchment area from country wide gdf\n",
    "define_catchment_polygon(\n",
    "    england_catchment_gdf_path=config[catchment]['paths']['gis_catchment_boundary'],\n",
    "    target_mncat=config[catchment]['target_mncat'],\n",
    "    catchment=catchment,\n",
    "    polygon_output_path=config[catchment]['paths']['gis_catchment_dir']\n",
    ")\n",
    "\n",
    "# Build catchment mesh\n",
    "mesh_nodes_table, mesh_nodes_gdf, mesh_cells_gdf_polygons, catchment_polygon = build_mesh(\n",
    "    shape_filepath=config[catchment]['paths']['gis_catchment_dir'],\n",
    "    output_path=config[catchment]['paths']['mesh_nodes_output'],\n",
    "    catchment=catchment,\n",
    "    grid_resolution=config[catchment]['preprocessing']['graph_construction']['grid_resolution']\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Build Mesh' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_edge_path = config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "# Create specific node_id column to merge\n",
    "directional_edge_weights[\"node_id\"] = range(0, len(directional_edge_weights))\n",
    "directional_edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_cells_gdf_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Split into Train/Val/Test Split ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tensors from file if needed\n",
    "# edge_index_tensor, edge_attr_tensor = load_graph_tensors(\n",
    "#     graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "#     catchment=catchment\n",
    "# )\n",
    "\n",
    "# Load main_df_full from file if needed\n",
    "load_path = config[catchment][\"paths\"][\"final_df_path\"] + 'final_df.csv'\n",
    "main_df_full = pd.read_csv(load_path)\n",
    "main_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6a. Define Spatial Split for Observed Stations ---\n",
    "\n",
    "train_station_ids, val_station_ids, test_station_ids = define_station_id_splits(\n",
    "    main_df_full=main_df_full,\n",
    "    catchment=catchment,\n",
    "    test_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"test_station_shortlist\"],\n",
    "    val_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"val_station_shortlist\"],\n",
    "    random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "    output_dir=config[catchment][\"paths\"][\"aux_dir\"],\n",
    "    perc_train=config[catchment][\"model\"][\"data_partioning\"][\"percentage_train\"],\n",
    "    perc_val=config[catchment][\"model\"][\"data_partioning\"][\"percentage_val\"],\n",
    "    perc_test=config[catchment][\"model\"][\"data_partioning\"][\"percentage_test\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'define station splits' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### BUILD EDGE WEIGHTS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in directional edge weights and mean elevation (not req. in main pipeline)\n",
    "directional_edge_path=config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "station_node_ids = np.array(sorted(\n",
    "    set(train_station_ids) | set(val_station_ids) | set(test_station_ids)\n",
    "), dtype=int)\n",
    "\n",
    "edge_attr_tensor, edge_index_tensor = define_graph_adjacency(\n",
    "    directional_edge_weights=directional_edge_weights,\n",
    "    elevation_geojson_path=config[catchment]['paths']['elevation_geojson_path'],\n",
    "    graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "    mesh_cells_gdf_polygons=mesh_cells_gdf_polygons,\n",
    "    epsilon_path=config[\"global\"][\"graph\"][\"epsilon\"],\n",
    "    station_node_ids=station_node_ids,\n",
    "    station_radius_m=config[\"global\"][\"graph\"][\"graph_construction\"][\"station_radius_m\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Define Graph Adjacency' complete for {catchment} catchment.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Preprocess Shared Features Prior to Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6b. Preprocess (Standardise, one hot encode, round to 4dp) all shared features (not GWL) ---\n",
    "\n",
    "processed_df, shared_scaler, shared_encoder, gwl_feats = preprocess_shared_features(\n",
    "    main_df_full=main_df_full,\n",
    "    catchment=catchment,\n",
    "    random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "    violin_plt_path=config[catchment][\"visualisations\"][\"violin_plt_path\"],\n",
    "    scaler_dir = config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    aux_dir=config[catchment][\"paths\"][\"aux_dir\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Preprocess Final Shared Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Split processed_df into train/val/test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6c. Preprocess all GWL features using training data only ---\n",
    "\n",
    "processed_df, gwl_scaler, gwl_encoder = preprocess_gwl_features(\n",
    "    processed_df=processed_df,\n",
    "    catchment=catchment,\n",
    "    train_station_ids=train_station_ids,\n",
    "    val_station_ids=val_station_ids,\n",
    "    test_station_ids=test_station_ids,\n",
    "    sentinel_value=config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "    scaler_dir=config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    parquet_path=os.path.join(config[catchment][\"paths\"][\"final_df_path\"], 'processed_df.parquet')\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Preprocess Final GWL Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in processed_df after preprocessing:\", processed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shorten df to test range to reduce computation requirements\n",
    "\n",
    "# test_start_date_str = config[\"global\"][\"data_ingestion\"][\"test_start_date\"]\n",
    "# test_end_date_str = config[\"global\"][\"data_ingestion\"][\"test_end_date\"]\n",
    "\n",
    "# # Convert 'timestep' column to datetime objects\n",
    "# processed_df['timestep'] = pd.to_datetime(processed_df['timestep'])\n",
    "\n",
    "# processed_df_test = processed_df.loc[\n",
    "#     (processed_df['timestep'] >= test_start_date_str) &\n",
    "#     (processed_df['timestep'] <= test_end_date_str)\n",
    "# ].copy()\n",
    "\n",
    "# print(f\"Original processed_df shape (full data): {processed_df.shape}\")\n",
    "# print(f\"Test processed_df_test shape (sliced from {test_start_date_str} to {test_end_date_str}): {processed_df_test.shape}\")\n",
    "# processed_df_test = processed_df.drop(columns=['streamflow_total_m3', 'HOST_soil_class_freely_draining_soils', 'HOST_soil_class_high_runoff_(impermeable)', \n",
    "#                                                'HOST_soil_class_impeded_saturated_subsurface_flow', 'HOST_soil_class_peat_soils', 'aquifer_productivity_High',\n",
    "#                                                'aquifer_productivity_Low', 'aquifer_productivity_Mixed', 'aquifer_productivity_Moderate',\n",
    "#                                                'aquifer_productivity_nan']).copy()\n",
    "# processed_df_test = processed_df.copy()\n",
    "\n",
    "# TESTING WITH GWL LAGS (AR Inputs) TO UNDERSTAND PERFORMANCE\n",
    "processed_df_test = processed_df.drop(columns=['gwl_lag1', 'gwl_lag2', 'gwl_lag3', 'gwl_lag4', 'gwl_lag5', 'gwl_lag6', 'gwl_lag7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display full (test) processed df\n",
    "processed_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_df_test.columns[54:61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_df_test.columns[62:63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Assign validation stations using geographic proximity with buffering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to final preprocessed static columns\n",
    "column_indices = list(range(0, 9)) + list(range(25, 54)) + list(range(61, 63))\n",
    "split_df = processed_df_test.iloc[:, column_indices]\n",
    "\n",
    "# Aggregate to node_id\n",
    "aggregated_df = split_df.groupby('node_id').first().reset_index()\n",
    "\n",
    "# Get station data\n",
    "station_nodes = [430, 902, 1254, 1326, 1335, 1420, 1556, 1648, 1772, 1858, 1983, 2388, 2487, 2594]\n",
    "station_dfs = aggregated_df[aggregated_df['node_id'].isin(station_nodes)]\n",
    "station_dfs = station_dfs.drop(columns='timestep')\n",
    "\n",
    "# Load reference df\n",
    "station_metadata = pd.read_csv(\"data/02_processed/eden/gwl_station_data/snapped_station_node_mapping.csv\")\n",
    "\n",
    "# Merge in required data\n",
    "merged_station_df = station_dfs.merge(\n",
    "    station_metadata[['node_id', 'station_name', 'easting', 'northing', 'geometry']],\n",
    "    on='node_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean station_name column and drop unneeded stations\n",
    "merged_station_df['station_name'] = merged_station_df['station_name'].astype(str).str.lower().str.replace(\" \", \"_\")\n",
    "rows_to_drop = merged_station_df[merged_station_df['station_name'] == 'cliburn_town_bridge_1'].index\n",
    "merged_station_df.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "merged_station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "DF = merged_station_df.copy() \n",
    "BUFFER_KM = 5.0  # km\n",
    "USE_BUFFER = True\n",
    "RIDGE = 1e-3\n",
    "\n",
    "# Columns not used as environmental features\n",
    "EXCLUDE = {'node_id', 'station_name', 'easting', 'northing', 'geometry'}\n",
    "\n",
    "# Get pairwise geographic distances\n",
    "def pairwise_geo_km(xy_m):\n",
    "    d = xy_m[:, None, :] - xy_m[None, :, :]\n",
    "    return np.hypot(d[...,0], d[...,1]) / 1000.0\n",
    "\n",
    "# Calc per-fold whitening (Mahalanobis)\n",
    "def whiten_per_fold(X, train_idx, ridge=1e-3):\n",
    "    \"\"\"\n",
    "    Returns X_whitened for all rows, using mean/cov estimated on train_idx only.\n",
    "    \"\"\"\n",
    "    mu = X[train_idx].mean(axis=0)\n",
    "    Xc = X - mu\n",
    "    Xt = X[train_idx] - mu\n",
    "    # covariance on training stations\n",
    "    C  = np.cov(Xt, rowvar=False)\n",
    "    # ridge on diagonal\n",
    "    C.flat[::C.shape[0]+1] += ridge\n",
    "    # Σ^{-1/2}\n",
    "    U,S,_ = np.linalg.svd(C, full_matrices=False)\n",
    "    W = U @ np.diag(1.0/np.sqrt(S)) @ U.T\n",
    "    return Xc @ W\n",
    "\n",
    "# prepare merged_station_df matrices\n",
    "ids = DF['node_id'].to_numpy()\n",
    "XY  = DF[['easting','northing']].to_numpy(float)  # metres (BNG)\n",
    "feat_cols = [c for c in DF.columns if c not in EXCLUDE]\n",
    "X = DF[feat_cols].to_numpy(float)\n",
    "\n",
    "D_geo = pairwise_geo_km(XY)\n",
    "\n",
    "# proximity vs environmental similarity diagnostic\n",
    "def proximity_env_report(X, D_geo, use_whitening=True, ridge=1e-3):\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # global whitening for diagnostic only (not selection)\n",
    "    if use_whitening:\n",
    "        Xc = X - X.mean(0)\n",
    "        C  = np.cov(Xc, rowvar=False)\n",
    "        C.flat[::C.shape[0]+1] += ridge\n",
    "        U,S,_ = np.linalg.svd(C, full_matrices=False)\n",
    "        W = U @ np.diag(1.0/np.sqrt(S)) @ U.T\n",
    "        Xw = Xc @ W\n",
    "    else:\n",
    "        Xw = X\n",
    "\n",
    "    diff = Xw[:,None,:] - Xw[None,:,:]\n",
    "    D_env = np.sqrt((diff**2).sum(-1))\n",
    "\n",
    "    # upper triangle\n",
    "    mask = np.triu(np.ones_like(D_env, dtype=bool), 1)\n",
    "    rho, p_spear = spearmanr(D_env[mask], D_geo[mask])\n",
    "\n",
    "    # quick Mantel permutation test\n",
    "    def mantel(A, B, perms=20000, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        a = A[mask]; b = B[mask]\n",
    "        obs = np.corrcoef(a, b)[0,1]\n",
    "        cnt = 0\n",
    "        for _ in range(perms):\n",
    "            p = rng.permutation(n)\n",
    "            bb = B[np.ix_(p, p)][mask]\n",
    "            if np.corrcoef(a, bb)[0,1] >= obs:\n",
    "                cnt += 1\n",
    "        p_val = (cnt + 1) / (perms + 1)\n",
    "        return float(obs), float(p_val)\n",
    "\n",
    "    r_mantel, p_mantel = mantel(D_env, D_geo, perms=20000, seed=42)\n",
    "\n",
    "    print(f\"Spearman ρ(env, geo) = {rho:.3f}  (p={p_spear:.3f})\")\n",
    "    print(f\"Mantel r = {r_mantel:.3f}  (p={p_mantel:.3f})\")\n",
    "    return D_env\n",
    "\n",
    "# Run diagnostic report\n",
    "D_env_diag = proximity_env_report(X, D_geo, use_whitening=True)\n",
    "\n",
    "# Get assignments: two val stations by environmental similarity\n",
    "def assign_two_validations_env_only(ids, X, XY, buffer_km=5.0, use_buffer=True, ridge=1e-3):\n",
    "    \"\"\"\n",
    "    For each test station i, choose two validation stations j by:\n",
    "      - computing Mahalanobis distances in static space with params\n",
    "      - ranking by environmental dist only\n",
    "      - discarding candidates within buffer dist of test station.\n",
    "    \"\"\"\n",
    "    n = len(ids)\n",
    "    D_geo = pairwise_geo_km(XY)\n",
    "    rows = []\n",
    "\n",
    "    for i in range(n):\n",
    "        train_idx = np.arange(n) != i\n",
    "        Xw = whiten_per_fold(X, train_idx, ridge=ridge)\n",
    "\n",
    "        # environmental distances from test i to everyone\n",
    "        d_env = np.linalg.norm(Xw - Xw[i], axis=1)\n",
    "        d_env[i] = np.inf\n",
    "\n",
    "        # candidate mask\n",
    "        if use_buffer and buffer_km > 0:\n",
    "            cand = (D_geo[i] >= buffer_km) & (np.arange(n) != i)\n",
    "            # relax if too few\n",
    "            if cand.sum() < 2:\n",
    "                cand = (D_geo[i] >= max(3.0, 0.6*buffer_km)) & (np.arange(n) != i)\n",
    "        else:\n",
    "            cand = (np.arange(n) != i)\n",
    "\n",
    "        idxs = np.where(cand)[0]\n",
    "        order = idxs[np.argsort(d_env[idxs])]\n",
    "\n",
    "        v1 = order[0]\n",
    "        v2 = order[1] if len(order) > 1 else None\n",
    "\n",
    "        rows.append({\n",
    "            \"test_node\": ids[i],\n",
    "            \"val1_node\": ids[v1],\n",
    "            \"val2_node\": (ids[v2] if v2 is not None else np.nan),\n",
    "            \"env_d_val1\": float(d_env[v1]),\n",
    "            \"env_d_val2\": (float(d_env[v2]) if v2 is not None else np.nan),\n",
    "            \"geo_km_val1\": float(D_geo[i, v1]),\n",
    "            \"geo_km_val2\": (float(D_geo[i, v2]) if v2 is not None else np.nan)\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "assignments = assign_two_validations_env_only(\n",
    "    ids=ids, X=X, XY=XY,\n",
    "    buffer_km=BUFFER_KM, use_buffer=USE_BUFFER, ridge=RIDGE\n",
    ")\n",
    "\n",
    "assignments.sort_values(\"test_node\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# 2-D MDS on the precomputed Mahalanobis (environmental) distances\n",
    "mds_model = MDS(\n",
    "    n_components=2,\n",
    "    dissimilarity=\"precomputed\",\n",
    "    random_state=42,\n",
    "    n_init=8,\n",
    "    max_iter=3000,\n",
    "    normalized_stress=\"auto\",\n",
    ")\n",
    "mds = mds_model.fit_transform(D_env_diag)   # <— this defines `mds`\n",
    "print(\"MDS stress:\", mds_model.stress_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import OAS, LedoitWolf\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# ---------- distances in environmental space ----------\n",
    "\n",
    "def whiten_global(X):\n",
    "    Xc = X - X.mean(0)\n",
    "    C = LedoitWolf().fit(Xc).covariance_\n",
    "    U,S,_ = np.linalg.svd(C, full_matrices=False)\n",
    "    W = U @ np.diag(1/np.sqrt(S)) @ U.T\n",
    "    return Xc @ W\n",
    "\n",
    "def whiten_global_shrink(X, method=\"oas\", center=True, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Global whitening so Euclidean ≡ Mahalanobis under a shrinkage covariance.\n",
    "    Returns X_w (whitened), mean vector mu, and whitening matrix W (Σ^{-1/2}).\n",
    "    \"\"\"\n",
    "    Xc = X - X.mean(0) if center else X.copy()\n",
    "    if method == \"oas\":\n",
    "        est = OAS().fit(Xc)\n",
    "    elif method == \"lw\":\n",
    "        est = LedoitWolf().fit(Xc)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'oas' or 'lw'\")\n",
    "    C = est.covariance_\n",
    "    # symmetric eigendecomposition for Σ^{-1/2}\n",
    "    eigvals, eigvecs = np.linalg.eigh(C)\n",
    "    eigvals = np.clip(eigvals, eps, None)\n",
    "    W = eigvecs @ np.diag(1.0 / np.sqrt(eigvals)) @ eigvecs.T\n",
    "    Xw = Xc @ W\n",
    "    return Xw, Xc.mean(0), W\n",
    "\n",
    "def reduce_dimensionality(Xw, var_keep=0.95, min_dim=3, max_dim=12):\n",
    "    \"\"\"\n",
    "    PCA on whitened features; keeps enough components to explain var_keep.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=min(max_dim, Xw.shape[1]), svd_solver=\"full\")\n",
    "    pca.fit(Xw)\n",
    "    cum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    r = np.searchsorted(cum, var_keep) + 1\n",
    "    r = max(min_dim, min(r, max_dim))\n",
    "    Z = pca.transform(Xw)[:, :r]\n",
    "    return Z, r, pca\n",
    "\n",
    "# Build environmental distance matrix\n",
    "# Xw, mu_w, W = whiten_global_shrink(X, method=\"oas\")       # stable Σ^{-1/2}\n",
    "Xw = whiten_global(X)\n",
    "Z, r, pca = reduce_dimensionality(Xw, var_keep=0.95)      # contrast restored\n",
    "D_env = cdist(Z, Z, metric=\"euclidean\")                   # ≡ Mahalanobis on reduced space\n",
    "\n",
    "# ---------- optional geographic matrix ----------\n",
    "\n",
    "def pairwise_geo_km(xy_m):\n",
    "    d = xy_m[:, None, :] - xy_m[None, :, :]\n",
    "    return np.hypot(d[...,0], d[...,1]) / 1000.0\n",
    "\n",
    "D_geo = pairwise_geo_km(XY)  # as before\n",
    "\n",
    "# ---------- k-center with buffer and graceful relaxation ----------\n",
    "\n",
    "def kcenter_greedy(D_env, k, D_geo=None, min_geo_km=0.0, relax_to=0.0):\n",
    "    \"\"\"\n",
    "    Farthest-first traversal (k-center). Enforces a geo buffer to ALL chosen\n",
    "    centers; if infeasible, relaxes the buffer multiplicatively to 'relax_to'.\n",
    "    Returns: centers list, per-point coverage (dist to nearest center),\n",
    "    R_max and R_95.\n",
    "    \"\"\"\n",
    "    n = D_env.shape[0]\n",
    "    centers = [int(np.argmax(D_env.mean(1)))]  # farthest from global mean\n",
    "\n",
    "    while len(centers) < k:\n",
    "        d_to_S = D_env[:, centers].min(1)\n",
    "        ok = np.ones(n, dtype=bool)\n",
    "        ok[centers] = False\n",
    "\n",
    "        if D_geo is not None and min_geo_km > 0:\n",
    "            buf = float(min_geo_km)\n",
    "            while True:\n",
    "                ok = np.ones(n, dtype=bool); ok[centers] = False\n",
    "                for c in centers:\n",
    "                    ok &= (D_geo[:, c] >= buf)\n",
    "                if ok.any() or buf <= relax_to:\n",
    "                    break\n",
    "                buf = max(relax_to, 0.8 * buf)  # relax by 20%\n",
    "\n",
    "        cand = np.argmax(np.where(ok, d_to_S, -np.inf))\n",
    "        centers.append(int(cand))\n",
    "\n",
    "    cover = D_env[:, centers].min(1)\n",
    "    R_max = float(cover.max())\n",
    "    R_95  = float(np.quantile(cover, 0.95))\n",
    "    return centers, cover, R_max, R_95\n",
    "\n",
    "# choose k and compute coverage\n",
    "k = 3\n",
    "centers, cover, R_max, R_95 = kcenter_greedy(D_env, k, D_geo=D_geo, min_geo_km=5.0, relax_to=2.0)\n",
    "labels = np.argmin(D_env[:, centers], axis=1)  # assignments\n",
    "medoid_idx = centers                            # prototypes are the medoids\n",
    "print(\"prototypes:\", centers, f\" | R_max={R_max:.3f}, R_95={R_95:.3f}\")\n",
    "\n",
    "# ---------- utility to scan k for an elbow ----------\n",
    "def scan_k(D_env, ks, D_geo=None, min_geo_km=5.0, relax_to=2.0):\n",
    "    rows = []\n",
    "    for kk in ks:\n",
    "        C, cov, rmax, r95 = kcenter_greedy(D_env, kk, D_geo, min_geo_km, relax_to)\n",
    "        rows.append((kk, rmax, r95))\n",
    "    return pd.DataFrame(rows, columns=[\"k\",\"R_max\",\"R_95\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# same distance matrix for k \n",
    "D_for_plot = D_env   \n",
    "\n",
    "mds_model = MDS(\n",
    "    n_components=2, dissimilarity=\"precomputed\",\n",
    "    random_state=42, n_init=8, max_iter=3000, normalized_stress=\"auto\"\n",
    ")\n",
    "mds = mds_model.fit_transform(D_for_plot)\n",
    "\n",
    "min_to_center = D_for_plot[:, centers].min(1)\n",
    "\n",
    "with mpl.rc_context({\n",
    "    \"figure.dpi\": 120,\n",
    "    \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "    \"axes.grid\": True, \"grid.linestyle\": \":\", \"grid.alpha\": 0.6, \"grid.linewidth\": 0.6,\n",
    "    \"axes.titlesize\": 12, \"axes.labelsize\": 11, \"xtick.labelsize\": 9, \"ytick.labelsize\": 9,\n",
    "}):\n",
    "    fig, ax = plt.subplots(figsize=(6.6, 4.6))\n",
    "\n",
    "    # sequential colormap for distances\n",
    "    vmax = np.percentile(min_to_center, 97.5)\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=vmax)\n",
    "    sc = ax.scatter(\n",
    "        mds[:, 0], mds[:, 1],\n",
    "        c=min_to_center, norm=mpl.colors.Normalize(0, vmax), cmap='viridis',\n",
    "        s=50, linewidths=0.4, edgecolors=\"white\", alpha=0.95, zorder=2\n",
    "    )\n",
    "\n",
    "    # ring prototypes\n",
    "    for c in centers:\n",
    "        ax.scatter(\n",
    "            mds[c, 0], mds[c, 1],\n",
    "            s=220, facecolors=\"none\", edgecolors=\"black\",\n",
    "            linewidths=0.8, linestyle=\"--\", zorder=4\n",
    "        )\n",
    "\n",
    "    # labels \n",
    "    names = DF[\"station_name\"].to_list()\n",
    "    nodes = DF[\"node_id\"].astype(int).to_list()\n",
    "    for i, node in enumerate(nodes):\n",
    "        ax.annotate(\n",
    "            node, (mds[i, 0], mds[i, 1]), xytext=(6, 6),\n",
    "            textcoords=\"offset points\", fontsize=8, color=\"#222\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.15\", fc=\"white\", ec=\"none\", alpha=0.7),\n",
    "            zorder=5\n",
    "        )\n",
    "\n",
    "    cbar = fig.colorbar(sc, ax=ax, shrink=0.86, pad=0.05)\n",
    "    cbar.set_label(\"Mahalanobis distance to nearest prototype\")\n",
    "\n",
    "    ax.set_title(\"Environmental space (MDS of Mahalanobis distances)\")\n",
    "    ax.set_xlabel(\"MDS-1\"); ax.set_ylabel(\"MDS-2\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"results/figures/eden/other/mds_prototypes.png\", bbox_inches=\"tight\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Create Train/Val/Test PyG Objects for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6d. Create PyG data objects using partioned station IDs (from 6a) ---\n",
    "\n",
    "# Run time approx. 13.5 mins to build 4018 timesteps of objects (0.201s per timestep)\n",
    "gwl_ohe_cols = joblib.load(os.path.join(config[catchment][\"paths\"][\"scalers_dir\"], \"gwl_ohe_cols.pkl\"))\n",
    "all_timesteps_list = build_pyg_object(\n",
    "    processed_df=processed_df_test,\n",
    "    sentinel_value=config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "    train_station_ids=train_station_ids,\n",
    "    val_station_ids=val_station_ids,\n",
    "    test_station_ids=test_station_ids,\n",
    "    gwl_feats=gwl_feats,\n",
    "    gwl_ohe_cols=gwl_ohe_cols,\n",
    "    edge_index_tensor=edge_index_tensor,\n",
    "    edge_attr_tensor=edge_attr_tensor,\n",
    "    scalers_dir=config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "# Save all_timesteps_list to file\n",
    "torch.save(all_timesteps_list, config[catchment][\"paths\"][\"pyg_object_path\"])\n",
    "logger.info(f\"Pipeline Step 'Build PyG Data Objects' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current git commit hash to help with reproducibility if run performance is lost\n",
    "git_commit = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode().strip()\n",
    "\n",
    "run_dir = os.path.join(\"runs\", datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "save_run_manifest(\n",
    "    run_dir=run_dir,\n",
    "    config=config,\n",
    "    git_commit=git_commit,\n",
    "    all_timesteps_list=all_timesteps_list,\n",
    "    temporal_features=config[catchment][\"model\"][\"architecture\"][\"temporal_features\"],\n",
    "    scalers_dir=config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    train_station_ids=train_station_ids,\n",
    "    val_station_ids=val_station_ids,\n",
    "    test_station_ids=test_station_ids,\n",
    "    edge_index_path=os.path.join(config[catchment][\"paths\"][\"graph_data_output_dir\"], \"edge_index_tensor.pt\"),\n",
    "    edge_attr_path=os.path.join(config[catchment][\"paths\"][\"graph_data_output_dir\"], \"edge_attr_tensor.pt\"),\n",
    "    sentinel_value=config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "    epsilon=config[\"global\"][\"graph\"][\"epsilon\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
