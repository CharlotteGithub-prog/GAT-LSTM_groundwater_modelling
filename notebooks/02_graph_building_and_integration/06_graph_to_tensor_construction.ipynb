{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config, deep_format, expanduser_tree\n",
    "from src.graph_building.graph_construction import build_mesh, \\\n",
    "    define_catchment_polygon, define_graph_adjacency\n",
    "from src.preprocessing.data_partitioning import define_station_id_splits, \\\n",
    "    load_graph_tensors, build_pyg_object\n",
    "from src.preprocessing.model_feature_engineering import preprocess_gwl_features, \\\n",
    "    preprocess_shared_features\n",
    "from src.utils.run_manifest import save_run_manifest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up root directory paths in config\n",
    "raw_data_root = config[\"global\"][\"paths\"][\"raw_data_root\"]\n",
    "results_root = config[\"global\"][\"paths\"][\"results_root\"]\n",
    "\n",
    "# Reformat config roots\n",
    "config = deep_format(\n",
    "    config,\n",
    "    raw_data_root=raw_data_root,\n",
    "    results_root=results_root\n",
    ")\n",
    "config = expanduser_tree(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Catchment area from country wide gdf\n",
    "define_catchment_polygon(\n",
    "    england_catchment_gdf_path=config[catchment]['paths']['gis_catchment_boundary'],\n",
    "    target_mncat=config[catchment]['target_mncat'],\n",
    "    catchment=catchment,\n",
    "    polygon_output_path=config[catchment]['paths']['gis_catchment_dir']\n",
    ")\n",
    "\n",
    "# Build catchment mesh\n",
    "mesh_nodes_table, mesh_nodes_gdf, mesh_cells_gdf_polygons, catchment_polygon = build_mesh(\n",
    "    shape_filepath=config[catchment]['paths']['gis_catchment_dir'],\n",
    "    output_path=config[catchment]['paths']['mesh_nodes_output'],\n",
    "    catchment=catchment,\n",
    "    grid_resolution=config[catchment]['preprocessing']['graph_construction']['grid_resolution']\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Build Mesh' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_edge_path = config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "# Create specific node_id column to merge\n",
    "directional_edge_weights[\"node_id\"] = range(0, len(directional_edge_weights))\n",
    "directional_edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_cells_gdf_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### BUILD EDGE WEIGHTS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in directional edge weights and mean elevation (not req. in main pipeline)\n",
    "directional_edge_path=config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "edge_attr_tensor, edge_index_tensor = define_graph_adjacency(\n",
    "    directional_edge_weights=directional_edge_weights,\n",
    "    elevation_geojson_path=config[catchment]['paths']['elevation_geojson_path'],\n",
    "    graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "    mesh_cells_gdf_polygons=mesh_cells_gdf_polygons,\n",
    "    epsilon_path=config[\"global\"][\"graph\"][\"epsilon\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Define Graph Adjacency' complete for {catchment} catchment.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Split into Train/Val/Test Split ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tensors from file if needed\n",
    "# edge_index_tensor, edge_attr_tensor = load_graph_tensors(\n",
    "#     graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "#     catchment=catchment\n",
    "# )\n",
    "\n",
    "# Load main_df_full from file if needed\n",
    "load_path = config[catchment][\"paths\"][\"final_df_path\"] + 'final_df.csv'\n",
    "main_df_full = pd.read_csv(load_path)\n",
    "main_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6a. Define Spatial Split for Observed Stations ---\n",
    "\n",
    "train_station_ids, val_station_ids, test_station_ids = define_station_id_splits(\n",
    "    main_df_full=main_df_full,\n",
    "    catchment=catchment,\n",
    "    test_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"test_station_shortlist\"],\n",
    "    val_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"val_station_shortlist\"],\n",
    "    random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "    output_dir=config[catchment][\"paths\"][\"aux_dir\"],\n",
    "    perc_train=config[catchment][\"model\"][\"data_partioning\"][\"percentage_train\"],\n",
    "    perc_val=config[catchment][\"model\"][\"data_partioning\"][\"percentage_val\"],\n",
    "    perc_test=config[catchment][\"model\"][\"data_partioning\"][\"percentage_test\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'define station splits' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Preprocess Shared Features Prior to Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6b. Preprocess (Standardise, one hot encode, round to 4dp) all shared features (not GWL) ---\n",
    "\n",
    "processed_df, shared_scaler, shared_encoder, gwl_feats = preprocess_shared_features(\n",
    "    main_df_full=main_df_full,\n",
    "    catchment=catchment,\n",
    "    random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "    violin_plt_path=config[catchment][\"visualisations\"][\"violin_plt_path\"],\n",
    "    scaler_dir = config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    aux_dir=config[catchment][\"paths\"][\"aux_dir\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Preprocess Final Shared Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Split processed_df into train/val/test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6c. Preprocess all GWL features using training data only ---\n",
    "\n",
    "processed_df, gwl_scaler, gwl_encoder = preprocess_gwl_features(\n",
    "    processed_df=processed_df,\n",
    "    catchment=catchment,\n",
    "    train_station_ids=train_station_ids,\n",
    "    val_station_ids=val_station_ids,\n",
    "    test_station_ids=test_station_ids,\n",
    "    sentinel_value=config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "    scaler_dir=config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    parquet_path=os.path.join(config[catchment][\"paths\"][\"final_df_path\"], 'processed_df.parquet')\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Preprocess Final GWL Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in processed_df after preprocessing:\", processed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shorten df to test range to reduce computation requirements\n",
    "\n",
    "# test_start_date_str = config[\"global\"][\"data_ingestion\"][\"test_start_date\"]\n",
    "# test_end_date_str = config[\"global\"][\"data_ingestion\"][\"test_end_date\"]\n",
    "\n",
    "# # Convert 'timestep' column to datetime objects\n",
    "# processed_df['timestep'] = pd.to_datetime(processed_df['timestep'])\n",
    "\n",
    "# processed_df_test = processed_df.loc[\n",
    "#     (processed_df['timestep'] >= test_start_date_str) &\n",
    "#     (processed_df['timestep'] <= test_end_date_str)\n",
    "# ].copy()\n",
    "\n",
    "# print(f\"Original processed_df shape (full data): {processed_df.shape}\")\n",
    "# print(f\"Test processed_df_test shape (sliced from {test_start_date_str} to {test_end_date_str}): {processed_df_test.shape}\")\n",
    "# processed_df_test = processed_df.drop(columns=['streamflow_total_m3', 'HOST_soil_class_freely_draining_soils', 'HOST_soil_class_high_runoff_(impermeable)', \n",
    "#                                                'HOST_soil_class_impeded_saturated_subsurface_flow', 'HOST_soil_class_peat_soils', 'aquifer_productivity_High',\n",
    "#                                                'aquifer_productivity_Low', 'aquifer_productivity_Mixed', 'aquifer_productivity_Moderate',\n",
    "#                                                'aquifer_productivity_nan']).copy()\n",
    "processed_df_test = processed_df.copy()\n",
    "\n",
    "# TESTING WITH GWL LAGS (AR Inputs) TO UNDERSTAND PERFORMANCE\n",
    "# processed_df_test = processed_df.drop(columns=['gwl_lag1', 'gwl_lag2', 'gwl_lag3', 'gwl_lag4', 'gwl_lag5', 'gwl_lag6', 'gwl_lag7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display full (test) processed df\n",
    "processed_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_df_test.columns[54:61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_df_test.columns[62:63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Assign validation stations using geographic proximity with buffering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to final preprocessed static columns\n",
    "column_indices = list(range(0, 9)) + list(range(25, 54)) + list(range(61, 63))\n",
    "split_df = processed_df_test.iloc[:, column_indices]\n",
    "\n",
    "# Aggregate to node_id\n",
    "aggregated_df = split_df.groupby('node_id').first().reset_index()\n",
    "\n",
    "# Get station data\n",
    "station_nodes = [430, 902, 1254, 1326, 1335, 1420, 1556, 1648, 1772, 1858, 1983, 2388, 2487, 2594]\n",
    "station_dfs = aggregated_df[aggregated_df['node_id'].isin(station_nodes)]\n",
    "station_dfs = station_dfs.drop(columns='timestep')\n",
    "\n",
    "# Load reference df\n",
    "station_metadata = pd.read_csv(\"data/02_processed/eden/gwl_station_data/snapped_station_node_mapping.csv\")\n",
    "\n",
    "# Merge in required data\n",
    "merged_station_df = station_dfs.merge(\n",
    "    station_metadata[['node_id', 'station_name', 'easting', 'northing', 'geometry']],\n",
    "    on='node_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean station_name column and drop unneeded stations\n",
    "merged_station_df['station_name'] = merged_station_df['station_name'].astype(str).str.lower().str.replace(\" \", \"_\")\n",
    "rows_to_drop = merged_station_df[merged_station_df['station_name'] == 'cliburn_town_bridge_1'].index\n",
    "merged_station_df.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "merged_station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "DF = merged_station_df.copy() \n",
    "BUFFER_KM = 5.0  # km\n",
    "USE_BUFFER = True\n",
    "RIDGE = 1e-3\n",
    "\n",
    "# Columns not used as environmental features\n",
    "EXCLUDE = {'node_id','station_name','easting','northing','geometry'}\n",
    "\n",
    "# Get pairwise geographic distances\n",
    "def pairwise_geo_km(xy_m):\n",
    "    d = xy_m[:, None, :] - xy_m[None, :, :]\n",
    "    return np.hypot(d[...,0], d[...,1]) / 1000.0\n",
    "\n",
    "# Calc per-fold whitening (Mahalanobis)\n",
    "def whiten_per_fold(X, train_idx, ridge=1e-3):\n",
    "    \"\"\"\n",
    "    Returns X_whitened for all rows, using mean/cov estimated on train_idx only.\n",
    "    \"\"\"\n",
    "    mu = X[train_idx].mean(axis=0)\n",
    "    Xc = X - mu\n",
    "    Xt = X[train_idx] - mu\n",
    "    # covariance on training stations\n",
    "    C  = np.cov(Xt, rowvar=False)\n",
    "    # ridge on diagonal\n",
    "    C.flat[::C.shape[0]+1] += ridge\n",
    "    # Σ^{-1/2}\n",
    "    U,S,_ = np.linalg.svd(C, full_matrices=False)\n",
    "    W = U @ np.diag(1.0/np.sqrt(S)) @ U.T\n",
    "    return Xc @ W\n",
    "\n",
    "# prepare merged_station_df matrices\n",
    "ids = DF['node_id'].to_numpy()\n",
    "XY  = DF[['easting','northing']].to_numpy(float)  # metres (BNG)\n",
    "feat_cols = [c for c in DF.columns if c not in EXCLUDE]\n",
    "X = DF[feat_cols].to_numpy(float)\n",
    "\n",
    "D_geo = pairwise_geo_km(XY)\n",
    "\n",
    "# proximity vs environmental similarity diagnostic\n",
    "def proximity_env_report(X, D_geo, use_whitening=True, ridge=1e-3):\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # global whitening for diagnostic only (not selection)\n",
    "    if use_whitening:\n",
    "        Xc = X - X.mean(0)\n",
    "        C  = np.cov(Xc, rowvar=False)\n",
    "        C.flat[::C.shape[0]+1] += ridge\n",
    "        U,S,_ = np.linalg.svd(C, full_matrices=False)\n",
    "        W = U @ np.diag(1.0/np.sqrt(S)) @ U.T\n",
    "        Xw = Xc @ W\n",
    "    else:\n",
    "        Xw = X\n",
    "\n",
    "    diff = Xw[:,None,:] - Xw[None,:,:]\n",
    "    D_env = np.sqrt((diff**2).sum(-1))\n",
    "\n",
    "    # upper triangle\n",
    "    mask = np.triu(np.ones_like(D_env, dtype=bool), 1)\n",
    "    rho, p_spear = spearmanr(D_env[mask], D_geo[mask])\n",
    "\n",
    "    # quick Mantel permutation test\n",
    "    def mantel(A, B, perms=20000, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        a = A[mask]; b = B[mask]\n",
    "        obs = np.corrcoef(a, b)[0,1]\n",
    "        cnt = 0\n",
    "        for _ in range(perms):\n",
    "            p = rng.permutation(n)\n",
    "            bb = B[np.ix_(p, p)][mask]\n",
    "            if np.corrcoef(a, bb)[0,1] >= obs:\n",
    "                cnt += 1\n",
    "        p_val = (cnt + 1) / (perms + 1)\n",
    "        return float(obs), float(p_val)\n",
    "\n",
    "    r_mantel, p_mantel = mantel(D_env, D_geo, perms=20000, seed=42)\n",
    "\n",
    "    print(f\"Spearman ρ(env, geo) = {rho:.3f}  (p={p_spear:.3f})\")\n",
    "    print(f\"Mantel r = {r_mantel:.3f}  (p={p_mantel:.3f})\")\n",
    "    return D_env\n",
    "\n",
    "# Run diagnostic report\n",
    "D_env_diag = proximity_env_report(X, D_geo, use_whitening=True)\n",
    "\n",
    "# Get assignments: two val stations by environmental similarity\n",
    "def assign_two_validations_env_only(ids, X, XY, buffer_km=5.0, use_buffer=True, ridge=1e-3):\n",
    "    \"\"\"\n",
    "    For each test station i, choose two validation stations j by:\n",
    "      - computing Mahalanobis distances in static space with params\n",
    "      - ranking by environmental dist only\n",
    "      - discarding candidates within buffer dist of test station.\n",
    "    \"\"\"\n",
    "    n = len(ids)\n",
    "    D_geo = pairwise_geo_km(XY)\n",
    "    rows = []\n",
    "\n",
    "    for i in range(n):\n",
    "        train_idx = np.arange(n) != i\n",
    "        Xw = whiten_per_fold(X, train_idx, ridge=ridge)\n",
    "\n",
    "        # environmental distances from test i to everyone\n",
    "        d_env = np.linalg.norm(Xw - Xw[i], axis=1)\n",
    "        d_env[i] = np.inf\n",
    "\n",
    "        # candidate mask\n",
    "        if use_buffer and buffer_km > 0:\n",
    "            cand = (D_geo[i] >= buffer_km) & (np.arange(n) != i)\n",
    "            # relax if too few\n",
    "            if cand.sum() < 2:\n",
    "                cand = (D_geo[i] >= max(3.0, 0.6*buffer_km)) & (np.arange(n) != i)\n",
    "        else:\n",
    "            cand = (np.arange(n) != i)\n",
    "\n",
    "        idxs = np.where(cand)[0]\n",
    "        order = idxs[np.argsort(d_env[idxs])]\n",
    "\n",
    "        v1 = order[0]\n",
    "        v2 = order[1] if len(order) > 1 else None\n",
    "\n",
    "        rows.append({\n",
    "            \"test_node\": ids[i],\n",
    "            \"val1_node\": ids[v1],\n",
    "            \"val2_node\": (ids[v2] if v2 is not None else np.nan),\n",
    "            \"env_d_val1\": float(d_env[v1]),\n",
    "            \"env_d_val2\": (float(d_env[v2]) if v2 is not None else np.nan),\n",
    "            \"geo_km_val1\": float(D_geo[i, v1]),\n",
    "            \"geo_km_val2\": (float(D_geo[i, v2]) if v2 is not None else np.nan)\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "assignments = assign_two_validations_env_only(\n",
    "    ids=ids, X=X, XY=XY,\n",
    "    buffer_km=BUFFER_KM, use_buffer=USE_BUFFER, ridge=RIDGE\n",
    ")\n",
    "\n",
    "assignments.sort_values(\"test_node\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Create Train/Val/Test PyG Objects for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6d. Create PyG data objects using partioned station IDs (from 6a) ---\n",
    "\n",
    "# Run time approx. 13.5 mins to build 4018 timesteps of objects (0.201s per timestep)\n",
    "gwl_ohe_cols = joblib.load(os.path.join(config[catchment][\"paths\"][\"scalers_dir\"], \"gwl_ohe_cols.pkl\"))\n",
    "all_timesteps_list = build_pyg_object(\n",
    "    processed_df=processed_df_test,\n",
    "    sentinel_value=config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "    train_station_ids=train_station_ids,\n",
    "    val_station_ids=val_station_ids,\n",
    "    test_station_ids=test_station_ids,\n",
    "    gwl_feats=gwl_feats,\n",
    "    gwl_ohe_cols=gwl_ohe_cols,\n",
    "    edge_index_tensor=edge_index_tensor,\n",
    "    edge_attr_tensor=edge_attr_tensor,\n",
    "    scalers_dir=config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "# Save all_timesteps_list to file\n",
    "torch.save(all_timesteps_list, config[catchment][\"paths\"][\"pyg_object_path\"])\n",
    "logger.info(f\"Pipeline Step 'Build PyG Data Objects' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current git commit hash to help with reproducibility if run performance is lost\n",
    "git_commit = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode().strip()\n",
    "\n",
    "run_dir = os.path.join(\"runs\", datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "save_run_manifest(\n",
    "    run_dir=run_dir,\n",
    "    config=config,\n",
    "    git_commit=git_commit,\n",
    "    all_timesteps_list=all_timesteps_list,\n",
    "    temporal_features=config[catchment][\"model\"][\"architecture\"][\"temporal_features\"],\n",
    "    scalers_dir=config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    train_station_ids=train_station_ids,\n",
    "    val_station_ids=val_station_ids,\n",
    "    test_station_ids=test_station_ids,\n",
    "    edge_index_path=os.path.join(config[catchment][\"paths\"][\"graph_data_output_dir\"], \"edge_index_tensor.pt\"),\n",
    "    edge_attr_path=os.path.join(config[catchment][\"paths\"][\"graph_data_output_dir\"], \"edge_attr_tensor.pt\"),\n",
    "    sentinel_value=config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "    epsilon=config[\"global\"][\"graph\"][\"epsilon\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
