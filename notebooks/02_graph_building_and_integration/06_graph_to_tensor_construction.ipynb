{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config, deep_format, expanduser_tree\n",
    "from src.graph_building.graph_construction import build_mesh, \\\n",
    "    define_catchment_polygon, define_graph_adjacency\n",
    "from src.preprocessing.data_partitioning import define_station_id_splits, \\\n",
    "    load_graph_tensors, build_pyg_object\n",
    "from src.preprocessing.model_feature_engineering import preprocess_gwl_features, \\\n",
    "    preprocess_shared_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up root directory paths in config\n",
    "raw_data_root = config[\"global\"][\"paths\"][\"raw_data_root\"]\n",
    "results_root = config[\"global\"][\"paths\"][\"results_root\"]\n",
    "\n",
    "# Reformat config roots\n",
    "config = deep_format(\n",
    "    config,\n",
    "    raw_data_root=raw_data_root,\n",
    "    results_root=results_root\n",
    ")\n",
    "config = expanduser_tree(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Catchment area from country wide gdf\n",
    "define_catchment_polygon(\n",
    "    england_catchment_gdf_path=config[catchment]['paths']['gis_catchment_boundary'],\n",
    "    target_mncat=config[catchment]['target_mncat'],\n",
    "    catchment=catchment,\n",
    "    polygon_output_path=config[catchment]['paths']['gis_catchment_dir']\n",
    ")\n",
    "\n",
    "# Build catchment mesh\n",
    "mesh_nodes_table, mesh_nodes_gdf, mesh_cells_gdf_polygons, catchment_polygon = build_mesh(\n",
    "    shape_filepath=config[catchment]['paths']['gis_catchment_dir'],\n",
    "    output_path=config[catchment]['paths']['mesh_nodes_output'],\n",
    "    catchment=catchment,\n",
    "    grid_resolution=config[catchment]['preprocessing']['graph_construction']['grid_resolution']\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Build Mesh' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_edge_path = config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "# Create specific node_id column to merge\n",
    "directional_edge_weights[\"node_id\"] = range(0, len(directional_edge_weights))\n",
    "directional_edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_cells_gdf_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### BUILD EDGE WEIGHTS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in directional edge weights and mean elevation (not req. in main pipeline)\n",
    "directional_edge_path=config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "edge_attr_tensor, edge_index_tensor = define_graph_adjacency(\n",
    "    directional_edge_weights=directional_edge_weights,\n",
    "    elevation_geojson_path=config[catchment]['paths']['elevation_geojson_path'],\n",
    "    graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "    mesh_cells_gdf_polygons=mesh_cells_gdf_polygons,\n",
    "    epsilon_path=config[\"global\"][\"graph\"][\"epsilon\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Define Graph Adjacency' complete for {catchment} catchment.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Split into Train/Val/Test Split ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tensors from file if needed\n",
    "# edge_index_tensor, edge_attr_tensor = load_graph_tensors(\n",
    "#     graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "#     catchment=catchment\n",
    "# )\n",
    "\n",
    "# Load main_df_full from file if needed\n",
    "load_path = config[catchment][\"paths\"][\"final_df_path\"] + 'final_df.csv'\n",
    "main_df_full = pd.read_csv(load_path)\n",
    "main_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6a. Define Spatial Split for Observed Stations ---\n",
    "\n",
    "train_station_ids, val_station_ids, test_station_ids = define_station_id_splits(\n",
    "    main_df_full=main_df_full,\n",
    "    catchment=catchment,\n",
    "    test_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"test_station_shortlist\"],\n",
    "    val_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"val_station_shortlist\"],\n",
    "    random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "    output_dir=config[catchment][\"paths\"][\"aux_dir\"],\n",
    "    perc_train=config[catchment][\"model\"][\"data_partioning\"][\"percentage_train\"],\n",
    "    perc_val=config[catchment][\"model\"][\"data_partioning\"][\"percentage_val\"],\n",
    "    perc_test=config[catchment][\"model\"][\"data_partioning\"][\"percentage_test\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'define station splits' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Preprocess Shared Features Prior to Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6b. Preprocess (Standardise, one hot encode, round to 4dp) all shared features (not GWL) ---\n",
    "\n",
    "processed_df, shared_scaler, shared_encoder, gwl_feats = preprocess_shared_features(\n",
    "    main_df_full=main_df_full,\n",
    "    catchment=catchment,\n",
    "    random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "    violin_plt_path=config[catchment][\"visualisations\"][\"violin_plt_path\"],\n",
    "    scaler_dir = config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    aux_dir=config[catchment][\"paths\"][\"aux_dir\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Preprocess Final Shared Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Split processed_df into train/val/test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6c. Preprocess all GWL features using training data only ---\n",
    "\n",
    "processed_df, gwl_scaler, gwl_encoder = preprocess_gwl_features(\n",
    "    processed_df=processed_df,\n",
    "    catchment=catchment,\n",
    "    train_station_ids=train_station_ids,\n",
    "    val_station_ids=val_station_ids,\n",
    "    test_station_ids=test_station_ids,\n",
    "    sentinel_value=config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "    scaler_dir=config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    parquet_path=os.path.join(config[catchment][\"paths\"][\"final_df_path\"], 'processed_df.parquet')\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Preprocess Final GWL Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in processed_df after preprocessing:\", processed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shorten df to test range to reduce computation requirements\n",
    "\n",
    "# test_start_date_str = config[\"global\"][\"data_ingestion\"][\"test_start_date\"]\n",
    "# test_end_date_str = config[\"global\"][\"data_ingestion\"][\"test_end_date\"]\n",
    "\n",
    "# # Convert 'timestep' column to datetime objects\n",
    "# processed_df['timestep'] = pd.to_datetime(processed_df['timestep'])\n",
    "\n",
    "# processed_df_test = processed_df.loc[\n",
    "#     (processed_df['timestep'] >= test_start_date_str) &\n",
    "#     (processed_df['timestep'] <= test_end_date_str)\n",
    "# ].copy()\n",
    "\n",
    "# print(f\"Original processed_df shape (full data): {processed_df.shape}\")\n",
    "# print(f\"Test processed_df_test shape (sliced from {test_start_date_str} to {test_end_date_str}): {processed_df_test.shape}\")\n",
    "processed_df_test = processed_df.drop(columns=['streamflow_total_m3', 'HOST_soil_class_freely_draining_soils', 'HOST_soil_class_high_runoff_(impermeable)', \n",
    "                                               'HOST_soil_class_impeded_saturated_subsurface_flow', 'HOST_soil_class_peat_soils', 'aquifer_productivity_High',\n",
    "                                               'aquifer_productivity_Low', 'aquifer_productivity_Mixed', 'aquifer_productivity_Moderate',\n",
    "                                               'aquifer_productivity_nan']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full (test) processed df\n",
    "processed_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Create Train/Val/Test PyG Objects for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6d. Create PyG data objects using partioned station IDs (from 6a) ---\n",
    "\n",
    "# Run time approx. 13.5 mins to build 4018 timesteps of objects (0.201s per timestep)\n",
    "gwl_ohe_cols = joblib.load(os.path.join(config[catchment][\"paths\"][\"scalers_dir\"], \"gwl_ohe_cols.pkl\"))\n",
    "all_timesteps_list = build_pyg_object(\n",
    "    processed_df=processed_df_test,\n",
    "    sentinel_value=config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "    train_station_ids=train_station_ids,\n",
    "    val_station_ids=val_station_ids,\n",
    "    test_station_ids=test_station_ids,\n",
    "    gwl_feats=gwl_feats,\n",
    "    gwl_ohe_cols=gwl_ohe_cols,\n",
    "    edge_index_tensor=edge_index_tensor,\n",
    "    edge_attr_tensor=edge_attr_tensor,\n",
    "    scalers_dir=config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "# Save all_timesteps_list to file\n",
    "torch.save(all_timesteps_list, config[catchment][\"paths\"][\"pyg_object_path\"])\n",
    "logger.info(f\"Pipeline Step 'Build PyG Data Objects' complete for {catchment} catchment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
