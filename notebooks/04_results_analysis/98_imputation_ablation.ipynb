{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import joblib\n",
    "import logging\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config, deep_format, expanduser_tree\n",
    "from src.data_ingestion.gwl_data_ingestion import process_station_coordinates, \\\n",
    "    fetch_and_process_station_data, download_and_save_station_readings\n",
    "from src.preprocessing.gwl_preprocessing import load_timeseries_to_dict, outlier_detection, \\\n",
    "    resample_timestep_average, remove_spurious_data, handle_short_gaps\n",
    "from src.archive.gap_imputation_copy import synthetic_gap_imputation_validation, define_catchment_size, \\\n",
    "    calculate_station_distances, calculate_station_correlations, score_station_proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up root directory paths in config\n",
    "raw_data_root = config[\"global\"][\"paths\"][\"raw_data_root\"]\n",
    "results_root = config[\"global\"][\"paths\"][\"results_root\"]\n",
    "\n",
    "# Reformat config roots\n",
    "config = deep_format(\n",
    "    config,\n",
    "    raw_data_root=raw_data_root,\n",
    "    results_root=results_root\n",
    ")\n",
    "config = expanduser_tree(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]  # True to run API calls\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### DATA INGESTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Load gwl station list with grid references and convert grid references to easting, northing, longitude and latitude form for plotting and data alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Process Catchment Stations List ----\n",
    "stations_with_coords_df = process_station_coordinates(\n",
    "    os_grid_squares=config[\"global\"][\"paths\"][\"gis_os_grid_squares\"],\n",
    "    station_list_input=config[catchment][\"paths\"][\"gwl_station_list\"],\n",
    "    station_list_output=config[catchment][\"paths\"][\"gwl_station_list_with_coords\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Process Station Coordinates for {catchment}' complete.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "**API Documentation notes:**\n",
    "\n",
    "1. The API calls that return readings data have a soft limit of 100,000 rows per-call which can be overridden by setting a _limit parameter. There is a hard limit of 2,000,000 rows, which cannot be overridden.\n",
    "2. The primary identifier for most stations uses a GUID style identifier called an SUID. These are used in the URL for the station and given as the value of the notation property in the station metadata.  \n",
    "    a. Wiski identifier (wiskiID) is also available for my subset of stations and data type  \n",
    "3. All monitoring stations can be filtered by name, location and other parameters. See https://environment.data.gov.uk/hydrology/doc/reference#stations-summary for full metadata details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_defra_API_calls:\n",
    "    # Retrieve gwl monitoring station metadata and measures from DEFRA API\n",
    "    stations_with_metadata_measures = fetch_and_process_station_data(\n",
    "        stations_df=stations_with_coords_df,\n",
    "        base_url=config[\"global\"][\"paths\"][\"defra_station_base_url\"],\n",
    "        output_path=config[catchment][\"paths\"][\"gwl_station_metadata_measures\"]\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Pipeline step 'Pull Hydrological Station Metadata for {catchment}' complete.\\n\")\n",
    "\n",
    "    stations_with_metadata_measures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_defra_API_calls:\n",
    "    download_and_save_station_readings(\n",
    "        stations_df=stations_with_metadata_measures,\n",
    "        start_date=config[\"global\"][\"data_ingestion\"][\"api_start_date\"],\n",
    "        end_date=config[\"global\"][\"data_ingestion\"][\"api_end_date\"],\n",
    "        gwl_data_output_dir=config[catchment][\"paths\"][\"gwl_data_output_dir\"]\n",
    "    )\n",
    "\n",
    "    logger.info(f\"All timeseries groundwater level data saved for {catchment} catchment.\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    loaded_csv_path = config[catchment][\"paths\"][\"gwl_station_metadata_measures\"]\n",
    "    stations_with_metadata_measures = pd.read_csv(loaded_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### PREPROCESSING ###\n",
    "\n",
    "Remove stations with insufficient data and clean ts data from outliers and incorrect measurements. Interpolate between small data gaps using rational spline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "1. Load station df's into dict, dropping catchments with insufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load timeseries CSVs from API into reference dict\n",
    "gwl_time_series_dict = load_timeseries_to_dict(\n",
    "    stations_df=stations_with_metadata_measures,\n",
    "    col_order=config[\"global\"][\"data_ingestion\"][\"col_order\"],\n",
    "    data_dir=config[catchment][\"paths\"][\"gwl_data_output_dir\"],\n",
    "    inclusion_threshold=config[catchment][\"preprocessing\"][\"inclusion_threshold\"],\n",
    "    station_list_output=config[catchment][\"paths\"][\"gwl_station_list_output\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"All timeseries data converted to dict for {catchment} catchment.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "2. Remove outlying and incorrect data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_name, df in gwl_time_series_dict.items():\n",
    "    gwl_time_series_dict[station_name] = remove_spurious_data(\n",
    "        target_df=df,\n",
    "        station_name=station_name,\n",
    "        path=config[catchment][\"visualisations\"][\"ts_plots\"][\"time_series_gwl_output\"],\n",
    "        pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"],\n",
    "        notebook=notebook\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_outlier_processing = config[\"global\"][\"pipeline_settings\"][\"run_outlier_detection\"]\n",
    "\n",
    "if run_outlier_processing:\n",
    "    # run outlier detection and processing\n",
    "    processed_gwl_time_series_dict = outlier_detection(\n",
    "        gwl_time_series_dict=gwl_time_series_dict,\n",
    "        output_path=config[catchment][\"visualisations\"][\"ts_plots\"][\"time_series_gwl_output\"],\n",
    "        dpi=config[catchment][\"visualisations\"][\"ts_plots\"][\"dpi_save\"],\n",
    "        dict_output=config[catchment][\"paths\"][\"gwl_outlier_dict\"],\n",
    "        notebook=notebook\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "3. Aggregate to timestep frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_outlier_processing:\n",
    "    input_dict = config[catchment][\"paths\"][\"gwl_outlier_dict\"]\n",
    "    processed_gwl_time_series_dict = joblib.load(input_dict)\n",
    "\n",
    "timestep_data = resample_timestep_average(\n",
    "    gwl_data_dict=processed_gwl_time_series_dict,\n",
    "    start_date=config[\"global\"][\"data_ingestion\"][\"api_start_date\"],\n",
    "    end_date=config[\"global\"][\"data_ingestion\"][\"api_end_date\"],\n",
    "    path=config[catchment][\"visualisations\"][\"ts_plots\"][\"time_series_gwl_output\"],\n",
    "    pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"],\n",
    "    notebook=notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "4. Interpolate across small gaps in the ts data using rational spline or PCHIP - try both (& define threshold n/o missing time steps for interpolation eligibility) + Add binary interpolation flag column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_data, gaps_list, station_max_gap_lengths_calculated = handle_short_gaps(\n",
    "    timestep_data=timestep_data,\n",
    "    path=config[catchment][\"visualisations\"][\"ts_plots\"][\"time_series_gwl_output\"],\n",
    "    max_steps=config[\"global\"][\"data_ingestion\"][\"max_interp_length\"],\n",
    "    start_date=config[\"global\"][\"data_ingestion\"][\"api_start_date\"],\n",
    "    end_date=config[\"global\"][\"data_ingestion\"][\"api_end_date\"],\n",
    "    pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"],\n",
    "    notebook=notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Handle large gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all names match formatting to avoid mismatches\n",
    "def _clean_name(s):\n",
    "    s_clean = s.lower().replace(\" \", \"_\")\n",
    "    return s_clean\n",
    "\n",
    "timestep_data = {_clean_name(key): value for key, value in timestep_data.items()}\n",
    "gaps_list = [_clean_name(station) for station in gaps_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dist + corr matrices\n",
    "spatial_df = pd.read_csv(config[catchment][\"paths\"][\"gwl_station_list_output\"])\n",
    "spatial_df[\"station_name\"] = spatial_df[\"station_name\"].str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "large_catchment = define_catchment_size(\n",
    "    spatial_df, catchment,\n",
    "    threshold_m=config[catchment][\"preprocessing\"][\"large_catchment_threshold_m\"]\n",
    ")\n",
    "distance_matrix = calculate_station_distances(\n",
    "    spatial_df,\n",
    "    use_haversine=large_catchment,\n",
    "    radius=config[\"global\"][\"preprocessing\"][\"radius\"],\n",
    ")\n",
    "distance_matrix = np.round(distance_matrix, 2)\n",
    "\n",
    "correlation_matrix = calculate_station_correlations(timestep_data, catchment)\n",
    "\n",
    "# 2) score donors and threshold\n",
    "filtered_scores = score_station_proximity(\n",
    "    df_dist=timestep_data,\n",
    "    gaps_list=gaps_list,  # stations to validate\n",
    "    correlation_matrix=correlation_matrix,\n",
    "    distance_matrix=distance_matrix,\n",
    "    k_decay=config[catchment][\"preprocessing\"][\"dist_corr_score_k_decay\"],\n",
    "    output_path=config[catchment][\"visualisations\"][\"corr_dist_score_scatters\"],\n",
    "    threshold=config[catchment][\"preprocessing\"][\"dist_corr_score_threshold\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal keys  ->  Pretty labels in the paper\n",
    "methods = [\"pchip\", \"spline\", \"linear\", \"donor_station\"]\n",
    "gap_lengths = [7,14,30,60,90,120,150,180,250,365]\n",
    "labels = {\"pchip\": \"PCHIP\", \"spline\": \"Spline\",\n",
    "          \"linear\": \"Linear\", \"donor_station\": \"Donor-station\"}\n",
    "\n",
    "rows = []\n",
    "for m in methods:\n",
    "    for L in gap_lengths:\n",
    "        \n",
    "        # run once per call to collect results\n",
    "        (_, _, _, summary_df) = synthetic_gap_imputation_validation(\n",
    "            df_dict_original=timestep_data,\n",
    "            gaps_list=gaps_list,\n",
    "            min_around=config[\"global\"][\"preprocessing\"][\"min_data_points_around_gap\"],\n",
    "            predefined_large_gap_lengths=[L],\n",
    "            max_imputation_length_threshold=L,\n",
    "            filtered_scores=filtered_scores,\n",
    "            validation_plot_path=\"results/figures/eden/imputation_validation/validation\",\n",
    "            imputation_plot_path=\"results/figures/eden/imputation_validation/imputation\",\n",
    "            pred_frequency=config[\"global\"][\"pipeline_settings\"][\"prediction_resolution\"],\n",
    "            station_max_gap_lengths=station_max_gap_lengths_calculated,\n",
    "            random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "            imputation_method=(\"weighted_average\" if m == \"donor_station\" else m),\n",
    "            distance_matrix=distance_matrix,   # <— NEW\n",
    "            k_decay=config[catchment][\"preprocessing\"][\"dist_corr_score_k_decay\"],  # <— NEW\n",
    "        )\n",
    "        \n",
    "        # summary_df of metrics\n",
    "        summary_df = summary_df.copy()\n",
    "        summary_df[\"method\"] = labels[m]\n",
    "        summary_df[\"gap_days\"] = L\n",
    "        rows.append(summary_df)\n",
    "\n",
    "import pandas as pd\n",
    "tidy = pd.concat(rows, ignore_index=True)\n",
    "tidy = tidy[[\"gap_days\", \"method\", \"RMSE_mean\", \"MAE_mean\", \"KGE_mean\"]]\n",
    "\n",
    "# Pivot to match table layout\n",
    "wide = (tidy\n",
    "        .set_index([\"gap_days\",\"method\"])\n",
    "        .sort_index()\n",
    "        .unstack(\"method\"))  # MultiIndex columns: (metric, method)\n",
    "\n",
    "# Reorder the column MultiIndex as in headings\n",
    "method_order = [\"PCHIP\", \"Spline\", \"Linear\", \"Donor-station\"]\n",
    "metric_order = [\"RMSE_mean\", \"MAE_mean\", \"KGE_mean\"]\n",
    "\n",
    "# Build new ordered set of columns\n",
    "new_cols = []\n",
    "for meth in method_order:\n",
    "    for metric in metric_order:\n",
    "        new_cols.append((metric, meth))\n",
    "\n",
    "# reindex cols to order and round to table accuracy\n",
    "wide = wide.reindex(columns=pd.MultiIndex.from_tuples(new_cols))\n",
    "wide = wide.round(3)\n",
    "\n",
    "print(wide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
