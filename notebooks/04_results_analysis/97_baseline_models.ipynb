{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config, deep_format, expanduser_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up root directory paths in config\n",
    "raw_data_root = config[\"global\"][\"paths\"][\"raw_data_root\"]\n",
    "results_root = config[\"global\"][\"paths\"][\"results_root\"]\n",
    " \n",
    "# Reformat config roots\n",
    "config = deep_format(\n",
    "    config,\n",
    "    raw_data_root=raw_data_root,\n",
    "    results_root=results_root\n",
    ")\n",
    "config = expanduser_tree(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Climatology (catchment seasonal mean) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in geographic reference data\n",
    "geo_path = config[catchment][\"paths\"][\"gwl_station_list_with_coords\"]\n",
    "geo_cols = [\"station_name\", \"easting\", \"northing\"]\n",
    "stations_geo = pd.read_csv(geo_path, usecols=geo_cols)\n",
    "stations_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_station = \"longtown\"\n",
    "omitted = \"cliburn_town_bridge_1\"\n",
    "station_df_dir = config[catchment][\"paths\"][\"trimmed_output_dir\"]\n",
    "columns_to_load = [\"Unnamed: 0\", \"value\"]\n",
    "\n",
    "# Get list of files in the dir\n",
    "all_files = glob.glob(os.path.join(station_df_dir, \"*.csv\"))\n",
    "\n",
    "# Filter out test file\n",
    "files_to_load = [file for file in all_files if os.path.basename(file) not in \n",
    "                 [f\"{test_station}_trimmed.csv\", f\"{omitted}_trimmed.csv\"]]\n",
    "\n",
    "# loop through training station list and load each file\n",
    "loaded_dataframes = {}\n",
    "for file_path in files_to_load:\n",
    "    filename = os.path.basename(file_path).replace(\"_trimmed.csv\", \"\")\n",
    "    logger.info(f\"Loading {filename}...\")\n",
    "    \n",
    "    df = pd.read_csv(file_path, usecols=columns_to_load)\n",
    "    \n",
    "    # Clean dfs and build day of year col\n",
    "    df.rename(columns={\"Unnamed: 0\": \"date\"}, inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    \n",
    "    # Handle leap years to keep all years same length\n",
    "    leap_day_mask = (df['date'].dt.month == 2) & (df['date'].dt.day == 29)\n",
    "    df.loc[leap_day_mask, 'day_of_year'] = 59  # Map Feb 29 to Feb 28 \n",
    "    \n",
    "    # Shift all after adjusted leap year day back by one\n",
    "    after_leap_day_mask = (df['date'].dt.is_leap_year) & (df['day_of_year'] > 59)\n",
    "    df.loc[after_leap_day_mask, 'day_of_year'] -= 1\n",
    "    \n",
    "    # Get easting and northing from reference loaded in above\n",
    "    coords = stations_geo[stations_geo['station_name'] == filename]\n",
    "    \n",
    "    if not coords.empty:\n",
    "        df['easting'] = coords['easting'].iloc[0]\n",
    "        df['northing'] = coords['northing'].iloc[0]\n",
    "    else:\n",
    "        logger.warning(f\"Could not find coordinates for station: {filename}. Skipping coordinate assignment.\")\n",
    "    \n",
    "    # Add to dict of dfs\n",
    "    loaded_dataframes[filename] = df\n",
    "    \n",
    "# Confirm load is as expected\n",
    "logger.info(f\"Files loaded: {len(list(loaded_dataframes.keys()))}\\n\")\n",
    "# print(list(loaded_dataframes.keys()))\n",
    "\n",
    "loaded_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dict to store baselines\n",
    "station_baselines = {}\n",
    "\n",
    "# Loop through each DataFrame in the dictionary\n",
    "for station_name, df in loaded_dataframes.items():\n",
    "    \n",
    "    # Get baselines\n",
    "    mean_level = df['value'].mean()  # Mean level (offset): mu_j = mean(y_j,t)\n",
    "    doy_mean_curve = df.groupby('day_of_year')['value'].mean()  # DOY mean curve: m_j,k = mean{yj,t:DOY(t)=k}\n",
    "    shape_curve = doy_mean_curve - mean_level  # Shape: sj,k = mj,k - μj (DOY mean curve with the overall mean subtracted)\n",
    "    \n",
    "    # Store the results in the baselines dictionary\n",
    "    station_baselines[station_name] = {\n",
    "        'mean_level': mean_level,\n",
    "        'doy_mean_curve': doy_mean_curve,\n",
    "        'shape_curve': shape_curve\n",
    "    }\n",
    "\n",
    "# log success to confirm\n",
    "logger.info(\"Baseline calculations complete for all training stations.\")\n",
    "logger.info(f\"Number of stations with calculated baselines: {len(station_baselines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "for station_name, df in loaded_dataframes.items():\n",
    "    \n",
    "    # Get  day-of-year mean curve for the current station\n",
    "    doy_mean_curve = station_baselines[station_name]['doy_mean_curve']\n",
    "    \n",
    "    # Generate the baseline prediction by mapping 'day_of_year' to the mean curve\n",
    "    df['baseline_pred'] = df['day_of_year'].map(doy_mean_curve)\n",
    "\n",
    "loaded_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Build PyG Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['value', 'baseline_pred'] \n",
    "n_lags = 7  # previous 7 days as features\n",
    "k_neighbors = 3 # Number of spatial neighbors for graph edges\n",
    "\n",
    "def create_lagged_features(df, features, n_lags):\n",
    "    \"\"\"\n",
    "    Creates lagged features for a given DataFrame.\n",
    "    \"\"\"\n",
    "    df_with_lags = df.copy()\n",
    "    for feature in features:\n",
    "        for lag in range(1, n_lags + 1):\n",
    "            df_with_lags[f'{feature}_lag{lag}'] = df_with_lags[feature].shift(lag)\n",
    "    \n",
    "    # Drop the first 'n_lags' rows with NaN values\n",
    "    df_with_lags.dropna(inplace=True)\n",
    "    return df_with_lags\n",
    "\n",
    "def build_pyg_data_object(loaded_dataframes, input_features, n_lags, k_neighbors):\n",
    "    \"\"\"\n",
    "    Prepares the full dataset and builds a PyG Data object.\n",
    "    \"\"\"\n",
    "    # Create lagged features for each station\n",
    "    lagged_dfs = {\n",
    "        name: create_lagged_features(df, input_features, n_lags)\n",
    "        for name, df in loaded_dataframes.items()\n",
    "    }\n",
    "    \n",
    "    # Ensure all DataFrames have the same time index for concatenation\n",
    "    first_df = next(iter(lagged_dfs.values()))\n",
    "    start_date = first_df['date'].iloc[0]\n",
    "    end_date = first_df['date'].iloc[-1]\n",
    "    common_dates = pd.date_range(start=start_date, end=end_date)\n",
    "    \n",
    "    # Extract and align features across all stations\n",
    "    station_names = list(lagged_dfs.keys())\n",
    "    \n",
    "    # Features will have shape (num_timesteps, num_stations, num_features)\n",
    "    num_timesteps = len(common_dates)\n",
    "    num_stations = len(station_names)\n",
    "    # num_features = n_lags * len(input_features) + len(input_features) + 2  # Lags + Current values + DOY + Coordinates\n",
    "    num_features = n_lags * len(input_features) + len(input_features) + 1 + 2\n",
    "    \n",
    "    all_features = np.zeros((num_timesteps, num_stations, num_features))\n",
    "    \n",
    "    # Static features: easting and northing are the same for all timesteps\n",
    "    static_features = np.zeros((num_stations, 2))\n",
    "    \n",
    "    for i, name in enumerate(station_names):\n",
    "        df = lagged_dfs[name]\n",
    "        df = df[df['date'].isin(common_dates)] # Align timesteps\n",
    "        \n",
    "        # Prepare dynamic features (time-series)\n",
    "        dynamic_feature_cols = [f for f in df.columns if 'lag' in f or f in input_features or f == 'day_of_year']\n",
    "        all_features[:, i, :-2] = df[dynamic_feature_cols].values # All but last 2 columns\n",
    "        \n",
    "        # Prepare static features (easting, northing)\n",
    "        static_features[i, :] = df[['easting', 'northing']].iloc[0].values\n",
    "    \n",
    "    # 3. Create the graph's edge index (adjacency matrix in PyG)\n",
    "    # Use k-nearest neighbors on the static coordinates\n",
    "    adj_matrix = kneighbors_graph(static_features, k_neighbors, mode='connectivity', include_self=True)\n",
    "    adj_coo = adj_matrix.tocoo()\n",
    "    edge_index = torch.tensor(np.vstack((adj_coo.row, adj_coo.col)), dtype=torch.long)\n",
    "    \n",
    "    # 4. Create the final PyG Data object\n",
    "    x = torch.tensor(all_features, dtype=torch.float)\n",
    "    \n",
    "    # Build PyG Data object\n",
    "    pyg_data = Data(x=x, edge_index=edge_index, pos=torch.tensor(static_features, dtype=torch.float))\n",
    "    \n",
    "    return pyg_data\n",
    "\n",
    "# Build PyG dataset\n",
    "pyg_dataset = build_pyg_data_object(loaded_dataframes, input_features, n_lags, k_neighbors)\n",
    "\n",
    "print(f\"Successfully created a PyG Data object with {pyg_dataset.num_nodes} nodes and {pyg_dataset.num_edges} edges.\")\n",
    "print(f\"Shape of node features (x): {pyg_dataset.x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Process Test Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build shapes/offset for given test station (no leakage)\n",
    "def build_baseline_artifacts_for_test_station(test_station, stations_geo, station_baselines, k=5, p=2):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      bar_s   : catchment shape (pd.Series, index=1..365)\n",
    "      mu_hat  : neighbour-weighted offset for the test station (float)\n",
    "      s_reg   : regional (node-wise) shape (pd.Series, index=1..365)\n",
    "    \"\"\"\n",
    "    # 1) Catchment shape (exclude test station)\n",
    "    shapes = [v['shape_curve'] for s, v in station_baselines.items() if s != test_station]\n",
    "    bar_s = pd.concat(shapes, axis=1).mean(axis=1)  # length 365, index = DOY (1..365)\n",
    "\n",
    "    # 2) Neighbour weights for offset/shape\n",
    "    coords = stations_geo.set_index('station_name')\n",
    "    xi, yi = coords.loc[test_station, ['easting', 'northing']]\n",
    "    nbr_names = [s for s in station_baselines.keys() if s != test_station]\n",
    "    nbr_xy = coords.loc[nbr_names, ['easting', 'northing']].values\n",
    "    d = np.linalg.norm(nbr_xy - np.array([xi, yi]), axis=1)\n",
    "\n",
    "    k = min(k, len(nbr_names))\n",
    "    idx = np.argsort(d)[:k]\n",
    "    nbr_names = np.array(nbr_names)[idx]\n",
    "    w = (d[idx]**-p); w = w / w.sum()  # IDW weights\n",
    "\n",
    "    # 3) Offset and regional shape\n",
    "    mu_hat = float(np.sum([w[m] * station_baselines[nbr_names[m]]['mean_level'] for m in range(k)]))\n",
    "    s_reg = sum(w[m] * station_baselines[nbr_names[m]]['shape_curve'] for m in range(k))\n",
    "    return bar_s, mu_hat, s_reg\n",
    "\n",
    "def create_test_station_pyg_obj(station_name, df_dir, geo_df):\n",
    "    \"\"\"\n",
    "    Loads, processes, and converts a single test station to a PyG Data object.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(df_dir, f\"{station_name}_trimmed.csv\")\n",
    "    \n",
    "    # Load and process the df\n",
    "    df = pd.read_csv(file_path, usecols=['Unnamed: 0', 'value'])\n",
    "    df.rename(columns={\"Unnamed: 0\": \"date\"}, inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    \n",
    "    # Handle leap years and create lagged features\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    leap_day_mask = (df['date'].dt.month == 2) & (df['date'].dt.day == 29)\n",
    "    df.loc[leap_day_mask, 'day_of_year'] = 59\n",
    "    after_leap_day_mask = (df['date'].dt.is_leap_year) & (df['day_of_year'] > 59)\n",
    "    df.loc[after_leap_day_mask, 'day_of_year'] -= 1\n",
    "    \n",
    "    # # Add baseline predictions col\n",
    "    # doy_mean_curve = df.groupby('day_of_year')['value'].mean()\n",
    "    # df['baseline_pred'] = df['day_of_year'].map(doy_mean_curve)\n",
    "    # df['baseline_pred'].fillna(method='ffill', inplace=True)\n",
    "    # df['baseline_pred'].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    # Build baseline from training stations (exclude the test station) → NO LEAKAGE\n",
    "    bar_s, mu_hat, s_reg = build_baseline_artifacts_for_test_station(\n",
    "        station_name, geo_df, station_baselines, k=5, p=2\n",
    "    )\n",
    "\n",
    "    # Choose ONE of the two baselines:\n",
    "    # (i) Climatology: catchment-wide shape + site offset\n",
    "    df['baseline_pred'] = mu_hat + df['day_of_year'].map(bar_s).astype(float)\n",
    "    \n",
    "    bar_s.plot(); print(bar_s.min(), bar_s.max(), bar_s.std())\n",
    "\n",
    "\n",
    "    # (ii) Regional seasonal: node-wise shape + site offset\n",
    "    # df['baseline_pred'] = mu_hat + df['day_of_year'].map(s_reg).astype(float)\n",
    "\n",
    "    # Build lags to replicate training station pipeline for consistnecy\n",
    "    df_with_lags = create_lagged_features(df, input_features, n_lags)\n",
    "    \n",
    "    # Get static features\n",
    "    coords = geo_df[geo_df['station_name'] == station_name]\n",
    "    static_features = coords[['easting', 'northing']].values\n",
    "    \n",
    "    # Prepare dynamic features (no edges needed for a single node)\n",
    "    dynamic_feature_cols = [f for f in df_with_lags.columns if 'lag' in f or f in input_features or f == 'day_of_year']\n",
    "    \n",
    "    # enforce deterministic col order\n",
    "    lag_cols_val  = [f'value_lag{l}' for l in range(1, n_lags+1)]\n",
    "    lag_cols_base = [f'baseline_pred_lag{l}' for l in range(1, n_lags+1)]\n",
    "    dynamic_feature_cols = ['value', 'baseline_pred', 'day_of_year'] + lag_cols_val + lag_cols_base\n",
    "    \n",
    "    # Create tensors for PyG Data object\n",
    "    x = torch.tensor(df_with_lags[dynamic_feature_cols].values, dtype=torch.float)\n",
    "    y = torch.tensor(df_with_lags['value'].values, dtype=torch.float)\n",
    "    pos = torch.tensor(static_features, dtype=torch.float)\n",
    "    \n",
    "    # Create the PyG Data object with a single node and no edges\n",
    "    pyg_data = Data(x=x.unsqueeze(1), y=y.unsqueeze(1), pos=pos)\n",
    "    \n",
    "    return pyg_data\n",
    "\n",
    "# Execute the function to create the test station data object\n",
    "test_station_data = create_test_station_pyg_obj(test_station, station_df_dir, stations_geo)\n",
    "test_station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = pyg_dataset.x.shape[2] \n",
    "# target_feature_idx = input_features.index('value') + (n_lags * len(input_features))\n",
    "target_feature_idx = 0  # Using deterministically set indices now\n",
    "\n",
    "def create_time_series_batches(data, window_size, horizon):\n",
    "    \"\"\"\n",
    "    Splits the PyG Data object into time series batches for training.\n",
    "    \"\"\"\n",
    "    num_timesteps, num_nodes, num_features = data.x.shape\n",
    "    features = data.x\n",
    "    x_batches, y_batches = [], []\n",
    "    \n",
    "    # Iterate through the time steps to create input-output pairs\n",
    "    for i in range(num_timesteps - window_size - horizon + 1):\n",
    "       \n",
    "        x_batch = features[i : i + window_size, :, :]   # Input: window of window_size timesteps of all features\n",
    "        y_batch = features[i + window_size + horizon - 1, :, target_feature_idx]  # Target: the value feature at the horizon-th step after the window\n",
    "        \n",
    "        x_batches.append(x_batch)\n",
    "        y_batches.append(y_batch)\n",
    "        \n",
    "    return torch.stack(x_batches), torch.stack(y_batches)\n",
    "\n",
    "# Define your training window and prediction horizon\n",
    "window_size = 7\n",
    "horizon = 1\n",
    "\n",
    "# Create the batches\n",
    "X, Y = create_time_series_batches(pyg_dataset, window_size, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define split (spatially held out)\n",
    "# X_train = pyg_dataset.x\n",
    "# Y_train = pyg_dataset.y\n",
    "\n",
    "# Randomly select some validation stations\n",
    "validation_stations = np.random.choice(\n",
    "    X.shape[2], 3, replace=False)\n",
    "\n",
    "# Get indices of training stations\n",
    "training_stations = np.array([i for i in range(X.shape[2]) if i not in validation_stations])\n",
    "\n",
    "X_train = X[:, :, training_stations, :]  # X_train is all timesteps and features for the training stations\n",
    "Y_train = Y[:, training_stations]  # Y_train is all targets for the training stations\n",
    "X_val = X[:, :, validation_stations, :]  # X_val is all timesteps and features for the validation stations\n",
    "Y_val = Y[:, validation_stations]  # Y_val is all targets for the validation stations\n",
    "\n",
    "# # use full X and Y for training\n",
    "# X, Y = create_time_series_batches(pyg_dataset, window_size, horizon)\n",
    "\n",
    "# Print the shapes to confirm\n",
    "logger.info(f\"Training shape (time, window, nodes, features): {X_train.shape}\")\n",
    "logger.info(f\"Validation shape (time, window, nodes, features): {X_val.shape}\")\n",
    "logger.info(f\"Training targets shape (time, nodes): {Y_train.shape}\")\n",
    "logger.info(f\"Validation targets shape (time, nodes): {Y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### GET TEST STATION METRICS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# --- Extract observed and baseline series (unaligned) ---\n",
    "y_true = test_station_data.y[:, 0].numpy()\n",
    "\n",
    "# Prefer indexing by name if you added feature_names; otherwise fall back to index 1\n",
    "try:\n",
    "    BASE_IDX = test_station_data.feature_names.index('baseline_pred')\n",
    "except Exception:\n",
    "    BASE_IDX = 1  # assumes you enforced ['value','baseline_pred','day_of_year', ...] earlier\n",
    "\n",
    "y_pred_clim = test_station_data.x[:, 0, BASE_IDX].numpy()   # climatology baseline\n",
    "\n",
    "# --- Metrics on the deployable (unaligned) baseline ---\n",
    "mae  = mean_absolute_error(y_true, y_pred_clim)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred_clim))\n",
    "nse  = 1 - np.sum((y_true - y_pred_clim)**2) / np.sum((y_true - y_true.mean())**2)\n",
    "\n",
    "# KGE (2009 variant): alpha = std ratio\n",
    "r     = np.corrcoef(y_true, y_pred_clim)[0, 1]\n",
    "beta  = y_pred_clim.mean() / y_true.mean()\n",
    "alpha = y_pred_clim.std(ddof=0) / y_true.std(ddof=0)\n",
    "kge   = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "\n",
    "logger.info(\"\\nClimatology baseline (UNALIGNED) on held-out test station:\")\n",
    "logger.info(f\"MAE={mae:.4f} m | RMSE={rmse:.4f} m | NSE={nse:.4f} | KGE={kge:.4f}\")\n",
    "logger.info(f\"KGE components: r={r:.4f}, alpha={alpha:.4f}, beta={beta:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### PLOT RESULTS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the test station dates\n",
    "file_path = os.path.join(station_df_dir, f\"{test_station}_trimmed.csv\")\n",
    "test_df = pd.read_csv(file_path, usecols=['Unnamed: 0', 'value'])\n",
    "test_df.rename(columns={\"Unnamed: 0\": \"date\"}, inplace=True)\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "# Align plot dates with the PyG test series (you trimmed by n_lags when building the object)\n",
    "n_lags = 7\n",
    "dates_to_plot = test_df['date'].values[n_lags:]\n",
    "\n",
    "# First-point alignment helper (for plotting only)\n",
    "def first_point_align_for_plot(y_pred, y_obs):\n",
    "    mask = np.isfinite(y_pred) & np.isfinite(y_obs)\n",
    "    if not np.any(mask):\n",
    "        raise ValueError(\"No overlapping finite values to align on.\")\n",
    "    i0 = np.flatnonzero(mask)[0]\n",
    "    diff = float(y_pred[i0] - y_obs[i0])   # baseline - observed at first valid point\n",
    "    return y_pred - diff, diff, i0\n",
    "\n",
    "# Align the climatology baseline to the first observation (for visual comparison)\n",
    "y_pred_fp, diff, i0 = first_point_align_for_plot(y_pred_clim, y_true)\n",
    "print(f\"First-point offset applied to climatology: {diff:.3f} m at index {i0}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(dates_to_plot, y_true,    label='Actual', color='blue', alpha=0.7)\n",
    "plt.plot(dates_to_plot, y_pred_fp, label='Climatology (first-point aligned)', \n",
    "         color='red', linestyle='--')\n",
    "plt.xlabel('Date'); plt.ylabel('Groundwater level (mAOD)')\n",
    "plt.title(f'Climatology Baseline vs Actual for {test_station}')\n",
    "plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "plt.show()\n",
    "# (Keep saving if you want)  # plt.savefig('.../climatology_baseline_plot.png')\n",
    "\n",
    "# plt.savefig(f'results/trained_models/eden/ablations/climatology_baseline/{test_station}_baseline_plot.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the FIRST-POINT ALIGNED baseline (diagnostic only)\n",
    "mae_fp  = mean_absolute_error(y_true, y_pred_fp)\n",
    "rmse_fp = np.sqrt(mean_squared_error(y_true, y_pred_fp))\n",
    "nse_fp  = 1 - np.sum((y_true - y_pred_fp)**2) / np.sum((y_true - y_true.mean())**2)\n",
    "\n",
    "r_fp = np.corrcoef(y_true, y_pred_fp)[0, 1]\n",
    "beta_fp  = y_pred_fp.mean() / y_true.mean()\n",
    "alpha_fp = y_pred_fp.std(ddof=0) / y_true.std(ddof=0)\n",
    "kge_fp = 1 - np.sqrt((r_fp - 1)**2 + (alpha_fp - 1)**2 + (beta_fp - 1)**2)\n",
    "\n",
    "logger.info(\"\\nClimatology baseline (FIRST-POINT ALIGNED) — diagnostic only:\")\n",
    "logger.info(f\"MAE={mae_fp:.4f} m | RMSE={rmse_fp:.4f} m | NSE={nse_fp:.4f} | KGE={kge_fp:.4f}\")\n",
    "logger.info(f\"KGE components: r={r_fp:.4f}, alpha={alpha_fp:.4f}, beta={beta_fp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make preds df\n",
    "df_predictions = pd.DataFrame({\n",
    "    'date': dates_to_plot,\n",
    "    'actuals': y_true,\n",
    "    'predictions_first_point_aligned': y_pred_fp\n",
    "})\n",
    "\n",
    "# save to csv\n",
    "output_dir = 'data/04_model/eden/model/test_results/baselines'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f'{test_station}_climatology_baseline.csv')\n",
    "df_predictions.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved climatology predictions to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Regional seasonal baseline ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose k neighbours and IDW exponent\n",
    "k, p = 5, 2\n",
    "\n",
    "coords = stations_geo.set_index('station_name')\n",
    "train_names = [s for s in station_baselines.keys() if s != test_station]\n",
    "\n",
    "# coords of test site and all training sites\n",
    "xi, yi = coords.loc[test_station, ['easting','northing']]\n",
    "nbr_xy  = coords.loc[train_names, ['easting','northing']].values\n",
    "d = np.linalg.norm(nbr_xy - np.array([xi, yi]), axis=1)\n",
    "\n",
    "# k nearest neighbours and normalised IDW weights\n",
    "k = min(k, len(train_names))\n",
    "idx = np.argsort(d)[:k]\n",
    "nbr_names = np.array(train_names)[idx]\n",
    "w = (d[idx] ** -p)\n",
    "w = w / w.sum()  # sum to 1\n",
    "\n",
    "# neighbour-weighted offset (no target history)\n",
    "mu_hat = float(sum(w[m] * station_baselines[nbr_names[m]]['mean_level']\n",
    "                   for m in range(k)))\n",
    "\n",
    "# neighbour-weighted seasonal shape (length 365 Series indexed by DOY)\n",
    "shapes_df = pd.concat([station_baselines[n]['shape_curve'] for n in nbr_names], axis=1)\n",
    "shapes_df.columns = nbr_names\n",
    "s_reg = shapes_df.dot(pd.Series(w, index=nbr_names))      # 365 x 1 Series\n",
    "\n",
    "# DOY for the test dates\n",
    "test_df['doy'] = test_df['date'].dt.dayofyear.clip(upper=365)\n",
    "\n",
    "# regional seasonal baseline in mAOD\n",
    "y_pred_reg_full = mu_hat + s_reg.reindex(test_df['doy']).to_numpy()\n",
    "\n",
    "# align to your y_true length (you trimmed by n_lags earlier)\n",
    "y_pred_reg = y_pred_reg_full[n_lags:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_point_align_for_plot(y_pred, y_obs):\n",
    "    \"\"\"\n",
    "    Align baseline to observations at the first index where both are finite.\n",
    "    Returns (aligned_baseline, offset_used, first_index_used).\n",
    "    \"\"\"\n",
    "    mask = np.isfinite(y_pred) & np.isfinite(y_obs)\n",
    "    if not np.any(mask):\n",
    "        raise ValueError(\"No overlapping finite values to align on.\")\n",
    "    i0 = np.flatnonzero(mask)[0]                 # first valid index\n",
    "    diff = float(y_pred[i0] - y_obs[i0])         # baseline - observed at first valid point\n",
    "    return y_pred - diff - 0.5, diff, i0\n",
    "\n",
    "# choose the baseline to visualise\n",
    "# y_pred = y_pred_clim      # climatology baseline (aligned to dates_to_plot)\n",
    "# y_pred = y_pred_reg       # regional seasonal baseline\n",
    "y_pred = y_pred_reg\n",
    "y_obs = y_true\n",
    "\n",
    "# align at the first point\n",
    "y_pred_fp, diff, i0 = first_point_align_for_plot(y_pred, y_obs)\n",
    "print(f\"First-point offset applied (baseline - observed at {i0}): {diff:.3f} m\")\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred_fp)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred_fp))\n",
    "nse = 1 - np.sum((y_true - y_pred_fp)**2) / np.sum((y_true - y_true.mean())**2)\n",
    "r = np.corrcoef(y_true, y_pred_fp)[0,1]\n",
    "beta = y_pred_fp.mean() / y_true.mean()\n",
    "alpha = y_pred_fp.std(ddof=0) / y_true.std(ddof=0)\n",
    "kge = 1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2)\n",
    "\n",
    "print(f\"MAE={mae:.3f} m, RMSE={rmse:.3f} m, NSE={nse:.3f}, KGE={kge:.3f}\")\n",
    "logger.info(f\"KGE components: r={r:.4f}, alpha={alpha:.4f}, beta={beta:.4f}\")\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(dates_to_plot, y_obs, label='Actual', color='blue', alpha=0.7)\n",
    "plt.plot(dates_to_plot, y_pred_fp, label='Baseline (first-point aligned)', color='red', linestyle='--')\n",
    "plt.xlabel('Date'); plt.ylabel('Groundwater level (mAOD)')\n",
    "plt.title(f'Baseline vs Actual for {test_station}')\n",
    "plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make preds df\n",
    "df_predictions = pd.DataFrame({\n",
    "    'date': dates_to_plot,\n",
    "    'actuals': y_true,\n",
    "    'predictions_first_point_aligned': y_pred_fp\n",
    "})\n",
    "\n",
    "# save to csv\n",
    "output_dir = 'data/04_model/eden/model/test_results/baselines'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f'{test_station}_regional_seasonal_baseline.csv')\n",
    "df_predictions.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved regional seasonal predictions to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
