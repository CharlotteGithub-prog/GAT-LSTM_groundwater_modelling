{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd01694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"All TODO\"\"\"\n",
    "# TODO: Add ground-truth data stations as different shape / colour nodes\n",
    "# TODO: Plot all timeseries data in full, knock out two unacceptable ones and then plot omitting suspect and unchecked data\n",
    "# TODO: interpolate missing data (with quick missingness checks - check DEVUL assignment feedback on this)\n",
    "# TODO: GNN iteration 1 inc. shallow aquifer then test metrics to decide on inclusion (RMSE, MSE etc). See Notion: Project Data > Preprocessing Steps > 0b. > Full Notes (Page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaffa19",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import folium\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import box\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"pyproj:\", pyproj.__version__)\n",
    "# print(\"shapely:\", shapely.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd0eee",
   "metadata": {},
   "source": [
    "Mesh building function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913dbbb",
   "metadata": {},
   "source": [
    "Create mesh using input shape file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d92332",
   "metadata": {},
   "source": [
    "Folium interactive map (open from html file for full view). Catchment boundary currently interactive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393949f2",
   "metadata": {},
   "source": [
    "Basic matplotlib map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb704954",
   "metadata": {},
   "source": [
    "Import Station Data using DEFRA API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5f590",
   "metadata": {},
   "source": [
    "**API Documentation notes:**\n",
    "\n",
    "1. The API calls that return readings data have a soft limit of 100,000 rows per-call which can be overridden by setting a _limit parameter. There is a hard limit of 2,000,000 rows, which cannot be overridden.\n",
    "2. The primary identifier for most stations uses a GUID style identifier called an SUID. These are used in the URL for the station and given as the value of the notation property in the station metadata.  \n",
    "    a. Wiski identifier (wiskiID) is also available for my subset of stations and data type  \n",
    "3. All monitoring stations can be filtered by name, location and other parameters. See https://environment.data.gov.uk/hydrology/doc/reference#stations-summary for full metadata details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe054efe",
   "metadata": {},
   "source": [
    "Convert raw csv files into catchment dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eddb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timeseries_to_dict(stations_df, col_order, data_dir=\"data/gwl/\"):\n",
    "    \"\"\"\n",
    "    Loads and cleans groundwater level timeseries data from CSV files.\n",
    "    \n",
    "    - Removes 'qcode' column if present.\n",
    "    - Ensures all columns in `col_order` are present (filling missing with NA).\n",
    "    - Reorders columns to match `col_order`.\n",
    "    - Returns a dictionary of cleaned DataFrames keyed by station name.\n",
    "    \"\"\" \n",
    "    # Save pandas dataframes to a dictionary by station name\n",
    "    time_series_data = {}\n",
    "\n",
    "    for index, row in stations_df.iterrows():\n",
    "        uri = row['measure_uri']\n",
    "        measure_id = uri.split(\"/\")[-1]\n",
    "        name = row['station_name'].title().strip().replace(\" \", \"_\")\n",
    "        \n",
    "        # Read CSV into placeholder df to manipulate\n",
    "        temp_df = pd.read_csv(f\"{data_dir}{measure_id}_readings.csv\", index_col=0, low_memory=False)\n",
    "        \n",
    "        # Drop 'qcode' column if present\n",
    "        if 'qcode' in temp_df.columns:\n",
    "            temp_df = temp_df.drop(columns=['qcode'])\n",
    "        \n",
    "        # Reorder columns (fill missing with NA)\n",
    "        for col in col_order:\n",
    "            if col not in temp_df.columns:\n",
    "                print(f'Warning: {name} did not contain {col}')\n",
    "                temp_df[col] = pd.NA\n",
    "        temp_df = temp_df[col_order]\n",
    "        \n",
    "        # Save to dictionary\n",
    "        time_series_data[name] = temp_df\n",
    "        \n",
    "    return time_series_data\n",
    "\n",
    "# Load timeseries CSVs from API into reference dict\n",
    "col_order = ['station_name', 'date', 'dateTime', 'value', 'quality', 'measure']\n",
    "time_series_data = load_timeseries_to_dict(stations_df, col_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc2261",
   "metadata": {},
   "source": [
    "Remove outliers and resample gwl to daily resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, z_thresh=4):\n",
    "    # Ensure numeric\n",
    "    df = df.copy()\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    \n",
    "    # Z-score outlier removal\n",
    "    z = (df['value'] - df['value'].mean()) / df['value'].std()\n",
    "    df.loc[z.abs() > z_thresh, 'value'] = pd.NA\n",
    "    return df\n",
    "\n",
    "def resample_daily_average(df):\n",
    "    df = df.copy()\n",
    "    df['dateTime'] = pd.to_datetime(df['dateTime'], errors='coerce')\n",
    "    df = df.dropna(subset=['dateTime'])\n",
    "    df = df.sort_values('dateTime')\n",
    "\n",
    "    # Set index for resampling\n",
    "    df = df.set_index('dateTime')\n",
    "    \n",
    "    # Define aggregation functions\n",
    "    agg_funcs = {\n",
    "        'station_name': 'first',\n",
    "        'date': 'first',\n",
    "        'value': 'mean',\n",
    "        'quality': lambda x: x.mode().iloc[0] if not x.mode().empty else pd.NA,\n",
    "        'measure': 'first'\n",
    "    }\n",
    "    \n",
    "    # Resample and aggregate\n",
    "    daily_df = df.resample('1D').agg(agg_funcs).reset_index()\n",
    "    \n",
    "    return daily_df\n",
    "\n",
    "# Clean data and resample to days\n",
    "daily_time_series = {}\n",
    "for station, df in time_series_data.items():\n",
    "    clean_df = remove_outliers(df)\n",
    "    daily_avg_df = resample_daily_average(clean_df)\n",
    "    daily_time_series[station] = daily_avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c63fe",
   "metadata": {},
   "source": [
    "Plot initial cleaned data as time series line graphs to begin to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cccb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(time_series_raw, station_name):\n",
    "    \"\"\"Resusable matplotlib time series plot\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 4))\n",
    "    time_series_raw['dateTime'] = pd.to_datetime(time_series_raw['dateTime'], errors='coerce')\n",
    "\n",
    "    # Define fixed colours for each quality level\n",
    "    quality_colors = {\n",
    "        'Good': '#70955F',\n",
    "        'Estimated': '#549EB1',\n",
    "        'Suspect': '#DF6607',\n",
    "        'Unchecked': '#e89c1d',\n",
    "        'Missing': '#9c9acd'\n",
    "    }\n",
    "    \n",
    "    # Plot using qualities score as legend\n",
    "    for quality, color in quality_colors.items():\n",
    "        temp = time_series_raw.copy()\n",
    "        temp['value'] = temp['value'].where(temp['quality'] == quality, pd.NA)\n",
    "        ax.plot(temp['dateTime'], temp['value'], label=quality, color=color, alpha=0.8)\n",
    "\n",
    "    # Apply auto locators and formatters to clean up ticks\n",
    "    locator = mdates.AutoDateLocator(minticks=10)\n",
    "    formatter = mdates.ConciseDateFormatter(locator)\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    ax.set_title(f'{station_name} Groundwater Level 2014-2025')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Groundwater Level (mAOD)')\n",
    "    ax.grid()\n",
    "    ax.legend(title=\"Quality\", loc=\"center left\", bbox_to_anchor=(1.01, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/eden_catchment/raw_timeseries_plots/{station_name}_raw_plot_.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot data at a daily resolution\n",
    "for station in daily_time_series:\n",
    "    print(f'Columns for {station}:\\n    {daily_time_series[station].columns}\\n    Total Entries: {len(daily_time_series[station])}\\n')\n",
    "    plot_timeseries(daily_time_series[station], station)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac87d9",
   "metadata": {},
   "source": [
    "Create geodataframes for stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9595c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.geometry import Point\n",
    "\n",
    "# # Convert to GeoDataFrame using WGS84 (lat/lon)\n",
    "# stations_gdf = gpd.GeoDataFrame(\n",
    "#     stations_df,\n",
    "#     geometry=gpd.points_from_xy(stations_df['lon'], stations_df['lat']),\n",
    "#     crs=\"EPSG:4326\"\n",
    "# )\n",
    "\n",
    "# # Reproject to match mesh CRS (British National Grid)\n",
    "# stations_gdf = stations_gdf.to_crs(\"EPSG:27700\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5738a1b",
   "metadata": {},
   "source": [
    "Snap stations to nearest mesh node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ca65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.ops import nearest_points\n",
    "# import shapely.geometry\n",
    "\n",
    "# # Rebuild mesh index\n",
    "# mesh_sindex = mesh_nodes_gdf.sindex\n",
    "\n",
    "# def find_nearest_node(station_point):\n",
    "#     # This is the key fix: pass geometry directly, not as a list\n",
    "#     nearest_idx = list(mesh_sindex.nearest(station_point, return_all=False))[0]\n",
    "\n",
    "#     nearest_row = mesh_nodes_gdf.iloc[nearest_idx]\n",
    "#     nearest_geom = nearest_row.geometry\n",
    "\n",
    "#     # Ensure it's a proper Shapely Point\n",
    "#     if hasattr(nearest_geom, '__geo_interface__') and not isinstance(nearest_geom, shapely.geometry.base.BaseGeometry):\n",
    "#         nearest_geom = shapely.geometry.shape(nearest_geom)\n",
    "\n",
    "#     return pd.Series({\n",
    "#         'nearest_node_id': int(nearest_row['node_id']),\n",
    "#         'nearest_geometry': nearest_geom\n",
    "#     })\n",
    "\n",
    "\n",
    "# # Apply snapping\n",
    "# stations_gdf[['nearest_node_id', 'nearest_geometry']] = stations_gdf.geometry.apply(find_nearest_node)\n",
    "# stations_gdf = stations_gdf.set_geometry('nearest_geometry').to_crs(\"EPSG:4326\")\n",
    "# stations_gdf['lat'] = stations_gdf.geometry.y\n",
    "# stations_gdf['lon'] = stations_gdf.geometry.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stations_gdf[['station_id', 'nearest_node_id', 'lat', 'lon']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "\n",
    "# # Recreate map centered on catchment\n",
    "# map_center = [mesh_nodes_gdf['lat'].mean(), mesh_nodes_gdf['lon'].mean()]\n",
    "# map = folium.Map(location=map_center, zoom_start=10, tiles=\"CartoDB positron\")\n",
    "\n",
    "# # Add snapped station locations\n",
    "# for _, row in stations_gdf.iterrows():\n",
    "#     folium.Marker(\n",
    "#         location=[row['lat'], row['lon']],\n",
    "#         popup=f\"{row['station_id']} → Node {row['nearest_node_id']}\"\n",
    "#     ).add_to(map)\n",
    "\n",
    "# # Optionally add mesh nodes\n",
    "# for _, row in mesh_nodes_gdf.iterrows():\n",
    "#     folium.CircleMarker(\n",
    "#         location=[row['lat'], row['lon']],\n",
    "#         radius=1,\n",
    "#         color=\"#354c7c\",\n",
    "#         fill=True,\n",
    "#         fill_opacity=0.6\n",
    "#     ).add_to(map)\n",
    "\n",
    "# # Save\n",
    "# map.save(\"figures/station_to_mesh_snapping.html\")\n",
    "# map\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
