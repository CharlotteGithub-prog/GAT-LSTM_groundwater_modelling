{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"All TODO\"\"\"\n",
    "# TODO: Add ground-truth data stations as different shape / colour nodes\n",
    "# TODO: Plot all timeseries data in full, knock out two unacceptable ones and then plot omitting suspect and unchecked data\n",
    "# TODO: interpolate missing data (with quick missingness checks - check DEVUL assignment feedback on this)\n",
    "#Â TODO: GNN iteration 1 inc. shallow aquifer then test metrics to decide on inclusion (RMSE, MSE etc). See Notion: Project Data > Preprocessing Steps > 0b. > Full Notes (Page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import folium\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import box\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"pyproj:\", pyproj.__version__)\n",
    "# print(\"shapely:\", shapely.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Mesh building function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Create mesh using input shape file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Folium interactive map (open from html file for full view). Catchment boundary currently interactive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Basic matplotlib map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Import Station Data using DEFRA API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "**API Documentation notes:**\n",
    "\n",
    "1. The API calls that return readings data have a soft limit of 100,000 rows per-call which can be overridden by setting a _limit parameter. There is a hard limit of 2,000,000 rows, which cannot be overridden.\n",
    "2. The primary identifier for most stations uses a GUID style identifier called an SUID. These are used in the URL for the station and given as the value of the notation property in the station metadata.  \n",
    "    a. Wiski identifier (wiskiID) is also available for my subset of stations and data type  \n",
    "3. All monitoring stations can be filtered by name, location and other parameters. See https://environment.data.gov.uk/hydrology/doc/reference#stations-summary for full metadata details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Convert raw csv files into catchment dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Remove outliers and resample gwl to daily resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Plot initial cleaned data as time series line graphs to begin to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hampel import hampel\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from src.utils.config_loader import load_project_config\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(message)s', # Uncomment for short logging\n",
    "    # format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', # Uncomment for full logging\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "\n",
    "def plot_timeseries(time_series_df, station_name, outlier_mask=None, plot_title_suffix=\"\"):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 4))\n",
    "\n",
    "    # Ensure dateTime is datetime type and value is numeric\n",
    "    time_series_df['dateTime'] = pd.to_datetime(time_series_df['dateTime'], errors='coerce')\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning if original_df is a slice\n",
    "    df_to_plot = time_series_df.copy()\n",
    "    df_to_plot['value'] = pd.to_numeric(df_to_plot['value'], errors='coerce')\n",
    "\n",
    "    # Define fixed colours for each quality level\n",
    "    quality_colors = {\n",
    "        'Good': '#70955F',\n",
    "        'Estimated': '#549EB1',\n",
    "        'Suspect': '#DF6607',\n",
    "        'Unchecked': '#e89c1d',\n",
    "        'Missing': '#9c9acd'\n",
    "    }\n",
    "\n",
    "    # Plot using qualities score as legend\n",
    "    for quality, color in quality_colors.items():\n",
    "        temp = df_to_plot.copy()\n",
    "        temp['value'] = temp['value'].where(temp['quality'] == quality, np.nan)\n",
    "        ax.plot(temp['dateTime'], temp['value'], label=quality, color=color, alpha=0.8, linewidth=1.5)\n",
    "\n",
    "    # If an outlier mask is provided, plot the outliers\n",
    "\n",
    "    if outlier_mask is not None and not outlier_mask.empty:\n",
    "        # Filter the ORIGINAL DataFrame to get just the outlier points' original values\n",
    "        # This is crucial for plotting the 'before' value of the outlier\n",
    "        original_values_for_outliers = time_series_df['value'][outlier_mask]\n",
    "        original_datetimes_for_outliers = time_series_df['dateTime'][outlier_mask]\n",
    "\n",
    "        # Plot markers for the detected outliers\n",
    "        ax.scatter(\n",
    "            original_datetimes_for_outliers,\n",
    "            original_values_for_outliers,\n",
    "            color='red',\n",
    "            marker='x',\n",
    "            s=50, # size of the marker\n",
    "            label='Detected Outlier',\n",
    "            zorder=5 # Ensures markers are on top\n",
    "        )\n",
    "\n",
    "    # Apply auto locators and formatters to clean up ticks\n",
    "    locator = mdates.AutoDateLocator(minticks=10)\n",
    "    formatter = mdates.ConciseDateFormatter(locator)\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    ax.set_title(f'{station_name} Groundwater Level 2014-2024{plot_title_suffix}')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Groundwater Level (mAOD)')\n",
    "    ax.grid(True)\n",
    "    ax.legend(title=\"Quality\", loc=\"center left\", bbox_to_anchor=(1.01, 0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"figures/eden_catchment/raw_timeseries_plots/{station_name}_raw_plot_.png\", dpi=300)\n",
    "\n",
    "    return plt\n",
    "\n",
    "def initial_threshold_cleaning(df: pd.DataFrame, station_name: str, iqr_multiplier: float = 5.0):\n",
    "    \"\"\"\n",
    "    Performs initial data type conversion, drops unparseable rows,\n",
    "    and applies a hard realistic range check based on IQR.\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # --- Dynamic Hard Range Check using IQR ---\n",
    "    clean_values = df_cleaned['value'].dropna()\n",
    "    \n",
    "    if not clean_values.empty:\n",
    "        Q1 = clean_values.quantile(0.25)\n",
    "        Q3 = clean_values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define limits as Q1 - (iqr_multiplier * IQR) and Q3 + (iqr_multiplier * IQR)\n",
    "        # Using 3.0 for iqr_multiplier is common for \"extreme\" outliers\n",
    "        lower_bound_iqr = Q1 - (iqr_multiplier * IQR)\n",
    "        upper_bound_iqr = Q3 + (iqr_multiplier * IQR)\n",
    "        \n",
    "        logger.info(f\"Station {station_name}: Automatically determined hard limits based on IQR (multiplier={iqr_multiplier}): [{lower_bound_iqr:.2f}-{upper_bound_iqr:.2f}].\")\n",
    "\n",
    "        print(\"Max value before IQR cleaning:\", df_cleaned['value'].max())\n",
    "\n",
    "        # Identify values outside this dynamic range\n",
    "        out_of_range_mask = (df_cleaned['value'] < lower_bound_iqr) | \\\n",
    "                            (df_cleaned['value'] > upper_bound_iqr)\n",
    "        \n",
    "        num_out_of_range = out_of_range_mask.sum()\n",
    "        if num_out_of_range > 0:\n",
    "            logger.info(f\"Station {station_name}: Identified {num_out_of_range} points outside IQR-based range and set to NaN.\")\n",
    "            df_cleaned.loc[out_of_range_mask, 'value'] = np.nan \n",
    "    else:\n",
    "        logger.warning(f\"Station {station_name}: No valid 'value' data found for IQR calculation. Skipping hard range check.\")\n",
    "        \n",
    "    return df_cleaned\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "test_dir = \"data/01_raw/eden/gwl_stations/API_data/copy/\"\n",
    "to_test = [4, 7, 8, 10, 14, 9]\n",
    "\n",
    "for n in to_test:\n",
    "    print(f'\\nTesting {n}...')\n",
    "    \n",
    "    test_path = test_dir + os.listdir(test_dir)[n]\n",
    "    test_csv = pd.read_csv(test_path)\n",
    "\n",
    "    # Ensure 'value' column is numeric and replace any non-numeric missing indicators with NaN\n",
    "    test_csv['value'] = pd.to_numeric(test_csv['value'], errors='coerce')\n",
    "\n",
    "    # Plot before Hampel outlier filtering\n",
    "    before_plot = plot_timeseries(test_csv, \"test_station\", plot_title_suffix=\" - Raw Data\")\n",
    "\n",
    "    #Â Apply initial threshold cleaning\n",
    "    test_csv_cleaned = initial_threshold_cleaning(test_csv, \"test_station\", iqr_multiplier=5.0)\n",
    "    # iqr_only_plot = plot_timeseries(test_csv_cleaned, \"test_station\", plot_title_suffix=\" - After IQR Cleaning\")\n",
    "\n",
    "    # Store original values to later detect changes made by Hampel filter\n",
    "    original_values = test_csv_cleaned['value'].copy()\n",
    "\n",
    "    # Apply Hampel filter\n",
    "    hampel_result = hampel(original_values, window_size=250, n_sigma=5.0)\n",
    "\n",
    "    # Get the filtered data Series (which now has outliers replaced by medians)\n",
    "    filtered_values = hampel_result.filtered_data\n",
    "\n",
    "    # --- Hampel outliers ---\n",
    "    hampel_outlier_mask = ~np.isclose(original_values, filtered_values, equal_nan=True)\n",
    "    hampel_outlier_mask &= original_values.notna()\n",
    "\n",
    "    # --- Create backup outlier mask for 'Unchecked'/'Suspect' near NaN gaps ---\n",
    "    eligible_quality_mask = test_csv['quality'].isin(['Unchecked', 'Suspect'])\n",
    "    eligible_values = original_values.where(eligible_quality_mask)\n",
    "\n",
    "    # Rolling median and residuals\n",
    "    rolling_median = eligible_values.rolling(window=30, min_periods=10, center=False).median()\n",
    "    residual = (eligible_values - rolling_median).abs()\n",
    "\n",
    "    # Threshold for significant deviation (tweakable)\n",
    "    residual_threshold = 0.3  # meters\n",
    "\n",
    "    # Identify values near a gap (NaN coming next)\n",
    "    gap_ahead_mask = eligible_values.shift(-1).isna() | eligible_values.shift(-2).isna()\n",
    "\n",
    "    # Combine into a new mask\n",
    "    residual_outlier_mask = residual > residual_threshold\n",
    "    end_of_segment_mask = gap_ahead_mask & residual_outlier_mask & eligible_quality_mask\n",
    "\n",
    "    # Combine Hampel and residual-based masks\n",
    "    final_outlier_mask = (hampel_outlier_mask | end_of_segment_mask) & (~test_csv['quality'].isin(['Good']))\n",
    "\n",
    "    test_csv_filtered = test_csv_cleaned.copy()\n",
    "\n",
    "    # Points Hampel handled: replace with filtered values\n",
    "    hampel_only_mask = hampel_outlier_mask & (~end_of_segment_mask)\n",
    "    test_csv_filtered.loc[hampel_only_mask, 'value'] = filtered_values.loc[hampel_only_mask]\n",
    "\n",
    "    # Points ONLY detected by residual check: set to NaN (or interpolate if you want)\n",
    "    residual_only_mask = end_of_segment_mask & (~hampel_outlier_mask)\n",
    "    test_csv_filtered.loc[residual_only_mask, 'value'] = np.nan  # or use interpolation\n",
    "\n",
    "    # Optional: interpolate missing data to fill cleaned points\n",
    "    # test_csv_filtered['value'] = test_csv_filtered['value'].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Log total number of replaced outliers\n",
    "    total_replaced_outliers = final_outlier_mask.sum()\n",
    "    logger.info(f\"Total {total_replaced_outliers} outliers detected and replaced by Hampel filter in '{test_path}'.\")\n",
    "\n",
    "    # Pass the filtered DataFrame and the generated outlier_mask, to highlight replaced points in plot\n",
    "    after_plot = plot_timeseries(test_csv_filtered, \"test_station\", outlier_mask=final_outlier_mask, plot_title_suffix=\" - Hampel Filtered\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Remove if outside check + quality marked as suspect or unchecked (use 4, 7, 8, 10, 14 & NB: 9 may be an issue)\n",
    "#Â First remove values outside a realistic range\n",
    "#Â Basic rule: If quality = unchecked/suspect and more then 3IQR from spline average line then mark as outlier. If good/estimated quality then flag for checking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Create geodataframes for stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.geometry import Point\n",
    "\n",
    "# # Convert to GeoDataFrame using WGS84 (lat/lon)\n",
    "# stations_gdf = gpd.GeoDataFrame(\n",
    "#     stations_df,\n",
    "#     geometry=gpd.points_from_xy(stations_df['lon'], stations_df['lat']),\n",
    "#     crs=\"EPSG:4326\"\n",
    "# )\n",
    "\n",
    "# # Reproject to match mesh CRS (British National Grid)\n",
    "# stations_gdf = stations_gdf.to_crs(\"EPSG:27700\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Snap stations to nearest mesh node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.ops import nearest_points\n",
    "# import shapely.geometry\n",
    "\n",
    "# # Rebuild mesh index\n",
    "# mesh_sindex = mesh_nodes_gdf.sindex\n",
    "\n",
    "# def find_nearest_node(station_point):\n",
    "#     # This is the key fix: pass geometry directly, not as a list\n",
    "#     nearest_idx = list(mesh_sindex.nearest(station_point, return_all=False))[0]\n",
    "\n",
    "#     nearest_row = mesh_nodes_gdf.iloc[nearest_idx]\n",
    "#     nearest_geom = nearest_row.geometry\n",
    "\n",
    "#     # Ensure it's a proper Shapely Point\n",
    "#     if hasattr(nearest_geom, '__geo_interface__') and not isinstance(nearest_geom, shapely.geometry.base.BaseGeometry):\n",
    "#         nearest_geom = shapely.geometry.shape(nearest_geom)\n",
    "\n",
    "#     return pd.Series({\n",
    "#         'nearest_node_id': int(nearest_row['node_id']),\n",
    "#         'nearest_geometry': nearest_geom\n",
    "#     })\n",
    "\n",
    "\n",
    "# # Apply snapping\n",
    "# stations_gdf[['nearest_node_id', 'nearest_geometry']] = stations_gdf.geometry.apply(find_nearest_node)\n",
    "# stations_gdf = stations_gdf.set_geometry('nearest_geometry').to_crs(\"EPSG:4326\")\n",
    "# stations_gdf['lat'] = stations_gdf.geometry.y\n",
    "# stations_gdf['lon'] = stations_gdf.geometry.x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
