{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd01694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"All TODO\"\"\"\n",
    "# TODO: Add ground-truth data stations as different shape / colour nodes\n",
    "# TODO: Plot all timeseries data in full, knock out two unacceptable ones and then plot omitting suspect and unchecked data\n",
    "# TODO: interpolate missing data (with quick missingness checks - check DEVUL assignment feedback on this)\n",
    "# TODO: GNN iteration 1 inc. shallow aquifer then test metrics to decide on inclusion (RMSE, MSE etc). See Notion: Project Data > Preprocessing Steps > 0b. > Full Notes (Page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaffa19",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import folium\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import box\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"pyproj:\", pyproj.__version__)\n",
    "# print(\"shapely:\", shapely.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd0eee",
   "metadata": {},
   "source": [
    "Mesh building function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28564091",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOCK MOVED! ###\n",
    "def build_mesh(shape_filepath, grid_resolution=1000):\n",
    "    \"\"\"\n",
    "    Builds a spatial mesh of nodes (centroids of grid cells) within the input catchment boundary.\n",
    "\n",
    "    Args:\n",
    "        shape_filepath (str): Path to the catchment boundary shapefile.\n",
    "        grid_resolution (int): Resolution of the grid (default 1 km resolution, EPSG:27700 is in meters).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mesh_nodes_table_df, mesh_nodes_gdf)\n",
    "            - mesh_nodes_table_df (pd.DataFrame): Node ID and coordinates.\n",
    "            - mesh_nodes_gdf (gpd.GeoDataFrame): Node ID, coordinates, and geometry (Point).\n",
    "    \"\"\"\n",
    "    # Load spatial boundary shape file\n",
    "    catchment_polygon = gpd.read_file(shape_filepath)\n",
    "    catchment_polygon = catchment_polygon.to_crs(epsg=27700) # Convert to British National Grid\n",
    "\n",
    "    # Check polygon geometry -> if shapefile has multiple features then dissolve them\n",
    "    if len(catchment_polygon) > 1:\n",
    "        print(\"\\nMultiple polygons found in the catchment boundary. Merging into a single geometry.\")\n",
    "        catchment_geometry = catchment_polygon.unary_union\n",
    "    else:\n",
    "        print(\"\\nSingle polygon found in the catchment boundary.\")\n",
    "        catchment_geometry = catchment_polygon.geometry.iloc[0]\n",
    "\n",
    "    # Get the bounds of the catchment and set grid resolution (in km)\n",
    "    minx, miny, maxx, maxy = catchment_polygon.total_bounds\n",
    "    print(f\"Catchment bounding box: min_x={minx}, min_y={miny}, max_x={maxx}, max_y={maxy}\")\n",
    "    \n",
    "    ## ---- Set up coordinate node mesh ----\n",
    "\n",
    "    # Generate bottom left coordinates of grid cells\n",
    "    x_coordinates_bottomleft = np.arange(minx, maxx + grid_resolution, grid_resolution)\n",
    "    y_coordinates_bottomleft = np.arange(miny, maxy + grid_resolution, grid_resolution)\n",
    "\n",
    "    print(f\"\\nNumber of x-coordinates (bottom-left): {len(x_coordinates_bottomleft)}\")\n",
    "    print(f\"Number of y-coordinates (bottom-left): {len(y_coordinates_bottomleft)}\")\n",
    "\n",
    "    # Initialise grid cell list and set up regular grid of points within the bounding box\n",
    "    grid_cells = []\n",
    "    for x in x_coordinates_bottomleft:\n",
    "        for y in y_coordinates_bottomleft:\n",
    "            cell = box(x, y, x + grid_resolution, y + grid_resolution)\n",
    "            grid_cells.append(cell)\n",
    "\n",
    "    print(f\"\\nGenerated {len(grid_cells)} grid cells within bounding box (before filtering).\")\n",
    "\n",
    "    grid_gdf = gpd.GeoDataFrame(geometry=grid_cells, crs=\"EPSG:27700\")\n",
    "\n",
    "    # Keep only grid cells that intersect the catchment\n",
    "    grid_intersected = gpd.overlay(grid_gdf, gpd.GeoDataFrame(geometry=[catchment_geometry], crs=\"EPSG:27700\"), how='intersection', keep_geom_type=True)\n",
    "    mesh_nodes_gdf = grid_intersected.copy() # Make copy to avoid SettingWithCopyWarning\n",
    "    print(f\"Filtered down to catchment boundary containing {len(grid_intersected)} nodes\")\n",
    "\n",
    "    ## ---- Convert to table ----\n",
    "\n",
    "    # Calculate the centroid of each (potentially clipped) grid cell\n",
    "    mesh_nodes_gdf['geometry'] = mesh_nodes_gdf.geometry.representative_point() # previously as .centroid\n",
    "\n",
    "    # Add original Easting/Northing coordinates (as in EPSG:27700)\n",
    "    mesh_nodes_gdf['easting'] = mesh_nodes_gdf.geometry.x\n",
    "    mesh_nodes_gdf['northing'] = mesh_nodes_gdf.geometry.y\n",
    "    mesh_nodes_gdf['node_id'] = range(len(mesh_nodes_gdf)) # UNID\n",
    "\n",
    "    # Convert to WGS84 (EPSG:4326) to add lat/lon for visualisations\n",
    "    mesh_nodes_4326 = mesh_nodes_gdf.to_crs(epsg=4326)\n",
    "    mesh_nodes_gdf['lon'] = mesh_nodes_4326.geometry.x\n",
    "    mesh_nodes_gdf['lat'] = mesh_nodes_4326.geometry.y\n",
    "\n",
    "    # Select the columns needed for node table\n",
    "    mesh_nodes_table = mesh_nodes_gdf[['node_id', 'easting', 'northing', 'lon', 'lat']]\n",
    "    \n",
    "    # Save the mesh nodes table and gdf to appropriate files\n",
    "    mesh_nodes_table.to_csv(\"data/eden_catchment_mesh_nodes.csv\", index=False)\n",
    "    mesh_nodes_gdf.to_file(\"data/eden_catchment_mesh_nodes.gpkg\", layer='mesh_nodes', driver='GPKG')  # GeoPackage\n",
    "    # mesh_nodes_gdf.to_file(\"data/eden_catchment_mesh_nodes.shp\", driver='ESRI Shapefile')  # Uncomment to save as Shapefile\n",
    "\n",
    "    print(\"\\nFirst few mesh nodes (centroids with coordinates):\")\n",
    "    print(mesh_nodes_table.head())\n",
    "    print(f\"\\nTotal number of mesh nodes (centroids) for the catchment: {len(mesh_nodes_table)}\\n\")\n",
    "    \n",
    "    return mesh_nodes_table, mesh_nodes_gdf, catchment_polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913dbbb",
   "metadata": {},
   "source": [
    "Create mesh using input shape file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOCK MOVED! ###\n",
    "# Define spatial boundary shape - file from https://nrfa.ceh.ac.uk/data/station/spatial_download/76007\n",
    "shape_filepath = \"data/eden_catchment_boundary/NRFA_catchments.shp\"\n",
    "mesh_nodes_table, mesh_nodes_gdf, catchment_polygon = build_mesh(shape_filepath, grid_resolution=1000)  # from graph_construction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d92332",
   "metadata": {},
   "source": [
    "Folium interactive map (open from html file for full view). Catchment boundary currently interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOCK MOVED! ###\n",
    "def plot_interactive_mesh(mesh_nodes_gdf, catchment_polygon, interactive=True):\n",
    "    \"\"\"\n",
    "    Build an interactive folium map of the mesh with map tile and node/boundary toggles.\n",
    "\n",
    "    Args:\n",
    "        mesh_nodes_gdf (gpd.GeoDataFrame): Node ID, coordinates, and geometry (Point).\n",
    "        catchment_polygon (gpd.GeoJson): Catchment area polygon.\n",
    "    \"\"\"\n",
    "    map_blue = '#354c7c'\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    esri = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Topo_Map/MapServer/tile/{z}/{y}/{x}\"\n",
    "    esri_attr = \"Tiles &copy; Esri &mdash; Esri, DeLorme, NAVTEQ\"\n",
    "    \n",
    "    # Static map when select, interactive otherwise\n",
    "    if not interactive:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        mesh_nodes_gdf.plot(ax=ax, color=map_blue, markersize=1.5)\n",
    "        catchment_polygon.plot(ax=ax, facecolor='none', edgecolor=map_blue, linewidth=1)\n",
    "\n",
    "        plt.xlabel(\"EPSG:27700 Easting (m)\")\n",
    "        plt.ylabel(\"EPSG:27700 Northing (m)\")\n",
    "        plt.title(\"Mesh Centroids for Eden Catchment (Resolution: 1km x 1km)\")\n",
    "        \n",
    "        plt.savefig(f\"figures/eden_catchment/mesh_map/{timestamp}_static_mesh.png\", dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        # Create base map centered on centre of mesh\n",
    "        map_center = [mesh_nodes_gdf['lat'].mean(), mesh_nodes_gdf['lon'].mean()]\n",
    "\n",
    "        # Define map tile layers\n",
    "        map = folium.Map(location=map_center, zoom_start=10, tiles=None)\n",
    "\n",
    "        # Add tile layers (esri visible by default, others in toggle)\n",
    "        folium.TileLayer(tiles=esri, attr=esri_attr, name='Topo', show=True).add_to(map)\n",
    "        folium.TileLayer('CartoDB positron', name='Light', show=False).add_to(map)\n",
    "        folium.TileLayer('CartoDB dark_matter', name='Dark', show=False).add_to(map)\n",
    "\n",
    "        # Add all node centroids as circle markers\n",
    "        mesh_layer = folium.FeatureGroup(name=\"Mesh Nodes\")\n",
    "        for col, row in mesh_nodes_gdf.iterrows():\n",
    "            folium.CircleMarker(location=[row['lat'], row['lon']], radius=1, color=map_blue,\n",
    "                                fill=True, fill_opacity=0.6).add_to(mesh_layer)\n",
    "            \n",
    "        # TODO: Add ground-truth data stations as different shape / colour nodes\n",
    "\n",
    "        # Add solid catchment boundary polygon to the map\n",
    "        folium.GeoJson(catchment_polygon, name='Catchment Boundary', \n",
    "                    style_function=lambda x: {'color': map_blue, 'weight': 2, 'fillColor': map_blue,\n",
    "                                                'fillOpacity': 0.15}).add_to(map)\n",
    "\n",
    "        # Add layer control to toggle catchment boundary, mesh and map tiles\n",
    "        mesh_layer.add_to(map)\n",
    "        folium.LayerControl().add_to(map)\n",
    "\n",
    "        # Save to html (unique by timestamp) and display in notebook\n",
    "        map.save(f\"figures/eden_catchment/mesh_map/mesh.html\")  # Previously {timestamp}_mesh.png -> removed due to cluttering\n",
    "        return map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ff301",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOCK MOVED! ###\n",
    "# Show map (select interactive vs static)\n",
    "interactive = True\n",
    "plot_interactive_mesh(mesh_nodes_gdf, catchment_polygon, interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393949f2",
   "metadata": {},
   "source": [
    "Basic matplotlib map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb704954",
   "metadata": {},
   "source": [
    "Import Station Data using DEFRA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31462c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOCK MOVED! ###\n",
    "# Convert alphanumeric OS grid ref to easting, northing, lat, lon\n",
    "def grid_ref_to_coords(grid_ref):\n",
    "    \"\"\"\n",
    "    Convert OS Grid references into espg 27700 easting and northing values, then into\n",
    "    espg 4326 coordinate ref system (longitude and latitude) for future visualisaations.\n",
    "    \"\"\"\n",
    "    # Transformer to WGS84 (for lat/long coordinate transformations)\n",
    "    transformer = Transformer.from_crs(\"epsg:27700\", \"epsg:4326\", always_xy=True)\n",
    "    \n",
    "    # Clean grid references\n",
    "    grid_ref = grid_ref.strip().upper()\n",
    "    \n",
    "    # Split to letter and numeric\n",
    "    letter_only = grid_ref[:2]  # e.g. \"NY\" (from NY123456)\n",
    "    numeric_only = grid_ref[2:]  # e.g. \"123456\" (from NY123456)\n",
    "    \n",
    "    # Check expected form (paired values)\n",
    "    if len(numeric_only) % 2 != 0:\n",
    "        raise ValueError(f\"Invalid grid reference: {grid_ref}\")\n",
    "    \n",
    "    easting_base, northing_base = grid_letters[letter_only]  # e.g. \"300000, 500000\" from NY (check ref file)\n",
    "    half = len(numeric_only) // 2  # Seperate easting and northing (e.g. \"123\", \"456\")\n",
    "    easting_offset = int(numeric_only[:half].ljust(5, '0'))  # e.g. \"123\" -> 12300 m\n",
    "    northing_offset = int(numeric_only[half:].ljust(5, '0'))  # e.g. \"456\" -> 45600 m\n",
    "    \n",
    "    easting = int(easting_base + easting_offset)  # e.g. 300000 + 12300 = 312300 m\n",
    "    northing = int(northing_base + northing_offset)  # e.g. 500000 + 45600 = 545600 m\n",
    "    lon, lat = transformer.transform(easting, northing)  # Transform 312300, 545600 to lat, long (epsg:4326)\n",
    "    \n",
    "    return pd.Series([easting, northing, lat, lon])\n",
    "\n",
    "# Use lookup file to create a dict of zipped grid letter and value lookup objects\n",
    "grid_lookup_df = pd.read_csv(\"data/os_grid_squares.csv\")\n",
    "grid_letters = dict(zip(grid_lookup_df['grid_letters'], zip(grid_lookup_df['easting_base'], grid_lookup_df['northing_base'])))\n",
    "\n",
    "# Apply to the station CSV\n",
    "stations_df = pd.read_csv(\"data/station_list.csv\")\n",
    "stations_df[['easting', 'northing', 'lat', 'lon']] = stations_df['grid_ref'].apply(grid_ref_to_coords)\n",
    "\n",
    "# Save as csv and check head and length\n",
    "stations_df.to_csv(\"data/station_list_with_coords.csv\", index=False)\n",
    "print(f\"Station location reference table:\\n\\n{stations_df.head()}\")\n",
    "print(f\"\\nTotal Stations: {len(stations_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5f590",
   "metadata": {},
   "source": [
    "**API Documentation notes:**\n",
    "\n",
    "1. The API calls that return readings data have a soft limit of 100,000 rows per-call which can be overridden by setting a _limit parameter. There is a hard limit of 2,000,000 rows, which cannot be overridden.\n",
    "2. The primary identifier for most stations uses a GUID style identifier called an SUID. These are used in the URL for the station and given as the value of the notation property in the station metadata.  \n",
    "    a. Wiski identifier (wiskiID) is also available for my subset of stations and data type  \n",
    "3. All monitoring stations can be filtered by name, location and other parameters. See https://environment.data.gov.uk/hydrology/doc/reference#stations-summary for full metadata details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOCK MOVED! ###\n",
    "# Get metadata - NB: Wiski is the water information system used by DEFRA (see doc notes)\n",
    "def get_station_metadata(wiski_id):\n",
    "    \n",
    "    params = {'wiskiID': wiski_id}\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # If response is good (200) return metadata items as single metadata column\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data['items']:\n",
    "            return data['items'][0]  # Only one list index per station\n",
    "    return None\n",
    "\n",
    "# Retrieving full metadata for all EA hydrological monitoring stations from the DEFRA Hyprology Explorer API\n",
    "base_url = \"https://environment.data.gov.uk/hydrology/id/stations\" \n",
    "stations_df['metadata'] = stations_df['station_id'].apply(get_station_metadata)\n",
    "\n",
    "# Convert metadata from string to dict using ast\n",
    "stations_df['metadata'] = stations_df['metadata'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "stations_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_measures(row):\n",
    "    \"\"\"See https://environment.data.gov.uk/hydrology/doc/reference#measures-summary for measures data\"\"\"\n",
    "    # Exract ID and metadata\n",
    "    metadata = row.get('metadata')\n",
    "    station_id = row.get('station_id')\n",
    "\n",
    "    # Continue if the required things exist in the correct form\n",
    "    if isinstance(metadata, dict) and '@id' in metadata:\n",
    "        \n",
    "        # Request the measures data\n",
    "        response = requests.get(f\"{metadata['@id']}/measures\", timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # If response code okay then return json response (items key) converted to dict\n",
    "            return response.json().get('items', [])\n",
    "        \n",
    "        else:\n",
    "            print(f\"API request failed for station {station_id}: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"Warning: Station {station_id} has no associated @id metadata\")\n",
    "    return []\n",
    "\n",
    "# Apply function across rows and display head to check\n",
    "stations_df['measures'] = stations_df.apply(get_station_measures, axis=1)\n",
    "\n",
    "# Convert measures from string to dict using ast and extract some initial useful info as flattened columns\n",
    "stations_df['metadata'] = stations_df['metadata'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "stations_df['station_name'] = stations_df['metadata'].apply(lambda x: x.get('label') if isinstance(x, dict) else None)\n",
    "stations_df['measure_uri'] = stations_df['measures'].apply(lambda x: x[0]['@id'] if x else None)\n",
    "\n",
    "# Save as csv and check head\n",
    "stations_df.to_csv(\"data/station_list_with_metadata_measures.csv\", index=False)\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75613203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_readings(measure_uri, startdate_str, enddate_str, max_per_request=50000):\n",
    "    \"\"\"\n",
    "    Download hydrological readings for each station from DEFRA Hydrology API within given dates.\n",
    "    Max requests set at 50000 with pagination used when readings exceeed this.\n",
    "    \"\"\"\n",
    "    all_measure_readings = []\n",
    "    offset = 0\n",
    "    \n",
    "    while True:\n",
    "        if not isinstance(measure_uri, str) or not measure_uri.startswith(\"http\"):\n",
    "            print(f\"Invalid URI: {measure_uri}\")\n",
    "            return []\n",
    "\n",
    "        params = {\n",
    "            '_limit': max_per_request,\n",
    "            '_offset': offset,  # offset for when number of readings exceeds max_per_request (pagination)\n",
    "            'min-dateTime': startdate_str,\n",
    "            'max-dateTime': enddate_str\n",
    "        }\n",
    "        \n",
    "        # Call API with defined parameters\n",
    "        response = requests.get(f\"{measure_uri}/readings\", params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            readings = response.json().get('items', [])\n",
    "            \n",
    "            # If readings are found append them to all_measure_readings list\n",
    "            if readings:\n",
    "                df = pd.DataFrame(readings)\n",
    "                all_measure_readings.append(df)\n",
    "\n",
    "                # Check if the max_readings_per_request was received (indicating more data might exist)\n",
    "                if len(readings) < max_per_request:\n",
    "                    break  # No more readings\n",
    "                else:\n",
    "                    offset += max_per_request # Move offset for next chunk to retrieve\n",
    "            else:\n",
    "                print(f\"    No data in: {measure_uri}\\n    Length: 0\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Failed for {measure_uri} - response code: {response.status_code}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    if all_measure_readings:\n",
    "        final_df = pd.concat(all_measure_readings, ignore_index=True)\n",
    "        print(f\"    Done: {measure_uri}\\n    Length: {len(final_df)}\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(f\"    No data in: {measure_uri}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "# Download station data for stations in catchment\n",
    "start = '2014-01-01T00:00:00'\n",
    "end = '2025-01-01T00:00:00'\n",
    "\n",
    "print(f\"Collecting data from {start[:-9]} to {end[:-9]}...\")\n",
    "\n",
    "count = 1\n",
    "\n",
    "for index, row in stations_df.iterrows():\n",
    "    # Pull station data\n",
    "    uri = row['measure_uri']\n",
    "    print(f\"\\n({count}/{len(stations_df)}) Processing measure: {uri+'/readings'}\")\n",
    "    df_readings = download_readings(uri, start, end)\n",
    "    \n",
    "    # Assign names to timeseries data using stations_df\n",
    "    df_readings['station_name'] = row['station_name'].title().strip()\n",
    "    \n",
    "    # Save returned dataframe\n",
    "    measure_id = uri.split(\"/\")[-1]\n",
    "    df_readings.to_csv(f\"data/gwl/{measure_id}_readings.csv\")\n",
    "    count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe054efe",
   "metadata": {},
   "source": [
    "Convert raw csv files into catchment dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eddb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timeseries_to_dict(stations_df, col_order, data_dir=\"data/gwl/\"):\n",
    "    \"\"\"\n",
    "    Loads and cleans groundwater level timeseries data from CSV files.\n",
    "    \n",
    "    - Removes 'qcode' column if present.\n",
    "    - Ensures all columns in `col_order` are present (filling missing with NA).\n",
    "    - Reorders columns to match `col_order`.\n",
    "    - Returns a dictionary of cleaned DataFrames keyed by station name.\n",
    "    \"\"\" \n",
    "    # Save pandas dataframes to a dictionary by station name\n",
    "    time_series_data = {}\n",
    "\n",
    "    for index, row in stations_df.iterrows():\n",
    "        uri = row['measure_uri']\n",
    "        measure_id = uri.split(\"/\")[-1]\n",
    "        name = row['station_name'].title().strip().replace(\" \", \"_\")\n",
    "        \n",
    "        # Read CSV into placeholder df to manipulate\n",
    "        temp_df = pd.read_csv(f\"{data_dir}{measure_id}_readings.csv\", index_col=0, low_memory=False)\n",
    "        \n",
    "        # Drop 'qcode' column if present\n",
    "        if 'qcode' in temp_df.columns:\n",
    "            temp_df = temp_df.drop(columns=['qcode'])\n",
    "        \n",
    "        # Reorder columns (fill missing with NA)\n",
    "        for col in col_order:\n",
    "            if col not in temp_df.columns:\n",
    "                print(f'Warning: {name} did not contain {col}')\n",
    "                temp_df[col] = pd.NA\n",
    "        temp_df = temp_df[col_order]\n",
    "        \n",
    "        # Save to dictionary\n",
    "        time_series_data[name] = temp_df\n",
    "        \n",
    "    return time_series_data\n",
    "\n",
    "# Load timeseries CSVs from API into reference dict\n",
    "col_order = ['station_name', 'date', 'dateTime', 'value', 'quality', 'measure']\n",
    "time_series_data = load_timeseries_to_dict(stations_df, col_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc2261",
   "metadata": {},
   "source": [
    "Remove outliers and resample gwl to daily resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, z_thresh=4):\n",
    "    # Ensure numeric\n",
    "    df = df.copy()\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    \n",
    "    # Z-score outlier removal\n",
    "    z = (df['value'] - df['value'].mean()) / df['value'].std()\n",
    "    df.loc[z.abs() > z_thresh, 'value'] = pd.NA\n",
    "    return df\n",
    "\n",
    "def resample_daily_average(df):\n",
    "    df = df.copy()\n",
    "    df['dateTime'] = pd.to_datetime(df['dateTime'], errors='coerce')\n",
    "    df = df.dropna(subset=['dateTime'])\n",
    "    df = df.sort_values('dateTime')\n",
    "\n",
    "    # Set index for resampling\n",
    "    df = df.set_index('dateTime')\n",
    "    \n",
    "    # Define aggregation functions\n",
    "    agg_funcs = {\n",
    "        'station_name': 'first',\n",
    "        'date': 'first',\n",
    "        'value': 'mean',\n",
    "        'quality': lambda x: x.mode().iloc[0] if not x.mode().empty else pd.NA,\n",
    "        'measure': 'first'\n",
    "    }\n",
    "    \n",
    "    # Resample and aggregate\n",
    "    daily_df = df.resample('1D').agg(agg_funcs).reset_index()\n",
    "    \n",
    "    return daily_df\n",
    "\n",
    "# Clean data and resample to days\n",
    "daily_time_series = {}\n",
    "for station, df in time_series_data.items():\n",
    "    clean_df = remove_outliers(df)\n",
    "    daily_avg_df = resample_daily_average(clean_df)\n",
    "    daily_time_series[station] = daily_avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c63fe",
   "metadata": {},
   "source": [
    "Plot initial cleaned data as time series line graphs to begin to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cccb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(time_series_raw, station_name):\n",
    "    \"\"\"Resusable matplotlib time series plot\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 4))\n",
    "    time_series_raw['dateTime'] = pd.to_datetime(time_series_raw['dateTime'], errors='coerce')\n",
    "\n",
    "    # Define fixed colours for each quality level\n",
    "    quality_colors = {\n",
    "        'Good': '#70955F',\n",
    "        'Estimated': '#549EB1',\n",
    "        'Suspect': '#DF6607',\n",
    "        'Unchecked': '#e89c1d',\n",
    "        'Missing': '#9c9acd'\n",
    "    }\n",
    "    \n",
    "    # Plot using qualities score as legend\n",
    "    for quality, color in quality_colors.items():\n",
    "        temp = time_series_raw.copy()\n",
    "        temp['value'] = temp['value'].where(temp['quality'] == quality, pd.NA)\n",
    "        ax.plot(temp['dateTime'], temp['value'], label=quality, color=color, alpha=0.8)\n",
    "\n",
    "    # Apply auto locators and formatters to clean up ticks\n",
    "    locator = mdates.AutoDateLocator(minticks=10)\n",
    "    formatter = mdates.ConciseDateFormatter(locator)\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    ax.set_title(f'{station_name} Groundwater Level 2014-2025')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Groundwater Level (mAOD)')\n",
    "    ax.grid()\n",
    "    ax.legend(title=\"Quality\", loc=\"center left\", bbox_to_anchor=(1.01, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/eden_catchment/raw_timeseries_plots/{station_name}_raw_plot_.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot data at a daily resolution\n",
    "for station in daily_time_series:\n",
    "    print(f'Columns for {station}:\\n    {daily_time_series[station].columns}\\n    Total Entries: {len(daily_time_series[station])}\\n')\n",
    "    plot_timeseries(daily_time_series[station], station)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac87d9",
   "metadata": {},
   "source": [
    "Create geodataframes for stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9595c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.geometry import Point\n",
    "\n",
    "# # Convert to GeoDataFrame using WGS84 (lat/lon)\n",
    "# stations_gdf = gpd.GeoDataFrame(\n",
    "#     stations_df,\n",
    "#     geometry=gpd.points_from_xy(stations_df['lon'], stations_df['lat']),\n",
    "#     crs=\"EPSG:4326\"\n",
    "# )\n",
    "\n",
    "# # Reproject to match mesh CRS (British National Grid)\n",
    "# stations_gdf = stations_gdf.to_crs(\"EPSG:27700\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5738a1b",
   "metadata": {},
   "source": [
    "Snap stations to nearest mesh node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ca65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.ops import nearest_points\n",
    "# import shapely.geometry\n",
    "\n",
    "# # Rebuild mesh index\n",
    "# mesh_sindex = mesh_nodes_gdf.sindex\n",
    "\n",
    "# def find_nearest_node(station_point):\n",
    "#     # This is the key fix: pass geometry directly, not as a list\n",
    "#     nearest_idx = list(mesh_sindex.nearest(station_point, return_all=False))[0]\n",
    "\n",
    "#     nearest_row = mesh_nodes_gdf.iloc[nearest_idx]\n",
    "#     nearest_geom = nearest_row.geometry\n",
    "\n",
    "#     # Ensure it's a proper Shapely Point\n",
    "#     if hasattr(nearest_geom, '__geo_interface__') and not isinstance(nearest_geom, shapely.geometry.base.BaseGeometry):\n",
    "#         nearest_geom = shapely.geometry.shape(nearest_geom)\n",
    "\n",
    "#     return pd.Series({\n",
    "#         'nearest_node_id': int(nearest_row['node_id']),\n",
    "#         'nearest_geometry': nearest_geom\n",
    "#     })\n",
    "\n",
    "\n",
    "# # Apply snapping\n",
    "# stations_gdf[['nearest_node_id', 'nearest_geometry']] = stations_gdf.geometry.apply(find_nearest_node)\n",
    "# stations_gdf = stations_gdf.set_geometry('nearest_geometry').to_crs(\"EPSG:4326\")\n",
    "# stations_gdf['lat'] = stations_gdf.geometry.y\n",
    "# stations_gdf['lon'] = stations_gdf.geometry.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stations_gdf[['station_id', 'nearest_node_id', 'lat', 'lon']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "\n",
    "# # Recreate map centered on catchment\n",
    "# map_center = [mesh_nodes_gdf['lat'].mean(), mesh_nodes_gdf['lon'].mean()]\n",
    "# map = folium.Map(location=map_center, zoom_start=10, tiles=\"CartoDB positron\")\n",
    "\n",
    "# # Add snapped station locations\n",
    "# for _, row in stations_gdf.iterrows():\n",
    "#     folium.Marker(\n",
    "#         location=[row['lat'], row['lon']],\n",
    "#         popup=f\"{row['station_id']} → Node {row['nearest_node_id']}\"\n",
    "#     ).add_to(map)\n",
    "\n",
    "# # Optionally add mesh nodes\n",
    "# for _, row in mesh_nodes_gdf.iterrows():\n",
    "#     folium.CircleMarker(\n",
    "#         location=[row['lat'], row['lon']],\n",
    "#         radius=1,\n",
    "#         color=\"#354c7c\",\n",
    "#         fill=True,\n",
    "#         fill_opacity=0.6\n",
    "#     ).add_to(map)\n",
    "\n",
    "# # Save\n",
    "# map.save(\"figures/station_to_mesh_snapping.html\")\n",
    "# map\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
