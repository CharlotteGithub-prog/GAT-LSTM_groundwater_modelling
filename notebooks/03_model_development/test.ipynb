{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Define test path\n",
    "# model_path = 'data/04_model/eden/model/best_model.pt' # Adjust this path as needed\n",
    "\n",
    "# # Load the state dictionary\n",
    "# try:\n",
    "#     state_dict = torch.load(model_path)\n",
    "#     print(\"Model state dictionary loaded successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model: {e}\")\n",
    "#     exit() # Exit if loading fails\n",
    "\n",
    "# # --- Inspect the contents ---\n",
    "\n",
    "# # Print all keys (layer names) in the state dictionary\n",
    "# print(\"\\nKeys in the state dictionary:\")\n",
    "# for key in state_dict.keys():\n",
    "#     print(key)\n",
    "\n",
    "# # Inspect the shape and device of a few params\n",
    "# print(\"\\nExample parameters from the state dictionary (first few keys):\\n\")Ÿ\n",
    "# for i, (key, value) in enumerate(state_dict.items()):\n",
    "#     if i >= 5: # Limit to first 5\n",
    "#         break\n",
    "#     print(f\"  Key: {key}\")\n",
    "#     print(f\"  Shape: {value.shape}\")\n",
    "#     print(f\"  Device: {value.device}\")\n",
    "#     print(f\"      Value (first 5 elements): {value.flatten()[:5].tolist()}\\n\")\n",
    "\n",
    "# if 'gat_layers.0.lin_src.weight' in state_dict:\n",
    "#     print(f\"\\nShape of first GAT layer weights: {state_dict['gat_layers.0.lin_src.weight'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from collections import Counter\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config\n",
    "from src.model.model_building import build_data_loader, instantiate_model_and_associated\n",
    "from src.utils.config_loader import load_project_config\n",
    "from src.graph_building.graph_construction import build_mesh, \\\n",
    "    define_catchment_polygon, define_graph_adjacency\n",
    "from src.preprocessing.data_partitioning import define_station_id_splits, \\\n",
    "    load_graph_tensors, build_pyg_object\n",
    "from src.preprocessing.model_feature_engineering import preprocess_gwl_features, \\\n",
    "    preprocess_shared_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Catchment area from country wide gdf\n",
    "define_catchment_polygon(\n",
    "    england_catchment_gdf_path=config[catchment]['paths']['gis_catchment_boundary'],\n",
    "    target_mncat=config[catchment]['target_mncat'],\n",
    "    catchment=catchment,\n",
    "    polygon_output_path=config[catchment]['paths']['gis_catchment_dir']\n",
    ")\n",
    "\n",
    "# Build catchment mesh\n",
    "mesh_nodes_table, mesh_nodes_gdf, mesh_cells_gdf_polygons, catchment_polygon = build_mesh(\n",
    "    shape_filepath=config[catchment]['paths']['gis_catchment_dir'],\n",
    "    output_path=config[catchment]['paths']['mesh_nodes_output'],\n",
    "    catchment=catchment,\n",
    "    grid_resolution=config[catchment]['preprocessing']['graph_construction']['grid_resolution']\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Build Mesh' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_edge_path = config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "# Create specific node_id column to merge\n",
    "directional_edge_weights[\"node_id\"] = range(0, len(directional_edge_weights))\n",
    "directional_edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in directional edge weights and mean elevation (not req. in main pipeline)\n",
    "directional_edge_path=config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "edge_attr_tensor, edge_index_tensor = define_graph_adjacency(\n",
    "    directional_edge_weights=directional_edge_weights,\n",
    "    elevation_geojson_path=config[catchment]['paths']['elevation_geojson_path'],\n",
    "    graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "    mesh_cells_gdf_polygons=mesh_cells_gdf_polygons,\n",
    "    epsilon_path=config[\"global\"][\"graph\"][\"epsilon\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Define Graph Adjacency' complete for {catchment} catchment.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tensors from file if needed\n",
    "# edge_index_tensor, edge_attr_tensor = load_graph_tensors(\n",
    "#     graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "#     catchment=catchment\n",
    "# )\n",
    "\n",
    "# Load main_df_full from file if needed\n",
    "load_path = config[catchment][\"paths\"][\"final_df_path\"] + 'final_df.csv'\n",
    "main_df_full = pd.read_csv(load_path)\n",
    "main_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6a. Define Spatial Split for Observed Stations ---\n",
    "\n",
    "train_station_ids, val_station_ids, test_station_ids = define_station_id_splits(\n",
    "    main_df_full=main_df_full,\n",
    "    catchment=catchment,\n",
    "    test_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"test_station_shortlist\"],\n",
    "    val_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"val_station_shortlist\"],\n",
    "    random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "    perc_train=config[catchment][\"model\"][\"data_partioning\"][\"percentage_train\"],\n",
    "    perc_val=config[catchment][\"model\"][\"data_partioning\"][\"percentage_val\"],\n",
    "    perc_test=config[catchment][\"model\"][\"data_partioning\"][\"percentage_test\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'define station splits' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6b. Preprocess (Standardise, one hot encode, round to 4dp) all shared features (not GWL) ---\n",
    "\n",
    "processed_df, shared_scaler, shared_encoder, gwl_feats = preprocess_shared_features(\n",
    "    main_df_full=main_df_full,\n",
    "    catchment=catchment,\n",
    "    random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "    violin_plt_path=config[catchment][\"visualisations\"][\"violin_plt_path\"],\n",
    "    scaler_dir = config[catchment][\"paths\"][\"scalers_dir\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Preprocess Final Shared Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6c. Preprocess all GWL features using training data only ---\n",
    "\n",
    "processed_df, gwl_scaler, gwl_encoder = preprocess_gwl_features(\n",
    "    processed_df=processed_df,\n",
    "    catchment=catchment,\n",
    "    train_station_ids=train_station_ids,\n",
    "    val_station_ids=val_station_ids,\n",
    "    test_station_ids=test_station_ids,\n",
    "    sentinel_value = config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "    scaler_dir = config[catchment][\"paths\"][\"scalers_dir\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Preprocess Final GWL Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timesteps_list = torch.load(config[catchment][\"paths\"][\"pyg_object_path\"])\n",
    "all_timesteps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7a. Build Data Loaders by Timestep ---\n",
    "\n",
    "full_dataset_loader = build_data_loader(\n",
    "    all_timesteps_list=all_timesteps_list,\n",
    "    batch_size = config[\"global\"][\"model\"][\"data_loader_batch_size\"],\n",
    "    shuffle = config[\"global\"][\"model\"][\"data_loader_shuffle\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Create PyG DataLoaders' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7b. Define Graph Neural Network Architecture ---\n",
    "\n",
    "model, device, optimizer, criterion = instantiate_model_and_associated(\n",
    "    all_timesteps_list=all_timesteps_list,\n",
    "    config=config,\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Instantiate GAT-LSTM Model' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "path = \"data/04_model/eden/model/pt_model/model_20250727-225058_GATTrue_LSTMFalse_GATH12_GATD0-4_GATHC64_GATOC64_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0005_SM0-0_E200_ESP25_LRSF0-5_LRSP8_MINLR1e-06_LD0-0001_GCMN1-0.pt\"\n",
    "iteration = 5\n",
    "\n",
    "best_model = model  # Assume model object already defined and moved to correct device\n",
    "best_model.load_state_dict(torch.load(path))\n",
    "best_model.eval()\n",
    "logger.info(f\"Loaded best model from {path}\")\n",
    "\n",
    "# Load target scaler\n",
    "scaler_path = \"data/03_graph/eden/scalers/target_scaler.pkl\"\n",
    "target_scaler = joblib.load(scaler_path)\n",
    "logger.info(f\"Loaded target scaler from: {scaler_path}\")\n",
    "\n",
    "scale = torch.tensor(target_scaler.scale_, device=device)\n",
    "mean = torch.tensor(target_scaler.mean_, device=device)\n",
    "\n",
    "# Initialize global LSTM state\n",
    "if best_model.run_LSTM:\n",
    "    lstm_state_store = {\n",
    "        'h': torch.zeros(best_model.num_layers_lstm, best_model.num_nodes, best_model.hidden_channels_lstm).to(device),\n",
    "        'c': torch.zeros(best_model.num_layers_lstm, best_model.num_nodes, best_model.hidden_channels_lstm).to(device)\n",
    "    }\n",
    "else:\n",
    "    lstm_state_store = None\n",
    "\n",
    "# Prepare for evaluation\n",
    "test_predictions_unscaled = []\n",
    "test_actuals_unscaled = []\n",
    "\n",
    "logger.info(\"--- Starting Model Evaluation on Test Set ---\")\n",
    "test_loop = tqdm(all_timesteps_list, desc=\"Evaluating on Test Set\", leave=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loop):\n",
    "        data = data.to(device)\n",
    "        test_mask = data.test_mask\n",
    "\n",
    "        if test_mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Model forward pass on full node set\n",
    "        predictions_all, (h_new, c_new), returned_node_ids = best_model(\n",
    "            x=data.x,\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=data.edge_attr,\n",
    "            current_timestep_node_ids=data.node_id,\n",
    "            lstm_state_store=lstm_state_store\n",
    "        )\n",
    "\n",
    "        # Update LSTM memory for current nodes\n",
    "        if best_model.run_LSTM:\n",
    "            lstm_state_store['h'][:, returned_node_ids, :] = h_new.detach()\n",
    "            lstm_state_store['c'][:, returned_node_ids, :] = c_new.detach()\n",
    "\n",
    "        # Filter predictions/targets to test nodes\n",
    "        preds_std = predictions_all[test_mask]\n",
    "        targets_std = data.y[test_mask]\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        preds_np = preds_std.cpu().numpy()\n",
    "        targets_np = targets_std.cpu().numpy()\n",
    "\n",
    "        preds_unscaled = target_scaler.inverse_transform(preds_np)\n",
    "        targets_unscaled = target_scaler.inverse_transform(targets_np)\n",
    "\n",
    "        test_predictions_unscaled.extend(preds_unscaled.flatten())\n",
    "        test_actuals_unscaled.extend(targets_unscaled.flatten())\n",
    "\n",
    "        if i < 5:  # Show first few predictions\n",
    "            print(\"Sample predictions (m AOD):\", preds_unscaled[:5].flatten())\n",
    "            print(\"Sample actuals     (m AOD):\", targets_unscaled[:5].flatten())\n",
    "\n",
    "# Final evaluation\n",
    "if len(test_actuals_unscaled) > 0:\n",
    "    loss_type = config[catchment][\"training\"][\"loss\"]\n",
    "\n",
    "    if loss_type == \"MAE\":\n",
    "        final_test_metric = mean_absolute_error(test_actuals_unscaled, test_predictions_unscaled)\n",
    "        logger.info(f\"\\n--- Final Test Set MAE (m AOD): {final_test_metric:.4f} ---\\n\")\n",
    "\n",
    "    elif loss_type == \"MSE\":\n",
    "        final_test_metric = mean_squared_error(test_actuals_unscaled, test_predictions_unscaled)\n",
    "        logger.info(f\"\\n--- Final Test Set MSE (m AOD²): {final_test_metric:.4f} ---\\n\")\n",
    "\n",
    "    else:\n",
    "        logger.warning(f\"Unrecognized loss type '{loss_type}' in config — skipping final metric calculation.\")\n",
    "else:\n",
    "    logger.warning(\"No test data found — check 'data.test_mask'.\")\n",
    "\n",
    "logger.info(\"--- Model Evaluation on Test Set Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the target scaler\n",
    "from joblib import load\n",
    "\n",
    "# Load target scaler (y, 'gwl_value') in\n",
    "scalers_dir = config[catchment][\"paths\"][\"scalers_dir\"]\n",
    "target_scaler_path = os.path.join(scalers_dir, \"target_scaler.pkl\")\n",
    "try:\n",
    "    target_scaler = load(target_scaler_path)\n",
    "    logger.info(f\"Successfully loaded target scaler from: {target_scaler_path}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"No target scaler found or error loading it: {e}\")\n",
    "    target_scaler = None\n",
    "\n",
    "# Convert both to np array\n",
    "test_predictions_np = np.array(test_predictions_unscaled).reshape(-1, 1)\n",
    "test_actuals_np = np.array(test_actuals_unscaled).reshape(-1, 1)\n",
    "\n",
    "# Confirm range for sanity check\n",
    "logger.info(f\"Sample prediction range: {test_predictions_np.min():.2f} to {test_predictions_np.max():.2f}\")\n",
    "logger.info(f\"Sample actual range:     {test_actuals_np.min():.2f} to {test_actuals_np.max():.2f}\")\n",
    "\n",
    "# Assign reshaped values to final arrays for plotting/metrics\n",
    "test_predictions_final = test_predictions_np\n",
    "test_actuals_final = test_actuals_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "final_test_mae = mean_absolute_error(test_actuals_final, test_predictions_final)\n",
    "unit_label = \"mAOD\" if target_scaler else \"standard units\"\n",
    "logger.info(f\"Final Test Set MAE: {final_test_mae:.4f} {unit_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.plot(test_actuals_final, label='Actual GWL Value', color='blue', alpha=0.7)\n",
    "plt.plot(test_predictions_final, label='Predicted GWL Value', color='red', alpha=0.7, linestyle='--')\n",
    "\n",
    "plt.title('Actual vs. Predicted Groundwater Levels on Test Set')\n",
    "plt.xlabel('Data Point Index (Sequential Timesteps/Stations)')\n",
    "plt.ylabel('Groundwater Level (m AOD)' if target_scaler else 'Standardised GWL')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Get model path base name and split extension of so save path matches model file\n",
    "base_name = os.path.basename(path)\n",
    "filename_no_ext, extension = os.path.splitext(base_name)\n",
    "save_path = \"results/trained_models/eden/\" + f\"{iteration}_\" + filename_no_ext\n",
    "# save_path = os.path.join(\"results/trained_models/eden/2_\", filename_no_ext)\n",
    "plt.savefig(save_path, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a style for plots (optional)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# Plot the predicted values\n",
    "plt.plot(test_predictions_unscaled, label='Predicted GWL Value', color='red', alpha=0.7, linestyle='--')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Predicted Groundwater Levels on Test Set')\n",
    "plt.xlabel('Data Point Index (Sequential Timesteps/Stations)')\n",
    "plt.ylabel('Groundwater Level (m AOD)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Generated plot of predicted values.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
