{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from collections import Counter\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config\n",
    "from src.graph_building.graph_construction import build_mesh, \\\n",
    "    define_catchment_polygon, define_graph_adjacency\n",
    "from src.preprocessing.data_partitioning import define_station_id_splits, \\\n",
    "    load_graph_tensors, build_pyg_object\n",
    "from src.preprocessing.model_feature_engineering import preprocess_gwl_features, \\\n",
    "    preprocess_shared_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timesteps_list = torch.load(config[catchment][\"paths\"][\"pyg_object_path\"])\n",
    "all_timesteps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks\n",
    "example_data_obj = all_timesteps_list[0]\n",
    "print(example_data_obj.x.shape)\n",
    "print(example_data_obj.train_mask.sum(), example_data_obj.val_mask.sum(), example_data_obj.test_mask.sum())\n",
    "print(example_data_obj.timestep)\n",
    "\n",
    "train_counts = sum(d.train_mask.sum().item() for d in all_timesteps_list)\n",
    "val_counts = sum(d.val_mask.sum().item() for d in all_timesteps_list)\n",
    "test_counts = sum(d.test_mask.sum().item() for d in all_timesteps_list)\n",
    "\n",
    "print(f\"\\nTotal (across all timesteps) — Train: {train_counts}, Val: {val_counts}, Test: {test_counts}\\n\")\n",
    "\n",
    "x_tensor = example_data_obj.x\n",
    "x_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
