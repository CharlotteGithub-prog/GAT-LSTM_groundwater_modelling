{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from permetrics.regression import RegressionMetric\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config, deep_format, expanduser_tree\n",
    "from src.model.model_building import build_data_loader, instantiate_model_and_associated\n",
    "from src.utils.config_loader import load_project_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up root directory paths in config\n",
    "raw_data_root = config[\"global\"][\"paths\"][\"raw_data_root\"]\n",
    "results_root = config[\"global\"][\"paths\"][\"results_root\"]\n",
    "\n",
    "# Reformat config roots\n",
    "config = deep_format(\n",
    "    config,\n",
    "    raw_data_root=raw_data_root,\n",
    "    results_root=results_root\n",
    ")\n",
    "config = expanduser_tree(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# TESTING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINE STATION RUNS ---\n",
    "\n",
    "iteration = \"\"  # Mark if runs are an ablation (leave as \"\" if not)\n",
    "start_slice = 0  # Define start of testing range (e.g. BGS_EV2)\n",
    "end_slice = 0  # Define end of testing range\n",
    "\n",
    "# Mark any to not run\n",
    "exclude = []\n",
    "\n",
    "station_model_map = {\n",
    "    \"ainstable_20250901_133721\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-184132_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"baronwood_20250901_115622\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-122212_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"bgs_ev2_20250901_120534\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-122918_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 730, \"end\": 0\n",
    "        },\n",
    "    \"castle_carrock_20250901_121422\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-123011_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"cliburn_town_20250901_122225\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-123952_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"coupland_20250901_123028\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-134427_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"croglin_20250827_150320\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250828-100815_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-00091_WD2e-05_SM0-13_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 1095\n",
    "        },\n",
    "    \"east_brownrigg_20250901_123714\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-143709_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"great_musgrave_20250901_124500\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-154212_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"hilton_20250901_125340\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-155047_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"longtown_20250901_130138\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-160358_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"renwick_20250901_131054\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-165608_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"scaleby_20250901_131941\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-170916_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        },\n",
    "    \"skirwith_20250901_132823\": {\n",
    "        \"model_path\": \"data/04_model/eden/model/pt_model/model_20250901-182247_GATTrue_LSTMTrue_GATH8_GATD0-22_GATHC48_GATOC48_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-0002_SM0-15_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD1e-05_GCMN1-0.pt\",\n",
    "        \"start\": 0, \"end\": 0\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CHANGE NOTHING PAST HERE FOR EACH STATION RUN!\n",
    "\"\"\"\n",
    "\n",
    "# Get testing count\n",
    "num_stations = len(station_model_map)\n",
    "\n",
    "for i, station in enumerate(station_model_map.keys()):\n",
    "    test_number = i\n",
    "    station_append = station\n",
    "    \n",
    "    station_input = station_model_map[station]\n",
    "    path = station_input[\"model_path\"]\n",
    "    \n",
    "    # Skip if no path yet\n",
    "    if not path:\n",
    "        logger.info(f\"({test_number} / {num_stations}) - {station} has no associated model path, skipping...\\n\")\n",
    "        continue\n",
    "\n",
    "    if station in exclude:\n",
    "        logger.info(f\"({test_number} / {num_stations}) - {station} marked for exclusion, skipping...\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Define necessary dirs and load model\n",
    "    test_station = station_append[:-16]\n",
    "    all_timesteps_list = torch.load(f\"data/03_graph/eden/PyG/all_timesteps_list_{station_append}.pt\")\n",
    "    scalers_dir = f\"data/03_graph/eden/scalers/{station_append}/\"\n",
    "\n",
    "    # Initialise (or reset in loop) dict to store metrics\n",
    "    metrics = {}\n",
    "\n",
    "    # --- 7a. Build Data Loaders by Timestep ---\n",
    "\n",
    "    full_dataset_loader = build_data_loader(\n",
    "        all_timesteps_list=all_timesteps_list,\n",
    "        batch_size = config[\"global\"][\"model\"][\"data_loader_batch_size\"],\n",
    "        shuffle = config[\"global\"][\"model\"][\"data_loader_shuffle\"],\n",
    "        catchment=catchment\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Pipeline Step 'Create PyG DataLoaders' complete for {catchment} catchment.\")\n",
    "\n",
    "    # --- 7b. Define Graph Neural Network Architecture ---\n",
    "\n",
    "    model, device, optimizer, criterion = instantiate_model_and_associated(\n",
    "        all_timesteps_list=all_timesteps_list,\n",
    "        config=config,\n",
    "        catchment=catchment\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Pipeline Step 'Instantiate GAT-LSTM Model' complete for {catchment} catchment.\")\n",
    "\n",
    "    mean_gwl_map = {\n",
    "        \"ainstable\": 84.6333698214874,\n",
    "        \"baronwood\": 85.8373720963633,\n",
    "        \"bgs_ev2\": 87.2166125260539,\n",
    "        \"castle_carrock\": 133.19521880854,\n",
    "        \"cliburn_town\": 110.805906037388,\n",
    "        \"coupland\": 135.670365012452,\n",
    "        \"croglin\": 167.758299820582,\n",
    "        \"east_brownrigg\": 106.74319765862,\n",
    "        \"great_musgrave\": 152.209015790055,\n",
    "        \"hilton\": 214.739017912584,\n",
    "        \"longtown\": 18.1315500711501,\n",
    "        \"renwick\": 177.683627274689,\n",
    "        \"scaleby\": 41.1093269995661,\n",
    "        \"skirwith\": 130.796279748829\n",
    "    }\n",
    "\n",
    "    mean_gwl = mean_gwl_map[test_station]\n",
    "\n",
    "    best_model = model  # Assume model object already defined and moved to correct device\n",
    "    best_model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "    best_model.eval()\n",
    "    logger.info(f\"Loaded best model from {path}\")\n",
    "\n",
    "    # Load target scaler\n",
    "    target_scaler_path = os.path.join(scalers_dir, \"target_scaler.pkl\")\n",
    "    target_scaler = joblib.load(target_scaler_path)\n",
    "    logger.info(f\"Loaded target scaler from: {target_scaler_path}\")\n",
    "\n",
    "    # Load gwl scaler\n",
    "    gwl_scaler_path = os.path.join(scalers_dir, \"gwl_scaler.pkl\")\n",
    "    gwl_scaler = joblib.load(gwl_scaler_path)\n",
    "    logger.info(f\"Loaded gwl scaler from: {gwl_scaler_path}\\n\")\n",
    "\n",
    "    target_scale = float(target_scaler.scale_[0])  # [0] as only gwl_value processed in this scaler\n",
    "    target_mean = float(target_scaler.mean_[0])\n",
    "\n",
    "    # Initialise global LSTM state\n",
    "    if best_model.run_LSTM:\n",
    "        lstm_state_store = {\n",
    "            'h': torch.zeros(best_model.num_layers_lstm, best_model.num_nodes, best_model.hidden_channels_lstm).to(device),\n",
    "            'c': torch.zeros(best_model.num_layers_lstm, best_model.num_nodes, best_model.hidden_channels_lstm).to(device)\n",
    "        }\n",
    "    else:\n",
    "        lstm_state_store = None\n",
    "\n",
    "    # Prepare lists for evaluation\n",
    "    test_predictions_unscaled = []\n",
    "    test_actuals_unscaled = []\n",
    "    fusion_alphas = [] \n",
    "    \n",
    "    # Collect per-timestep interpretability signals for this station\n",
    "    alpha_series = []\n",
    "    gamma_series = []\n",
    "    beta_series = []\n",
    "\n",
    "    logger.info(f\"--- Starting Model Evaluation on Test Set ---\\n\")\n",
    "    test_loop = tqdm(all_timesteps_list, desc=\"Evaluating on Test Set\", leave=False)\n",
    "\n",
    "    logger.info(f\"\\n\\nSTATION TEST: {station_append}\")\n",
    "    logger.info(f\"    Running Station {test_number} / {num_stations}..\\n\\n\")\n",
    "\n",
    "    # Run brief all_timesteps_list assertions to ensure no critical errors\n",
    "    assert len(all_timesteps_list) > 0, \"Empty timesteps list.\"\n",
    "    first = all_timesteps_list[0]\n",
    "    # assert first.x.size(1) == 70, \"Feature dimension different than expected — check column order.\"\n",
    "\n",
    "    dip_col = 53\n",
    "    mask_col = 61\n",
    "    drift_lim = 365\n",
    "\n",
    "    # Get burn in period (ensureing int dtype and appropriate length)\n",
    "    burn_in = int(config[\"global\"][\"pipeline_settings\"][\"burn_in\"])\n",
    "    burn_in = max(burn_in, 7)  # ensure it's at least 7 days\n",
    "\n",
    "    # Start testing loop\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in enumerate(test_loop):\n",
    "            data = data.to(device)\n",
    "            test_mask = data.test_mask\n",
    "            known_data_mask = (data.train_mask | data.val_mask | data.test_mask)\n",
    "            \n",
    "            # Skip timesteps with no nodes with known ground truth\n",
    "            if known_data_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Confirm assertation that there is exactly 1 test node\n",
    "            n_test = int(data.test_mask.sum().item())\n",
    "            assert n_test == 1, f\"Expected exactly 1 test node, got {n_test}.\"\n",
    "\n",
    "            # Model forward pass on full node set\n",
    "            predictions_all, (h_new, c_new), returned_node_ids = best_model(\n",
    "                x=data.x,  # data.x updated with warmed up / autoregressive lags\n",
    "                edge_index=data.edge_index,\n",
    "                edge_attr=data.edge_attr,\n",
    "                current_timestep_node_ids=data.node_id,\n",
    "                lstm_state_store=lstm_state_store\n",
    "            )\n",
    "            \n",
    "            # Update LSTM memory for current nodes\n",
    "            if best_model.run_LSTM:\n",
    "                lstm_state_store['h'][:, returned_node_ids, :] = h_new.detach()\n",
    "                lstm_state_store['c'][:, returned_node_ids, :] = c_new.detach()\n",
    "\n",
    "            # Filter predictions/targets to test nodes\n",
    "            preds_std = predictions_all[test_mask]\n",
    "            targets_std = data.y[test_mask]\n",
    "\n",
    "            # Inverse transform to original scale\n",
    "            preds_np = preds_std.cpu().numpy()\n",
    "            targets_np = targets_std.cpu().numpy()\n",
    "\n",
    "            preds_unscaled = target_scaler.inverse_transform(preds_np)\n",
    "            targets_unscaled = target_scaler.inverse_transform(targets_np)\n",
    "\n",
    "            test_predictions_unscaled.extend(preds_unscaled.flatten())\n",
    "            test_actuals_unscaled.extend(targets_unscaled.flatten())\n",
    "            \n",
    "            # Capture residual contribution relative to baseline (for interpretability)\n",
    "            if best_model.run_GAT and best_model.run_LSTM:\n",
    "                dbg = getattr(best_model, \"last_debug\", None)\n",
    "                if dbg is not None:\n",
    "                    residual = dbg.get(\"residual\", None)\n",
    "                    baseline = dbg.get(\"baseline\", None)\n",
    "                    if isinstance(residual, torch.Tensor) and isinstance(baseline, torch.Tensor):\n",
    "                        res_abs = torch.abs(residual[test_mask]).sum().item()\n",
    "                        base_abs = torch.abs(baseline[test_mask]).sum().item()\n",
    "                        if base_abs > 0:\n",
    "                            fusion_alphas.append(res_abs / base_abs)  # store rel contribution ratio\n",
    "                            \n",
    "            # capture alpha / gamma / beta for the single TEST node at this step\n",
    "            dbg = getattr(best_model, \"last_debug\", None)\n",
    "            if dbg is not None:\n",
    "                # alpha: (N,1) -> scalar for the test node\n",
    "                if dbg.get(\"alpha\", None) is not None:\n",
    "                    a = dbg[\"alpha\"][test_mask].squeeze().item()\n",
    "                    alpha_series.append(float(a))\n",
    "\n",
    "                # gamma/beta: (N, d_h) -> reduce over hidden dim (mean) for the test node\n",
    "                if dbg.get(\"gamma\", None) is not None:\n",
    "                    g = dbg[\"gamma\"][test_mask].mean(dim=1).item()\n",
    "                    gamma_series.append(float(g))\n",
    "                if dbg.get(\"beta\", None) is not None:\n",
    "                    b = dbg[\"beta\"][test_mask].mean(dim=1).item()\n",
    "                    beta_series.append(float(b))\n",
    "\n",
    "            if burn_in <= i < burn_in + 5:  # Show first few predictions\n",
    "                print(\"Sample predictions (m AOD):\", preds_unscaled[:5].flatten())\n",
    "                print(\"Sample actuals     (m AOD):\", targets_unscaled[:5].flatten())\n",
    "                \n",
    "    # --- DEFINED HELPER FUNCS FOR DIAGNOSTICS ---\n",
    "    \n",
    "    def _best_shift(y, yhat, max_lag=30):\n",
    "        # returns lag* (positive = prediction lags observation)\n",
    "        lags = range(-max_lag, max_lag+1)\n",
    "        corr = [np.corrcoef(y[max(0,l):len(yhat)+min(0,l)],\n",
    "                            yhat[max(0,-l):len(y)-max(0,l)])[0,1] for l in lags]\n",
    "        return lags[int(np.nanargmax(corr))], np.nanmax(corr)\n",
    "\n",
    "    def align_by_lag(y, yhat, lag):\n",
    "        \"\"\"\n",
    "        Align y (actuals) and yhat (predictions) given an integer lag.\n",
    "        lag > 0 : predictions lag observations by lag days (shift preds left)\n",
    "        lag < 0 : predictions lead observations by |lag| (shift preds right)\n",
    "        \"\"\"\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        yh = np.asarray(yhat, dtype=float)\n",
    "        \n",
    "        if lag > 0:\n",
    "            # drop the first 'lag' from y, drop the last 'lag' from yh\n",
    "            return y[lag:], yh[:-lag]\n",
    "        elif lag < 0:\n",
    "            L = -lag\n",
    "            # drop the last 'L' from y, drop the first 'L' from yh\n",
    "            return y[:-L], yh[L:]\n",
    "        else:\n",
    "            return y, yh\n",
    "            \n",
    "    # --- CLIP AND APPLY DRIFTS ---\n",
    "\n",
    "    # Clip burn-in (before applying)\n",
    "    preds_full = np.asarray(test_predictions_unscaled, dtype=float)\n",
    "    acts_full = np.asarray(test_actuals_unscaled, dtype=float)\n",
    "\n",
    "    if len(preds_full) <= burn_in:\n",
    "        raise ValueError(f\"Series too short ({len(preds_full)}) for burn-in {burn_in}.\")\n",
    "\n",
    "    burn_in = 0  # 730\n",
    "    preds_full = preds_full[burn_in:]\n",
    "    acts_full = acts_full[burn_in:]\n",
    "\n",
    "    # Determine lag (capped to ±max_lag in best_shift)\n",
    "    drift, r_star = _best_shift(acts_full, preds_full, max_lag=300)\n",
    "    logger.info(f\"Best shift = {drift} days; r = {r_star:.3f}\")\n",
    "\n",
    "    # Align safely\n",
    "    acts_aln, preds_aln = align_by_lag(acts_full, preds_full, drift)\n",
    "\n",
    "    if len(acts_aln) == 0 or len(preds_aln) == 0:\n",
    "        raise ValueError(f\"Empty arrays after alignment: len(acts)={len(acts_aln)}, len(preds)={len(preds_aln)}\")\n",
    "\n",
    "    test_actuals_unscaled = acts_aln\n",
    "    test_predictions_unscaled = preds_aln\n",
    "\n",
    "    # Add metrics to dict\n",
    "    metrics['best_shift'] = drift\n",
    "    metrics['burn_in'] = burn_in\n",
    "    \n",
    "    # --- GET GAT AND LSTM CONTRIBUTIONS ---\n",
    "    \n",
    "    # --- Clipping if needed (e.g. masked missingness) ---\n",
    "    \n",
    "    start_slice = station_input[\"start\"] - burn_in\n",
    "    end_slice = station_input[\"end\"]\n",
    "\n",
    "    if start_slice != 0 and end_slice != 0:\n",
    "        logger.info(f\"Trimming {start_slice} timesteps from start and {end_slice} \"\n",
    "                    f\"timesteps from end (Current length: {len(test_predictions_unscaled)})\")\n",
    "        test_actuals_unscaled = test_actuals_unscaled[start_slice:-end_slice]\n",
    "        test_predictions_unscaled = test_predictions_unscaled[start_slice:-end_slice]\n",
    "        logger.info(f\"Trimmed length: {len(test_predictions_unscaled)}\\n\")\n",
    "    elif start_slice != 0:\n",
    "        logger.info(f\"Trimming {start_slice} timesteps from start (Current length: {len(test_predictions_unscaled)})\")\n",
    "        test_actuals_unscaled = test_actuals_unscaled[start_slice:]\n",
    "        test_predictions_unscaled = test_predictions_unscaled[start_slice:]\n",
    "        logger.info(f\"Trimmed length: {len(test_predictions_unscaled)}\\n\")\n",
    "    elif end_slice != 0:\n",
    "        logger.info(f\"Trimming {end_slice} timesteps from end (Current length: {len(test_predictions_unscaled)})\")\n",
    "        test_actuals_unscaled = test_actuals_unscaled[:-end_slice]\n",
    "        test_predictions_unscaled = test_predictions_unscaled[:-end_slice]\n",
    "        logger.info(f\"Trimmed length: {len(test_predictions_unscaled)}\\n\")\n",
    "\n",
    "    # --- Final model prediction evaluation ---\n",
    "\n",
    "    if len(test_actuals_unscaled) > 0:\n",
    "        loss_type = config[catchment][\"training\"][\"loss\"]\n",
    "\n",
    "        if loss_type == \"MAE\":\n",
    "            final_test_metric = mean_absolute_error(test_actuals_unscaled, test_predictions_unscaled)\n",
    "            logger.info(f\"--- Final Test Set MAE (m AOD): {final_test_metric:.4f} ---\\n\")\n",
    "\n",
    "        elif loss_type == \"MSE\":\n",
    "            final_test_metric = mean_squared_error(test_actuals_unscaled, test_predictions_unscaled)\n",
    "            logger.info(f\"--- Final Test Set MSE (m AOD²): {final_test_metric:.4f} ---\\n\")\n",
    "\n",
    "        else:\n",
    "            logger.warning(f\"Unrecognized loss type '{loss_type}' in config — skipping final metric calculation.\\n\")\n",
    "    else:\n",
    "        logger.warning(\"No test data found — check 'data.test_mask'.\\n\")\n",
    "\n",
    "    logger.info(\"--- Model Evaluation on Test Set Complete ---\\n\")\n",
    "\n",
    "    # Calculate and display the global average residual contribution\n",
    "    if fusion_alphas:\n",
    "        avg_rel_contrib = np.mean(fusion_alphas) * 100\n",
    "        logger.info(\"--- Residual Contribution (on test node) ---\")\n",
    "        logger.info(f\"Average GAT Residual Contribution: {avg_rel_contrib:.2f}%\")\n",
    "        logger.info(f\"Average LSTM Contribution: {100 - avg_rel_contrib:.2f}%\")\n",
    "        logger.info(\"-------------------------------------------\\n\")\n",
    "            \n",
    "    # --- GET SAMPLE AND PREDICTION RANGES ---\n",
    "            \n",
    "        # Add metrics to dict\n",
    "        metrics['GAT_contribution'] = avg_rel_contrib\n",
    "        metrics['LSTM_contribution'] = avg_rel_contrib\n",
    "    \n",
    "    mean_error = np.mean(np.array(test_predictions_unscaled) - np.array(test_actuals_unscaled))\n",
    "    diff = np.mean(test_predictions_unscaled) - mean_gwl\n",
    "    logger.info(f\"Final Test Set Mean Error (Bias): {mean_error:.4f} m AOD [{diff}]\\n\")\n",
    "\n",
    "    # Add metrics to dict\n",
    "    metrics['mae_unadjusted'] = mean_error\n",
    "    \n",
    "    # Convert both to np array\n",
    "    test_predictions_np = np.array(test_predictions_unscaled).reshape(-1, 1)\n",
    "    test_actuals_np = np.array(test_actuals_unscaled).reshape(-1, 1)\n",
    "\n",
    "    # Confirm range (sanity checker)\n",
    "    logger.info(f\"Sample prediction range: {test_predictions_np.min():.2f} to {test_predictions_np.max():.2f}\")\n",
    "    logger.info(f\"Sample actual range:     {test_actuals_np.min():.2f} to {test_actuals_np.max():.2f}\\n\")\n",
    "\n",
    "    # Assign reshaped vals to final arrs for plotting and metrics\n",
    "    test_predictions_final = test_predictions_np\n",
    "    test_actuals_final = test_actuals_np\n",
    "    \n",
    "    # --- GET MAIN METRICS ---\n",
    "    \n",
    "    # diff = np.mean(test_predictions_final) - mean_gwl  # predictions - [\"gwl_mean\"]\n",
    "\n",
    "    final_test_mae = mean_absolute_error(test_actuals_final, test_predictions_final)\n",
    "    adjusted_mae = mean_absolute_error(test_actuals_final, test_predictions_final-diff)\n",
    "    unit_label = \"mAOD\" if target_scaler else \"standard units\"\n",
    "    evaluator = RegressionMetric(test_actuals_final, test_predictions_final)  # Before offset correction\n",
    "    adj_evaluator = RegressionMetric(test_actuals_final, test_predictions_final-diff)  # After offset correction\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    logger.info(f\"Final Test Set MAE: {final_test_mae:.4f} {unit_label} [Target 0.2 to 0.5 mAOD]\")\n",
    "    logger.info(f\"Final Adjusted MAE: {adjusted_mae:.4f} {unit_label} [Target 0.2 to 0.5 mAOD]\\n\")\n",
    "\n",
    "    # Root Mean Square Error (RMSE)\n",
    "    baseline_rmse = evaluator.root_mean_squared_error()\n",
    "    adjusted_rmse = adj_evaluator.root_mean_squared_error()\n",
    "    logger.info(f\"Baseline RMSE: {baseline_rmse:.4f} {unit_label} [Target 0.25 to 0.6 mAOD]\")\n",
    "    logger.info(f\"Adjusted RMSE: {adjusted_rmse:.4f} {unit_label} [Target 0.25 to 0.6 mAOD]\\n\")\n",
    "\n",
    "    # Coefficient of Determination (R^2)\n",
    "    logger.info(f\"Baseline R^2: {evaluator.coefficient_of_determination():.4f} [Target 0.80 or higher]\")\n",
    "    logger.info(f\"Adjusted R^2: {adj_evaluator.coefficient_of_determination():.4f} [Target 0.80 or higher]\\n\")\n",
    "\n",
    "    # Nash-Sutcliffe Efficiency (NSE)\n",
    "    baseline_nse = evaluator.nash_sutcliffe_efficiency()\n",
    "    adjusted_nse = adj_evaluator.nash_sutcliffe_efficiency()\n",
    "    logger.info(f\"Baseline NSE: {baseline_nse:.4f} [Target 0.75 or higher]\")\n",
    "    logger.info(f\"Adjusted NSE: {adjusted_nse:.4f} [Target 0.75 or higher]\\n\")\n",
    "\n",
    "    # Kling Gupta Efficiency (KGE)\n",
    "    baseline_kge = evaluator.kling_gupta_efficiency()\n",
    "    adjusted_kge = adj_evaluator.kling_gupta_efficiency()\n",
    "    logger.info(f\"Baseline KGE: {baseline_kge:.4f} [Target 0.75 or higher]\")\n",
    "    logger.info(f\"Adjusted KGE: {adjusted_kge:.4f} [Target 0.75 or higher]\\n\")\n",
    "    \n",
    "    # Add metrics to dict\n",
    "    metrics['final_metrics_baseline_mae'] = final_test_mae\n",
    "    metrics['final_metrics_baseline_rmse'] = baseline_rmse\n",
    "    metrics['final_metrics_baseline_nse'] = baseline_nse\n",
    "    metrics['final_metrics_baseline_kge'] = baseline_kge\n",
    "\n",
    "    metrics['final_metrics_adjusted_mae'] = adjusted_mae\n",
    "    metrics['final_metrics_adjusted_rmse'] = adjusted_rmse\n",
    "    metrics['final_metrics_adjusted_nse'] = adjusted_nse\n",
    "    metrics['final_metrics_adjusted_kge'] = adjusted_kge\n",
    "    \n",
    "    # --- GET COMPONENTS ---\n",
    "    \n",
    "    def calculate_kge_components(actuals, predictions):\n",
    "        \"\"\"Get KGE Component parts as individual values.\"\"\"\n",
    "        r = np.corrcoef(actuals.flatten(), predictions.flatten())[0, 1]\n",
    "        beta = np.mean(predictions.flatten()) / np.mean(actuals.flatten())\n",
    "        gamma = np.std(predictions.flatten()) / np.std(actuals.flatten())\n",
    "        return r, beta, gamma\n",
    "\n",
    "    # Using precalc'd adjusted predictions\n",
    "    adjusted_predictions = test_predictions_final - diff\n",
    "    r_actual, beta_actual, gamma_actual = calculate_kge_components(test_actuals_final, test_predictions_final)\n",
    "    r_adjusted, beta_adjusted, gamma_adjusted = calculate_kge_components(test_actuals_final, adjusted_predictions)\n",
    "\n",
    "    # Log component results (baseline)\n",
    "    print(f\"Baseline KGE Components:\")\n",
    "    print(f\"  Correlation (r): {r_actual:.4f};\")\n",
    "    print(f\"  Bias (beta): {beta_actual:.4f};\")\n",
    "    print(f\"  Variability (gamma): {gamma_actual:.4f}\\n\")\n",
    "\n",
    "    # Log component results (adjusted)\n",
    "    print(f\"Adjusted KGE Components:\")\n",
    "    print(f\"  Correlation (r): {r_adjusted:.4f};\")\n",
    "    print(f\"  Bias (beta): {beta_adjusted:.4f};\")\n",
    "    print(f\"  Variability (gamma): {gamma_adjusted:.4f}\\n\")\n",
    "\n",
    "    # Add metrics to dict\n",
    "    metrics['kge_components_baseline_corr'] = r_actual\n",
    "    metrics['kge_components_baseline_bias'] = beta_actual\n",
    "    metrics['kge_components_baseline_var'] = gamma_actual\n",
    "    metrics['kge_components_adjusted_corr'] = r_adjusted\n",
    "    metrics['kge_components_adjusted_bias'] = beta_adjusted\n",
    "    metrics['kge_components_adjusted_var'] = gamma_adjusted\n",
    "    \n",
    "    # Summarise alpha/gamma/beta over the test period for this station\n",
    "    def _median_iqr(arr):\n",
    "        arr = np.asarray(arr, float)\n",
    "        if arr.size == 0:\n",
    "            return np.nan, np.nan, np.nan, np.nan\n",
    "        q25, med, q75 = np.percentile(arr, [25, 50, 75])\n",
    "        return float(med), float(q75 - q25), float(arr.min()), float(arr.max())\n",
    "\n",
    "    alpha_med, alpha_iqr, alpha_min, alpha_max = _median_iqr(alpha_series)\n",
    "\n",
    "    # Keep γ and β unchanged (no min/max since they’re unbounded and noisier)\n",
    "    def _median_iqr_only(arr):\n",
    "        arr = np.asarray(arr, float)\n",
    "        if arr.size == 0:\n",
    "            return np.nan, np.nan\n",
    "        q25, med, q75 = np.percentile(arr, [25, 50, 75])\n",
    "        return float(med), float(q75 - q25)\n",
    "\n",
    "    gamma_med, gamma_iqr = _median_iqr_only(gamma_series)\n",
    "    beta_med,  beta_iqr  = _median_iqr_only(beta_series)\n",
    "\n",
    "    logger.info(f\"Fusion α — median: {alpha_med:.3f}, IQR: {alpha_iqr:.3f}, \"\n",
    "                f\"min: {alpha_min:.3f}, max: {alpha_max:.3f}\")\n",
    "    logger.info(f\"FiLM γ (scale) — median: {gamma_med:.3f}, IQR: {gamma_iqr:.3f}\")\n",
    "    logger.info(f\"FiLM β (shift) — median: {beta_med:.3f}, IQR: {beta_iqr:.3f}\\n\")\n",
    "\n",
    "    metrics['alpha_median'] = alpha_med\n",
    "    metrics['alpha_IQR'] = alpha_iqr\n",
    "    metrics['alpha_min'] = alpha_min\n",
    "    metrics['alpha_max'] = alpha_max\n",
    "    metrics['gamma_median'] = gamma_med\n",
    "    metrics['gamma_IQR'] = gamma_iqr\n",
    "    metrics['beta_median'] = beta_med\n",
    "    metrics['beta_IQR'] = beta_iqr\n",
    "        \n",
    "    # --- RUNNING DAILY RES PLOTS ---\n",
    "\n",
    "    diff = np.mean(test_predictions_final) - mean_gwl  # predictions - [\"gwl_mean\"]\n",
    "\n",
    "    # for x axis as date\n",
    "    start_date = config[\"global\"][\"data_ingestion\"][\"model_start_date\"]\n",
    "    test_start_date = pd.to_datetime(start_date) + pd.Timedelta(days=burn_in)  # don't plot burn in period\n",
    "    date_range = pd.date_range(start=test_start_date, periods=len(test_actuals_unscaled))\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.plot(date_range, test_actuals_final, label='Actual GWL Value', color='blue', alpha=0.7, linewidth=1)\n",
    "    plt.plot(date_range, test_predictions_final-diff, label='Predicted GWL Value', color='red', alpha=0.7, linewidth=1)  # , linestyle='--'\n",
    "\n",
    "    # Format the x-axis to show years\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "\n",
    "    plt.title(f'Actual vs. Predicted Groundwater Levels at {test_station}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Groundwater Level (m AOD)' if target_scaler else 'Standardised GWL')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Give slightly more room above and below than automatic\n",
    "    all_vals = np.concatenate([test_actuals_final, test_predictions_final-diff])\n",
    "    y_min = all_vals.min()\n",
    "    y_max = all_vals.max()\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min - y_range/3, y_max + y_range/3)\n",
    "\n",
    "    # Save plot\n",
    "    base_name = os.path.basename(path)\n",
    "    filename_no_ext, extension = os.path.splitext(base_name)\n",
    "    save_path = \"results/trained_models/eden/FINAL_daily/\" + f\"{test_station}_{iteration}_\" + filename_no_ext\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # --- SAVE PREDICTION RESULTS TO CSV ---\n",
    "    \n",
    "    logger.info(f\"Converting {test_station} results to dataframe for reference...\\n\")\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"ground_truth_values\": test_actuals_final.flatten(),\n",
    "        \"baseline_predictions\": test_predictions_final.flatten(),\n",
    "        \"drift_adjusted_predictions\": (test_predictions_final - diff).flatten()\n",
    "    })\n",
    "\n",
    "    # Ensure csv dir exists\n",
    "    csv_dir = os.path.join(config[catchment][\"paths\"][\"model_dir\"], \"test_results\")\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    csv_path = os.path.join(csv_dir, f\"{test_station}_{iteration}.csv\")\n",
    "    results_df.to_csv(csv_path)\n",
    "\n",
    "    logger.info(f\"{test_station} results saved to: {csv_path}\\n\")\n",
    "    \n",
    "    # --- SAVE METRICS DICT TO CSV ---\n",
    "    \n",
    "    # (Save raw alpha/gamma/beta time series)\n",
    "    diag_dir = os.path.join(\"data/04_model/eden/metrics\", \"diagnostics\")\n",
    "    os.makedirs(diag_dir, exist_ok=True)\n",
    "    pd.DataFrame({\n",
    "        \"alpha\": alpha_series,\n",
    "        \"gamma_mean_over_dh\": gamma_series,\n",
    "        \"beta_mean_over_dh\":  beta_series\n",
    "    }).to_csv(os.path.join(diag_dir, f\"{station_append}_alphagammabeta.csv\"), index=False)\n",
    "    \n",
    "    # Main Save\n",
    "    output_dir = \"data/04_model/eden/metrics/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f\"{station_append}_metrics.csv\")\n",
    "\n",
    "    if iteration != \"\":\n",
    "        output_file = os.path.join(output_dir, f\"{station_append}_metrics_{iteration}.csv\")\n",
    "\n",
    "    # Convert dictionary to pd df\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "\n",
    "    # If file doesn't exist then write the header\n",
    "    if os.path.exists(output_file):\n",
    "        metrics_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        metrics_df.to_csv(output_file, index=False)\n",
    "\n",
    "    logger.info(f\"Metrics saved to {output_file}\\n\")\n",
    "    \n",
    "    # --- RUNNING WEEKLY RES PLOTS ---\n",
    "\n",
    "    pred_daily = np.asarray(test_predictions_final).reshape(-1)\n",
    "    act_daily = np.asarray(test_actuals_final).reshape(-1)\n",
    "\n",
    "    # Calc means\n",
    "    k = 7\n",
    "    n_full = (len(act_daily) // k) * k\n",
    "    if n_full < len(act_daily):\n",
    "        logger.info(f\"Trimming {len(act_daily) - n_full} trailing day(s).\")\n",
    "    act_weekly = act_daily[:n_full].reshape(-1, k).mean(axis=1)\n",
    "    pred_weekly = pred_daily[:n_full].reshape(-1, k).mean(axis=1)\n",
    "\n",
    "    week0 = pd.to_datetime(config[\"global\"][\"data_ingestion\"][\"model_start_date\"]) + pd.Timedelta(days=burn_in)\n",
    "    week_dates = pd.date_range(start=week0, periods=len(act_weekly), freq=\"7D\")\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(act_weekly,  label='Actual GWL', alpha=0.8)\n",
    "    plt.plot(pred_weekly - diff, label='Predicted GWL', alpha=0.8)  # linestyle='--',\n",
    "    # plt.plot(pred_weekly + 0.3, label='Predicted GWL', alpha=0.8)  # linestyle='--',\n",
    "    plt.title('Actual vs. Predicted Groundwater Levels (Weekly Means)')\n",
    "    plt.xlabel('Week Index')\n",
    "    plt.ylabel('Groundwater Level (m AOD)' if target_scaler else 'Standardised GWL')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Set plot limits\n",
    "    all_vals = np.concatenate([act_weekly, pred_weekly - diff])\n",
    "    y_min = all_vals.min()\n",
    "    y_max = all_vals.max()\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min - y_range/2, y_max + y_range/2)\n",
    "    \n",
    "    weekly_base_name = os.path.basename(path)\n",
    "    weekly_filename_no_ext, extension = os.path.splitext(base_name)\n",
    "    weekly_save_path = \"results/trained_models/eden/FINAL_weekly/\" + f\"{test_station}_{iteration}_weekly_\" + filename_no_ext\n",
    "    plt.savefig(weekly_save_path, dpi=300)\n",
    "    \n",
    "    # --- VARIOUS RESOLUTIONS ---\n",
    "    \n",
    "    def _block_mean(arr: np.ndarray, k: int) -> np.ndarray:\n",
    "        \"\"\"Non-overlapping block means of length k, trimming any tail.\"\"\"\n",
    "        n = (len(arr) // k) * k\n",
    "        return arr[:n].reshape(-1, k).mean(axis=1)\n",
    "\n",
    "    def _metrics_row(y: np.ndarray, yhat: np.ndarray, mean_gwl_val: float):\n",
    "        \"\"\"Return a dict with the same metric fields as the daily CSV.\"\"\"\n",
    "        y = np.asarray(y, float).reshape(-1, 1)\n",
    "        yhat = np.asarray(yhat, float).reshape(-1, 1)\n",
    "\n",
    "        # baseline metrics\n",
    "        mae = mean_absolute_error(y, yhat)\n",
    "        ev = RegressionMetric(y, yhat)\n",
    "        rmse = ev.root_mean_squared_error()\n",
    "        nse = ev.nash_sutcliffe_efficiency()\n",
    "        kge = ev.kling_gupta_efficiency()\n",
    "\n",
    "        # simple offset correction to mean GWL (consistent with your daily logic)\n",
    "        diff_local = float(yhat.mean() - mean_gwl_val)\n",
    "        yhat_adj = (yhat - diff_local)\n",
    "        adj_mae = mean_absolute_error(y, yhat_adj)\n",
    "        ev_adj = RegressionMetric(y, yhat_adj)\n",
    "        adj_rmse = ev_adj.root_mean_squared_error()\n",
    "        adj_nse = ev_adj.nash_sutcliffe_efficiency()\n",
    "        adj_kge = ev_adj.kling_gupta_efficiency()\n",
    "\n",
    "        return {\n",
    "            \"best_shift\": metrics.get(\"best_shift\", np.nan),\n",
    "            \"burn_in\": metrics.get(\"burn_in\", np.nan),\n",
    "            \"GAT_contribution\": metrics.get(\"GAT_contribution\", np.nan),\n",
    "            \"LSTM_contribution\": metrics.get(\"LSTM_contribution\", np.nan),\n",
    "            \"mae_unadjusted\": float((yhat - y).mean()),  # keeps your existing field semantics\n",
    "            \"final_metrics_baseline_mae\": float(mae),\n",
    "            \"final_metrics_baseline_rmse\": float(rmse),\n",
    "            \"final_metrics_baseline_nse\": float(nse),\n",
    "            \"final_metrics_baseline_kge\": float(kge),\n",
    "            \"final_metrics_adjusted_mae\": float(adj_mae),\n",
    "            \"final_metrics_adjusted_rmse\": float(adj_rmse),\n",
    "            \"final_metrics_adjusted_nse\": float(adj_nse),\n",
    "            \"final_metrics_adjusted_kge\": float(adj_kge),\n",
    "        }\n",
    "\n",
    "    # --- Build daily series you already computed ---\n",
    "    pred_daily = np.asarray(test_predictions_final, dtype=float).reshape(-1)\n",
    "    act_daily  = np.asarray(test_actuals_final, dtype=float).reshape(-1)\n",
    "\n",
    "    # --- Weekly (7-day non-overlapping means) using arrays you already have for plotting ---\n",
    "    k7 = 7\n",
    "    n7 = (len(act_daily) // k7) * k7\n",
    "    if n7 >= k7:\n",
    "        act_7 = act_daily[:n7].reshape(-1, k7).mean(axis=1)\n",
    "        pred_7 = pred_daily[:n7].reshape(-1, k7).mean(axis=1)\n",
    "\n",
    "        weekly_row = _metrics_row(act_7, pred_7, mean_gwl)\n",
    "        weekly_file = output_file.replace(\"_metrics\", \"_metrics_7day\")\n",
    "        pd.DataFrame([weekly_row]).to_csv(\n",
    "            weekly_file,\n",
    "            mode=\"a\" if os.path.exists(weekly_file) else \"w\",\n",
    "            header=not os.path.exists(weekly_file),\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    # --- 30-day non-overlapping means ---\n",
    "    k30 = 30\n",
    "    act_30 = _block_mean(act_daily, k30)\n",
    "    pred_30 = _block_mean(pred_daily, k30)\n",
    "    if len(act_30) >= 1:\n",
    "        m30_row = _metrics_row(act_30, pred_30, mean_gwl)\n",
    "        m30_file = output_file.replace(\"_metrics\", \"_metrics_30day\")\n",
    "        pd.DataFrame([m30_row]).to_csv(\n",
    "            m30_file,\n",
    "            mode=\"a\" if os.path.exists(m30_file) else \"w\",\n",
    "            header=not os.path.exists(m30_file),\n",
    "            index=False,\n",
    "        )\n",
    "    \n",
    "    logger.info(f\"\\n\\nSTATION TEST: {station_append} COMPLETE. MOVING TO NEXT...\\n\\n\")\n",
    "    logger.info(f\"Station {test_number} / {num_stations} complete.\\n\\n\")\n",
    "    \n",
    "logger.info(f\"\\n\\nAll station tests complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "RAY TUNE RESULTS RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from ray.tune import ExperimentAnalysis\n",
    "# import os\n",
    "# import logging\n",
    "# import sys\n",
    "# import glob\n",
    "# import numpy as np\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(levelname)s - %(message)s',\n",
    "#     handlers=[logging.StreamHandler(sys.stdout)]\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# ray_tune_dir = \"data/04_model/eden/model/ray_tune_gwl\"\n",
    "\n",
    "# if not os.path.isdir(ray_tune_dir):\n",
    "#     logger.error(f\"Error: The specified directory does not exist or is not a directory: {ray_tune_dir}\")\n",
    "# else:\n",
    "#     try:\n",
    "#         abs_dir = os.path.abspath(ray_tune_dir)\n",
    "#         uri = f\"file://{abs_dir}\"\n",
    "\n",
    "#         analysis = ExperimentAnalysis(uri)\n",
    "#         df = analysis.dataframe()\n",
    "\n",
    "#         # Coerce numeric metrics (in case anything logged as strings)\n",
    "#         for col in ['val_loss', 'train_loss', 'epoch']:\n",
    "#             if col in df.columns:\n",
    "#                 df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "#         # Keep only rows with a valid trial_id\n",
    "#         if 'trial_id' not in df.columns:\n",
    "#             raise KeyError(\"Expected 'trial_id' column not found in Ray Tune results DataFrame.\")\n",
    "#         df = df[df['trial_id'].notna()].copy()\n",
    "\n",
    "#         # filter out errored trials (errored due to timeouts)\n",
    "#         if 'error' in df.columns:\n",
    "#             df = df[df['error'].isna()].copy()\n",
    "\n",
    "#         logger.info(f\"Total rows (iterations) loaded: {len(df)}\")\n",
    "#         n_trials = df['trial_id'].nunique()\n",
    "#         logger.info(f\"Trials represented: {n_trials}\")\n",
    "\n",
    "#         # Compute per-trial aggregates\n",
    "#         config_cols = [c for c in df.columns if c.startswith('config/')]\n",
    "#         keep_cols = ['trial_id', 'logdir'] + config_cols\n",
    "#         meta_first = (df[keep_cols]\n",
    "#                       .sort_values(['trial_id']) \n",
    "#                       .groupby('trial_id', as_index=False)\n",
    "#                       .first())\n",
    "        \n",
    "#         # Trials that never logged val_loss/train_los (timed out) become NaN -> drop for ranking\n",
    "#         agg = (df.groupby('trial_id')\n",
    "#                  .agg(min_val_loss=('val_loss', 'min'),\n",
    "#                       mean_val_loss=('val_loss', 'mean'),\n",
    "#                       min_train_loss=('train_loss', 'min'),\n",
    "#                       mean_train_loss=('train_loss', 'mean'),\n",
    "#                       last_epoch=('epoch', 'max'),\n",
    "#                       last_iter=('training_iteration', 'max'))\n",
    "#                  .reset_index())\n",
    "\n",
    "#         # Merge configs back in\n",
    "#         per_trial = meta_first.merge(agg, on='trial_id', how='left')\n",
    "\n",
    "#         # Keep only trials with at least some val_loss signal\n",
    "#         ranked = per_trial[per_trial['min_val_loss'].notna()].copy()\n",
    "#         if ranked.empty:\n",
    "#             raise RuntimeError(\"No trials have non-NaN 'val_loss'. Cannot rank.\")\n",
    "\n",
    "#         ranked = ranked.sort_values(['min_val_loss', 'mean_val_loss'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "#         best = ranked.iloc[0]\n",
    "#         trial_id = best['trial_id']\n",
    "\n",
    "#         # Pack nice dicts for logging\n",
    "#         best_config = {k: best[k] for k in config_cols if k in ranked.columns}\n",
    "#         best_metrics = {k: best[k] for k in ['min_val_loss', 'mean_val_loss', 'min_train_loss', 'mean_train_loss',\n",
    "#                                              'last_epoch', 'last_iter'] if k in ranked.columns}\n",
    "\n",
    "#         logger.info(\"\\n--- Best Trial (by min_val_loss, then mean_val_loss) ---\")\n",
    "#         logger.info(f\"Trial ID: {trial_id}\")\n",
    "#         logger.info(f\"Best metrics: {best_metrics}\")\n",
    "#         logger.info(f\"Best hyperparameters: {best_config}\")\n",
    "\n",
    "#         # Locate the corresponding PT model for this trial_id\n",
    "#         pt_root = os.path.abspath(\"data/04_model/eden/model/pt_model\")\n",
    "#         pattern = os.path.join(pt_root, f\"trial_{trial_id}\", \"pt_model\", \"*.pt\")\n",
    "#         pt_candidates = sorted(glob.glob(pattern))\n",
    "#         if pt_candidates:\n",
    "#             best_pt_model_path = pt_candidates[-1]  # choose last\n",
    "#             logger.info(f\"Best trial PT model: {best_pt_model_path}\")\n",
    "#         else:\n",
    "#             logger.warning(f\"No .pt model files found for trial_id={trial_id} with pattern: {pattern}\")\n",
    "\n",
    "#         # Show a compact table of the top 10 trials\n",
    "#         display_cols = (['trial_id'] + config_cols +\n",
    "#                         ['min_val_loss', 'mean_val_loss', 'min_train_loss', 'mean_train_loss', 'last_epoch'])\n",
    "#         display_cols = [c for c in display_cols if c in ranked.columns]\n",
    "#         top10 = ranked[display_cols].head(10)\n",
    "#         pd.set_option('display.max_columns', None)\n",
    "#         pd.set_option('display.width', 160)\n",
    "#         print(\"\\nTop 10 trials by validation loss:\")\n",
    "#         print(top10.to_string(index=False))\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred during analysis: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
