{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Define test path\n",
    "# model_path = 'data/04_model/eden/model/best_model.pt' # Adjust this path as needed\n",
    "\n",
    "# # Load the state dictionary\n",
    "# try:\n",
    "#     state_dict = torch.load(model_path)\n",
    "#     print(\"Model state dictionary loaded successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model: {e}\")\n",
    "#     exit() # Exit if loading fails\n",
    "\n",
    "# # --- Inspect the contents ---\n",
    "\n",
    "# # Print all keys (layer names) in the state dictionary\n",
    "# print(\"\\nKeys in the state dictionary:\")\n",
    "# for key in state_dict.keys():\n",
    "#     print(key)\n",
    "\n",
    "# # Inspect the shape and device of a few params\n",
    "# print(\"\\nExample parameters from the state dictionary (first few keys):\\n\")Ÿ\n",
    "# for i, (key, value) in enumerate(state_dict.items()):\n",
    "#     if i >= 5: # Limit to first 5\n",
    "#         break\n",
    "#     print(f\"  Key: {key}\")\n",
    "#     print(f\"  Shape: {value.shape}\")\n",
    "#     print(f\"  Device: {value.device}\")\n",
    "#     print(f\"      Value (first 5 elements): {value.flatten()[:5].tolist()}\\n\")\n",
    "\n",
    "# if 'gat_layers.0.lin_src.weight' in state_dict:\n",
    "#     print(f\"\\nShape of first GAT layer weights: {state_dict['gat_layers.0.lin_src.weight'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from collections import Counter\n",
    "from permetrics.regression import RegressionMetric\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config, deep_format, expanduser_tree\n",
    "from src.model.model_building import build_data_loader, instantiate_model_and_associated\n",
    "from src.utils.config_loader import load_project_config\n",
    "from src.graph_building.graph_construction import build_mesh, \\\n",
    "    define_catchment_polygon, define_graph_adjacency\n",
    "from src.preprocessing.data_partitioning import define_station_id_splits, \\\n",
    "    load_graph_tensors, build_pyg_object\n",
    "from src.preprocessing.model_feature_engineering import preprocess_gwl_features, \\\n",
    "    preprocess_shared_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up root directory paths in config\n",
    "raw_data_root = config[\"global\"][\"paths\"][\"raw_data_root\"]\n",
    "results_root = config[\"global\"][\"paths\"][\"results_root\"]\n",
    "\n",
    "# Reformat config roots\n",
    "config = deep_format(\n",
    "    config,\n",
    "    raw_data_root=raw_data_root,\n",
    "    results_root=results_root\n",
    ")\n",
    "config = expanduser_tree(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select Catchment area from country wide gdf\n",
    "# define_catchment_polygon(\n",
    "#     england_catchment_gdf_path=config[catchment]['paths']['gis_catchment_boundary'],\n",
    "#     target_mncat=config[catchment]['target_mncat'],\n",
    "#     catchment=catchment,\n",
    "#     polygon_output_path=config[catchment]['paths']['gis_catchment_dir']\n",
    "# )\n",
    "\n",
    "# # Build catchment mesh\n",
    "# mesh_nodes_table, mesh_nodes_gdf, mesh_cells_gdf_polygons, catchment_polygon = build_mesh(\n",
    "#     shape_filepath=config[catchment]['paths']['gis_catchment_dir'],\n",
    "#     output_path=config[catchment]['paths']['mesh_nodes_output'],\n",
    "#     catchment=catchment,\n",
    "#     grid_resolution=config[catchment]['preprocessing']['graph_construction']['grid_resolution']\n",
    "# )\n",
    "\n",
    "# logger.info(f\"Pipeline step 'Build Mesh' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional_edge_path = config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "# directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "# # Create specific node_id column to merge\n",
    "# directional_edge_weights[\"node_id\"] = range(0, len(directional_edge_weights))\n",
    "# directional_edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load in directional edge weights and mean elevation (not req. in main pipeline)\n",
    "# directional_edge_path=config[catchment][\"paths\"][\"direction_edge_weights_path\"]\n",
    "# directional_edge_weights = pd.read_csv(directional_edge_path)\n",
    "\n",
    "# edge_attr_tensor, edge_index_tensor = define_graph_adjacency(\n",
    "#     directional_edge_weights=directional_edge_weights,\n",
    "#     elevation_geojson_path=config[catchment]['paths']['elevation_geojson_path'],\n",
    "#     graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "#     mesh_cells_gdf_polygons=mesh_cells_gdf_polygons,\n",
    "#     epsilon_path=config[\"global\"][\"graph\"][\"epsilon\"],\n",
    "#     catchment=catchment\n",
    "# )\n",
    "\n",
    "# logger.info(f\"Pipeline step 'Define Graph Adjacency' complete for {catchment} catchment.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tensors from file if needed\n",
    "# edge_index_tensor, edge_attr_tensor = load_graph_tensors(\n",
    "#     graph_output_dir=config[catchment][\"paths\"][\"graph_data_output_dir\"],\n",
    "#     catchment=catchment\n",
    "# )\n",
    "\n",
    "# # Load main_df_full from file if needed\n",
    "# load_path = config[catchment][\"paths\"][\"final_df_path\"] + 'final_df.csv'\n",
    "# main_df_full = pd.read_csv(load_path)\n",
    "# main_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 6a. Define Spatial Split for Observed Stations ---\n",
    "\n",
    "# train_station_ids, val_station_ids, test_station_ids = define_station_id_splits(\n",
    "#     main_df_full=main_df_full,\n",
    "#     catchment=catchment,\n",
    "#     test_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"test_station_shortlist\"],\n",
    "#     val_station_shortlist=config[catchment][\"model\"][\"data_partioning\"][\"val_station_shortlist\"],\n",
    "#     random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "#     output_dir=config[catchment][\"paths\"][\"aux_dir\"],\n",
    "#     perc_train=config[catchment][\"model\"][\"data_partioning\"][\"percentage_train\"],\n",
    "#     perc_val=config[catchment][\"model\"][\"data_partioning\"][\"percentage_val\"],\n",
    "#     perc_test=config[catchment][\"model\"][\"data_partioning\"][\"percentage_test\"]\n",
    "# )\n",
    "\n",
    "# logger.info(f\"Pipeline Step 'define station splits' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 6b. Preprocess (Standardise, one hot encode, round to 4dp) all shared features (not GWL) ---\n",
    "\n",
    "# processed_df, shared_scaler, shared_encoder, gwl_feats = preprocess_shared_features(\n",
    "#     main_df_full=main_df_full,\n",
    "#     catchment=catchment,\n",
    "#     random_seed=config[\"global\"][\"pipeline_settings\"][\"random_seed\"],\n",
    "#     violin_plt_path=config[catchment][\"visualisations\"][\"violin_plt_path\"],\n",
    "#     scaler_dir = config[catchment][\"paths\"][\"scalers_dir\"],\n",
    "#     aux_dir=config[catchment][\"paths\"][\"aux_dir\"]\n",
    "# )\n",
    "\n",
    "# logger.info(f\"Pipeline Step 'Preprocess Final Shared Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 6c. Preprocess all GWL features using training data only ---\n",
    "\n",
    "# processed_df, gwl_scaler, gwl_encoder = preprocess_gwl_features(\n",
    "#     processed_df=processed_df,\n",
    "#     catchment=catchment,\n",
    "#     train_station_ids=train_station_ids,\n",
    "#     val_station_ids=val_station_ids,\n",
    "#     test_station_ids=test_station_ids,\n",
    "#     sentinel_value = config[\"global\"][\"graph\"][\"sentinel_value\"],\n",
    "#     scaler_dir = config[catchment][\"paths\"][\"scalers_dir\"]\n",
    "# )\n",
    "\n",
    "# logger.info(f\"Pipeline Step 'Preprocess Final GWL Features' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df_test = processed_df.drop(columns=['streamflow_total_m3', 'HOST_soil_class_freely_draining_soils', 'HOST_soil_class_high_runoff_(impermeable)', \n",
    "#                                                'HOST_soil_class_impeded_saturated_subsurface_flow', 'HOST_soil_class_peat_soils', 'aquifer_productivity_High',\n",
    "#                                                'aquifer_productivity_Low', 'aquifer_productivity_Mixed', 'aquifer_productivity_Moderate',\n",
    "#                                                'aquifer_productivity_nan']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timesteps_list = torch.load(config[catchment][\"paths\"][\"pyg_object_path\"])\n",
    "all_timesteps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7a. Build Data Loaders by Timestep ---\n",
    "\n",
    "full_dataset_loader = build_data_loader(\n",
    "    all_timesteps_list=all_timesteps_list,\n",
    "    batch_size = config[\"global\"][\"model\"][\"data_loader_batch_size\"],\n",
    "    shuffle = config[\"global\"][\"model\"][\"data_loader_shuffle\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Create PyG DataLoaders' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7b. Define Graph Neural Network Architecture ---\n",
    "\n",
    "model, device, optimizer, criterion = instantiate_model_and_associated(\n",
    "    all_timesteps_list=all_timesteps_list,\n",
    "    config=config,\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Instantiate GAT-LSTM Model' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "path = \"data/04_model/eden/model/pt_model/model_20250813-193958_GATTrue_LSTMFalse_GATH12_GATD0-4_GATHC64_GATOC64_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-001_SM0-1_E250_ESP30_LRSF0-5_LRSP8_MINLR1e-06_LD0-0001_GCMN1-0.pt\"\n",
    "iteration = 1  # Complete\n",
    "\n",
    "best_model = model  # Assume model object already defined and moved to correct device\n",
    "best_model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "best_model.eval()\n",
    "logger.info(f\"Loaded best model from {path}\")\n",
    "\n",
    "# Load target scaler\n",
    "scaler_path = \"data/03_graph/eden/scalers/target_scaler.pkl\"\n",
    "target_scaler = joblib.load(scaler_path)\n",
    "logger.info(f\"Loaded target scaler from: {scaler_path}\")\n",
    "\n",
    "scale = torch.tensor(target_scaler.scale_, device=device)\n",
    "mean = torch.tensor(target_scaler.mean_, device=device)\n",
    "\n",
    "# Initialise global LSTM state\n",
    "if best_model.run_LSTM:\n",
    "    lstm_state_store = {\n",
    "        'h': torch.zeros(best_model.num_layers_lstm, best_model.num_nodes, best_model.hidden_channels_lstm).to(device),\n",
    "        'c': torch.zeros(best_model.num_layers_lstm, best_model.num_nodes, best_model.hidden_channels_lstm).to(device)\n",
    "    }\n",
    "else:\n",
    "    lstm_state_store = None\n",
    "\n",
    "# Prepare for evaluation\n",
    "test_predictions_unscaled = []\n",
    "test_actuals_unscaled = []\n",
    "\n",
    "logger.info(\"--- Starting Model Evaluation on Test Set ---\")\n",
    "test_loop = tqdm(all_timesteps_list, desc=\"Evaluating on Test Set\", leave=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loop):\n",
    "        data = data.to(device)\n",
    "        test_mask = data.test_mask\n",
    "\n",
    "        if test_mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Model forward pass on full node set\n",
    "        predictions_all, (h_new, c_new), returned_node_ids = best_model(\n",
    "            x=data.x,\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=data.edge_attr,\n",
    "            current_timestep_node_ids=data.node_id,\n",
    "            lstm_state_store=lstm_state_store\n",
    "        )\n",
    "\n",
    "        # Update LSTM memory for current nodes\n",
    "        if best_model.run_LSTM:\n",
    "            lstm_state_store['h'][:, returned_node_ids, :] = h_new.detach()\n",
    "            lstm_state_store['c'][:, returned_node_ids, :] = c_new.detach()\n",
    "\n",
    "        # Filter predictions/targets to test nodes\n",
    "        preds_std = predictions_all[test_mask]\n",
    "        targets_std = data.y[test_mask]\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        preds_np = preds_std.cpu().numpy()\n",
    "        targets_np = targets_std.cpu().numpy()\n",
    "\n",
    "        preds_unscaled = target_scaler.inverse_transform(preds_np)\n",
    "        targets_unscaled = target_scaler.inverse_transform(targets_np)\n",
    "\n",
    "        test_predictions_unscaled.extend(preds_unscaled.flatten())\n",
    "        test_actuals_unscaled.extend(targets_unscaled.flatten())\n",
    "\n",
    "        if i < 5:  # Show first few predictions\n",
    "            print(\"Sample predictions (m AOD):\", preds_unscaled[:5].flatten())\n",
    "            print(\"Sample actuals     (m AOD):\", targets_unscaled[:5].flatten())\n",
    "\n",
    "# Final evaluation\n",
    "if len(test_actuals_unscaled) > 0:\n",
    "    loss_type = config[catchment][\"training\"][\"loss\"]\n",
    "\n",
    "    if loss_type == \"MAE\":\n",
    "        final_test_metric = mean_absolute_error(test_actuals_unscaled, test_predictions_unscaled)\n",
    "        logger.info(f\"\\n--- Final Test Set MAE (m AOD): {final_test_metric:.4f} ---\\n\")\n",
    "\n",
    "    elif loss_type == \"MSE\":\n",
    "        final_test_metric = mean_squared_error(test_actuals_unscaled, test_predictions_unscaled)\n",
    "        logger.info(f\"\\n--- Final Test Set MSE (m AOD²): {final_test_metric:.4f} ---\\n\")\n",
    "\n",
    "    else:\n",
    "        logger.warning(f\"Unrecognized loss type '{loss_type}' in config — skipping final metric calculation.\")\n",
    "else:\n",
    "    logger.warning(\"No test data found — check 'data.test_mask'.\")\n",
    "\n",
    "logger.info(\"--- Model Evaluation on Test Set Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the target scaler\n",
    "from joblib import load\n",
    "\n",
    "# Load target scaler (y, 'gwl_value') in\n",
    "scalers_dir = config[catchment][\"paths\"][\"scalers_dir\"]\n",
    "target_scaler_path = os.path.join(scalers_dir, \"target_scaler.pkl\")\n",
    "try:\n",
    "    target_scaler = load(target_scaler_path)\n",
    "    logger.info(f\"Successfully loaded target scaler from: {target_scaler_path}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"No target scaler found or error loading it: {e}\")\n",
    "    target_scaler = None\n",
    "\n",
    "# Convert both to np array\n",
    "test_predictions_np = np.array(test_predictions_unscaled).reshape(-1, 1)\n",
    "test_actuals_np = np.array(test_actuals_unscaled).reshape(-1, 1)\n",
    "\n",
    "# Confirm range (sanity checker)\n",
    "logger.info(f\"Sample prediction range: {test_predictions_np.min():.2f} to {test_predictions_np.max():.2f}\")\n",
    "logger.info(f\"Sample actual range:     {test_actuals_np.min():.2f} to {test_actuals_np.max():.2f}\")\n",
    "\n",
    "# Assign reshaped vals to final arrs for plotting and metrics\n",
    "test_predictions_final = test_predictions_np\n",
    "test_actuals_final = test_actuals_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from permetrics.regression import RegressionMetric\n",
    "\n",
    "final_test_mae = mean_absolute_error(test_actuals_final, test_predictions_final)\n",
    "adjusted_mae = mean_absolute_error(test_actuals_final, test_predictions_final-final_test_mae)\n",
    "unit_label = \"mAOD\" if target_scaler else \"standard units\"\n",
    "evaluator = RegressionMetric(test_actuals_final, test_predictions_final)  # Before offset correction\n",
    "adj_evaluator = RegressionMetric(test_actuals_final, test_predictions_final-final_test_mae)  # After offset correction\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "# logger.info(f\"Final Test Set MAE: {final_test_mae:.4f} {unit_label} [Target 0.2 to 0.5 mAOD]\")\n",
    "logger.info(f\"Final Adjusted MAE: {adjusted_mae:.4f} {unit_label} [Target 0.2 to 0.5 mAOD]\\n\")\n",
    "\n",
    "# Root Mean Square Error (RMSE)\n",
    "# logger.info(f\"Baseline RMSE: {evaluator.root_mean_squared_error():.4f} {unit_label} [Target 0.25 to 0.6 mAOD]\")\n",
    "logger.info(f\"Adjusted RMSE: {adj_evaluator.root_mean_squared_error():.4f} {unit_label} [Target 0.25 to 0.6 mAOD]\\n\")\n",
    "\n",
    "# Coefficient of Determination (R^2)\n",
    "# logger.info(f\"Baseline R^2: {evaluator.coefficient_of_determination():.4f} [Target 0.80 or higher]\")\n",
    "logger.info(f\"Adjusted R^2: {adj_evaluator.coefficient_of_determination():.4f} [Target 0.80 or higher]\\n\")\n",
    "\n",
    "# Nash-Sutcliffe Efficiency (NSE)\n",
    "# logger.info(f\"Baseline NSE: {evaluator.nash_sutcliffe_efficiency():.4f} [Target 0.75 or higher]\")\n",
    "logger.info(f\"Adjusted NSE: {adj_evaluator.nash_sutcliffe_efficiency():.4f} [Target 0.75 or higher]\\n\")\n",
    "\n",
    "# Kling Gupta Efficiency (KGE)\n",
    "# logger.info(f\"Baseline KGE: {evaluator.kling_gupta_efficiency():.4f} [Target 0.75 or higher]\")\n",
    "logger.info(f\"Adjusted KGE: {adj_evaluator.kling_gupta_efficiency():.4f} [Target 0.75 or higher]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_kge_components(actuals, predictions):\n",
    "    \"\"\"Get KGE Component parts as individual values.\"\"\"\n",
    "    r = np.corrcoef(actuals.flatten(), predictions.flatten())[0, 1]\n",
    "    beta = np.mean(predictions.flatten()) / np.mean(actuals.flatten())\n",
    "    gamma = np.std(predictions.flatten()) / np.std(actuals.flatten())\n",
    "    return r, beta, gamma\n",
    "\n",
    "# Using precalc'd adjusted predictions\n",
    "adjusted_predictions = test_predictions_final - mean_absolute_error(test_actuals_final, test_predictions_final)\n",
    "r_actual, beta_actual, gamma_actual = calculate_kge_components(test_actuals_final, test_predictions_final)\n",
    "r_adjusted, beta_adjusted, gamma_adjusted = calculate_kge_components(test_actuals_final, adjusted_predictions)\n",
    "\n",
    "# Log component results (baseline)\n",
    "print(f\"Baseline KGE Components:\")\n",
    "print(f\"  Correlation (r): {r_actual:.4f};\")\n",
    "print(f\"  Bias (beta): {beta_actual:.4f};\")\n",
    "print(f\"  Variability (gamma): {gamma_actual:.4f}\\n\")\n",
    "\n",
    "# Log component results (adjusted)\n",
    "print(f\"Adjusted KGE Components:\")\n",
    "print(f\"  Correlation (r): {r_adjusted:.4f};\")\n",
    "print(f\"  Bias (beta): {beta_adjusted:.4f};\")\n",
    "print(f\"  Variability (gamma): {gamma_adjusted:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# plt.plot(test_actuals_final, label='Actual GWL Value', color='blue', alpha=0.7)\n",
    "plt.plot(test_predictions_final - final_test_mae, label='Predicted GWL Value', color='red', alpha=0.7)  # , linestyle='--'\n",
    "# plt.plot(test_predictions_final, label='Predicted GWL Value', color='red', alpha=0.7)  # , linestyle='--'\n",
    "plt.plot(test_actuals_final, label='Actual GWL Value', color='blue', alpha=0.7)\n",
    "\n",
    "plt.title('Actual vs. Predicted Groundwater Levels on Test Set')\n",
    "plt.xlabel('Data Point Index (Sequential Timesteps/Stations)')\n",
    "plt.ylabel('Groundwater Level (m AOD)' if target_scaler else 'Standardised GWL')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Give slightly more room above and below than automatic\n",
    "all_vals = np.concatenate([test_actuals_final, test_predictions_final - final_test_mae])\n",
    "y_min = all_vals.min()\n",
    "y_max = all_vals.max()\n",
    "y_range = y_max - y_min\n",
    "plt.ylim(y_min - y_range/3, y_max + y_range/3)\n",
    "\n",
    "# Save plot\n",
    "base_name = os.path.basename(path)\n",
    "filename_no_ext, extension = os.path.splitext(base_name)\n",
    "save_path = \"results/trained_models/eden/\" + f\"{iteration}_\" + filename_no_ext\n",
    "# save_path = os.path.join(\"results/trained_models/eden/2_\", filename_no_ext)\n",
    "plt.savefig(save_path, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a style for plots (optional)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot the predicted values\n",
    "plt.plot(test_predictions_unscaled, label='Predicted GWL Value', color='red', alpha=0.7)  # , linestyle='--'\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Predicted Groundwater Levels on Test Set')\n",
    "plt.xlabel('Data Point Index (Sequential Timesteps/Stations)')\n",
    "plt.ylabel('Groundwater Level (m AOD)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Pipeline step 'Generate plot of predicted values' complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Plot at a weekly resolution (using daily predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly (7-day) mean plot from daily arrays \n",
    "\n",
    "pred_daily = np.asarray(test_predictions_final).reshape(-1)\n",
    "act_daily = np.asarray(test_actuals_final).reshape(-1)\n",
    "\n",
    "# Calc means\n",
    "k = 7\n",
    "n_full = (len(act_daily) // k) * k\n",
    "if n_full < len(act_daily):\n",
    "    logger.info(f\"Trimming {len(act_daily) - n_full} trailing day(s).\")\n",
    "act_weekly  = act_daily[:n_full].reshape(-1, k).mean(axis=1)\n",
    "pred_weekly = pred_daily[:n_full].reshape(-1, k).mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(act_weekly,  label='Actual GWL', alpha=0.8)\n",
    "plt.plot(pred_weekly - final_test_mae, label='Predicted GWL', alpha=0.8)  # linestyle='--',\n",
    "plt.title('Actual vs. Predicted Groundwater Levels (Weekly Means)')\n",
    "plt.xlabel('Week Index')\n",
    "plt.ylabel('Groundwater Level (m AOD)' if target_scaler else 'Standardised GWL')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Set plot limits\n",
    "all_vals = np.concatenate([act_weekly, pred_weekly - final_test_mae])\n",
    "y_min = all_vals.min()\n",
    "y_max = all_vals.max()\n",
    "y_range = y_max - y_min\n",
    "plt.ylim(y_min - y_range/2, y_max + y_range/2)\n",
    "\n",
    "# Save next to the daily figure with _weekly\n",
    "weekly_save_path = save_path + \"_weekly\"\n",
    "# plt.savefig(weekly_save_path, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ray.tune import ExperimentAnalysis\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ray_tune_dir = \"data/04_model/eden/model/ray_tune_gwl\"\n",
    "\n",
    "if not os.path.isdir(ray_tune_dir):\n",
    "    logger.error(f\"Error: The specified directory does not exist or is not a directory: {ray_tune_dir}\")\n",
    "else:\n",
    "    try:\n",
    "        abs_dir = os.path.abspath(ray_tune_dir)\n",
    "        uri = f\"file://{abs_dir}\"\n",
    "\n",
    "        analysis = ExperimentAnalysis(uri)\n",
    "        df = analysis.dataframe()\n",
    "\n",
    "        # Coerce numeric metrics (in case anything logged as strings)\n",
    "        for col in ['val_loss', 'train_loss', 'epoch']:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # Keep only rows with a valid trial_id\n",
    "        if 'trial_id' not in df.columns:\n",
    "            raise KeyError(\"Expected 'trial_id' column not found in Ray Tune results DataFrame.\")\n",
    "        df = df[df['trial_id'].notna()].copy()\n",
    "\n",
    "        # filter out errored trials (errored due to timeouts)\n",
    "        if 'error' in df.columns:\n",
    "            df = df[df['error'].isna()].copy()\n",
    "\n",
    "        logger.info(f\"Total rows (iterations) loaded: {len(df)}\")\n",
    "        n_trials = df['trial_id'].nunique()\n",
    "        logger.info(f\"Trials represented: {n_trials}\")\n",
    "\n",
    "        # Compute per-trial aggregates\n",
    "        config_cols = [c for c in df.columns if c.startswith('config/')]\n",
    "        keep_cols = ['trial_id', 'logdir'] + config_cols\n",
    "        meta_first = (df[keep_cols]\n",
    "                      .sort_values(['trial_id']) \n",
    "                      .groupby('trial_id', as_index=False)\n",
    "                      .first())\n",
    "        \n",
    "        # Trials that never logged val_loss/train_los (timed out) become NaN -> drop for ranking\n",
    "        agg = (df.groupby('trial_id')\n",
    "                 .agg(min_val_loss=('val_loss', 'min'),\n",
    "                      mean_val_loss=('val_loss', 'mean'),\n",
    "                      min_train_loss=('train_loss', 'min'),\n",
    "                      mean_train_loss=('train_loss', 'mean'),\n",
    "                      last_epoch=('epoch', 'max'),\n",
    "                      last_iter=('training_iteration', 'max'))\n",
    "                 .reset_index())\n",
    "\n",
    "        # Merge configs back in\n",
    "        per_trial = meta_first.merge(agg, on='trial_id', how='left')\n",
    "\n",
    "        # Keep only trials with at least some val_loss signal\n",
    "        ranked = per_trial[per_trial['min_val_loss'].notna()].copy()\n",
    "        if ranked.empty:\n",
    "            raise RuntimeError(\"No trials have non-NaN 'val_loss'. Cannot rank.\")\n",
    "\n",
    "        ranked = ranked.sort_values(['min_val_loss', 'mean_val_loss'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "        best = ranked.iloc[0]\n",
    "        trial_id = best['trial_id']\n",
    "\n",
    "        # Pack nice dicts for logging\n",
    "        best_config = {k: best[k] for k in config_cols if k in ranked.columns}\n",
    "        best_metrics = {k: best[k] for k in ['min_val_loss', 'mean_val_loss', 'min_train_loss', 'mean_train_loss',\n",
    "                                             'last_epoch', 'last_iter'] if k in ranked.columns}\n",
    "\n",
    "        logger.info(\"\\n--- Best Trial (by min_val_loss, then mean_val_loss) ---\")\n",
    "        logger.info(f\"Trial ID: {trial_id}\")\n",
    "        logger.info(f\"Best metrics: {best_metrics}\")\n",
    "        logger.info(f\"Best hyperparameters: {best_config}\")\n",
    "\n",
    "        # Locate the corresponding PT model for this trial_id\n",
    "        pt_root = os.path.abspath(\"data/04_model/eden/model/pt_model\")\n",
    "        pattern = os.path.join(pt_root, f\"trial_{trial_id}\", \"pt_model\", \"*.pt\")\n",
    "        pt_candidates = sorted(glob.glob(pattern))\n",
    "        if pt_candidates:\n",
    "            best_pt_model_path = pt_candidates[-1]  # choose last\n",
    "            logger.info(f\"Best trial PT model: {best_pt_model_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"No .pt model files found for trial_id={trial_id} with pattern: {pattern}\")\n",
    "\n",
    "        # Show a compact table of the top 10 trials\n",
    "        display_cols = (['trial_id'] + config_cols +\n",
    "                        ['min_val_loss', 'mean_val_loss', 'min_train_loss', 'mean_train_loss', 'last_epoch'])\n",
    "        display_cols = [c for c in display_cols if c in ranked.columns]\n",
    "        top10 = ranked[display_cols].head(10)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', 160)\n",
    "        print(\"\\nTop 10 trials by validation loss:\")\n",
    "        print(top10.to_string(index=False))\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during analysis: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
