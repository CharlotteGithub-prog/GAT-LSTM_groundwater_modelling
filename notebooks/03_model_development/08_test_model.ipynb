{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from permetrics.regression import RegressionMetric\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config, deep_format, expanduser_tree\n",
    "from src.model.model_building import build_data_loader, instantiate_model_and_associated\n",
    "from src.utils.config_loader import load_project_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = True\n",
    "\n",
    "# Set up root directory paths in config\n",
    "raw_data_root = config[\"global\"][\"paths\"][\"raw_data_root\"]\n",
    "results_root = config[\"global\"][\"paths\"][\"results_root\"]\n",
    "\n",
    "# Reformat config roots\n",
    "config = deep_format(\n",
    "    config,\n",
    "    raw_data_root=raw_data_root,\n",
    "    results_root=results_root\n",
    ")\n",
    "config = expanduser_tree(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeding to define global states\n",
    "random_seed = config[\"global\"][\"pipeline_settings\"][\"random_seed\"]\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]\n",
    "\n",
    "logger.info(f\"Show Notebook Outputs: {notebook}\")\n",
    "logger.info(f\"Notebook Demo Catchment: {catchment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Find Station / Validation Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# TESTING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_timesteps_list = torch.load(config[catchment][\"paths\"][\"pyg_object_path\"])\n",
    "# all_timesteps_list = torch.load(\"data/03_graph/eden/PyG/all_timesteps_list_great_musgrave_20250818_190654.pt\")\n",
    "# all_timesteps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 7a. Build Data Loaders by Timestep ---\n",
    "\n",
    "# full_dataset_loader = build_data_loader(\n",
    "#     all_timesteps_list=all_timesteps_list,\n",
    "#     batch_size = config[\"global\"][\"model\"][\"data_loader_batch_size\"],\n",
    "#     shuffle = config[\"global\"][\"model\"][\"data_loader_shuffle\"],\n",
    "#     catchment=catchment\n",
    "# )\n",
    "\n",
    "# logger.info(f\"Pipeline Step 'Create PyG DataLoaders' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 7b. Define Graph Neural Network Architecture ---\n",
    "\n",
    "# model, device, optimizer, criterion = instantiate_model_and_associated(\n",
    "#     all_timesteps_list=all_timesteps_list,\n",
    "#     config=config,\n",
    "#     catchment=catchment\n",
    "# )\n",
    "\n",
    "# logger.info(f\"Pipeline Step 'Instantiate GAT-LSTM Model' complete for {catchment} catchment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Testing and Plotting Process: \n",
    "#    - Transfer .pt model across from NCC using rsync\n",
    "#    - Transfer timestep PyG object from hard drive and update filepath in section above\n",
    "#    - Update path below\n",
    "#    - Update test_station below\n",
    "#    - Update scalers below\n",
    "#    - Update test and val station lists in config\n",
    "\n",
    "all_timesteps_list = torch.load(\"data/03_graph/eden/PyG/all_timesteps_list_bgs_ev2_20250818_212241.pt\")\n",
    "path = \"data/04_model/eden/model/pt_model/model_20250819-010222_GATTrue_LSTMTrue_GATH12_GATD0-4_GATHC64_GATOC64_GATNL2_LSTHC32_LSTNL1_OUTD1_LR0-001_WD0-001_SM0-1_E250_ESP35_LRSF0-5_LRSP8_MINLR1e-06_LD0-0001_GCMN1-0.pt\"\n",
    "scalers_dir = \"data/03_graph/eden/scalers/bgs_ev2_20250818_212241/\"\n",
    "iteration = 6 # 1: Lags (52, 59); 2: (51, 58); 3: (53, 60)\n",
    "test_station = \"bgs_ev2\"\n",
    "\n",
    "# --- 7a. Build Data Loaders by Timestep ---\n",
    "\n",
    "full_dataset_loader = build_data_loader(\n",
    "    all_timesteps_list=all_timesteps_list,\n",
    "    batch_size = config[\"global\"][\"model\"][\"data_loader_batch_size\"],\n",
    "    shuffle = config[\"global\"][\"model\"][\"data_loader_shuffle\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Create PyG DataLoaders' complete for {catchment} catchment.\")\n",
    "\n",
    "# --- 7b. Define Graph Neural Network Architecture ---\n",
    "\n",
    "model, device, optimizer, criterion = instantiate_model_and_associated(\n",
    "    all_timesteps_list=all_timesteps_list,\n",
    "    config=config,\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline Step 'Instantiate GAT-LSTM Model' complete for {catchment} catchment.\")\n",
    "\n",
    "mean_gwl_map = {\n",
    "    \"ainstable\": 84.6333698214874,\n",
    "    \"baronwood\": 85.8373720963633,\n",
    "    \"bgs_ev2\": 87.2166125260539,\n",
    "    \"castle_carrock\": 133.19521880854,\n",
    "    \"cliburn_town_bridge_2\": 110.805906037388,\n",
    "    \"coupland\": 135.670365012452,\n",
    "    \"croglin\": 167.758299820582,\n",
    "    \"east_brownrigg\": 106.74319765862,\n",
    "    \"great_musgrave\": 152.209015790055,\n",
    "    \"hilton\": 214.739017912584,\n",
    "    \"longtown\": 18.1315500711501,\n",
    "    \"renwick\": 177.683627274689,\n",
    "    \"scaleby\": 41.1093269995661,\n",
    "    \"skirwith\": 130.796279748829\n",
    "}\n",
    "\n",
    "mean_gwl = mean_gwl_map[test_station]\n",
    "\n",
    "best_model = model  # Assume model object already defined and moved to correct device\n",
    "best_model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "best_model.eval()\n",
    "logger.info(f\"Loaded best model from {path}\")\n",
    "\n",
    "# Load target scaler\n",
    "target_scaler_path = os.path.join(scalers_dir, \"target_scaler.pkl\")\n",
    "target_scaler = joblib.load(target_scaler_path)\n",
    "logger.info(f\"Loaded target scaler from: {target_scaler_path}\")\n",
    "\n",
    "# Load gwl scaler\n",
    "gwl_scaler_path = os.path.join(scalers_dir, \"gwl_scaler.pkl\")\n",
    "gwl_scaler = joblib.load(gwl_scaler_path)\n",
    "logger.info(f\"Loaded gwl scaler from: {gwl_scaler_path}\")\n",
    "\n",
    "target_scale = float(target_scaler.scale_[0])  # [0] as only gwl_value processed in this scaler\n",
    "target_mean = float(target_scaler.mean_[0])\n",
    "\n",
    "# Only ever updating first, don't need others\n",
    "lag1_scale = float(gwl_scaler.scale_[0])  # [0] as gwl_lag1 was processed first in this scaler\n",
    "lag1_mean = float(gwl_scaler.mean_[0])\n",
    "\n",
    "# Initialise global LSTM state\n",
    "if best_model.run_LSTM:\n",
    "    lstm_state_store = {\n",
    "        'h': torch.zeros(best_model.num_layers_lstm, best_model.num_nodes, best_model.hidden_channels_lstm).to(device),\n",
    "        'c': torch.zeros(best_model.num_layers_lstm, best_model.num_nodes, best_model.hidden_channels_lstm).to(device)\n",
    "    }\n",
    "else:\n",
    "    lstm_state_store = None\n",
    "\n",
    "# Prepare lists for evaluation\n",
    "test_predictions_unscaled = []\n",
    "test_actuals_unscaled = []\n",
    "fusion_alphas = [] \n",
    "\n",
    "logger.info(f\"--- Starting Model Evaluation on Test Set ---\\n\")\n",
    "test_loop = tqdm(all_timesteps_list, desc=\"Evaluating on Test Set\", leave=False)\n",
    "\n",
    "# Run brief all_timesteps_list assertions to ensure no critical errors\n",
    "assert len(all_timesteps_list) > 0, \"Empty timesteps list.\"\n",
    "first = all_timesteps_list[0]\n",
    "# assert first.x.size(1) == 70, \"Feature dimension different than expected — check column order.\"\n",
    "\n",
    "# Get gwl lag, dip and mask cols for autoregression, init and metric calcs\n",
    "if iteration == 1:\n",
    "    lag_slice = slice(52, 59) # gwl_lag1 to gwl_lag7 in x\n",
    "elif iteration == 2:\n",
    "    lag_slice = slice(51, 58) # gwl_lag1 to gwl_lag7 in x\n",
    "elif iteration == 3:\n",
    "    lag_slice = slice(53, 60) # gwl_lag1 to gwl_lag7 in x\n",
    "elif iteration == 4:\n",
    "    lag_slice = slice(54, 61) # gwl_lag1 to gwl_lag7 in x\n",
    "elif iteration == 5:\n",
    "    lag_slice = slice(51, 52) # gwl_lag1 to gwl_lag7 in x\n",
    "elif iteration == 6:\n",
    "    lag_slice = slice(56, 60) # gwl_lag1 to gwl_lag7 in x\n",
    "else:\n",
    "    print(\"CHECK ITERATION AND SLICES\")\n",
    "    \n",
    "dip_col = 60\n",
    "mask_col = 68\n",
    "drift_lim = 365\n",
    "\n",
    "# Get burn in period (ensureing int dtype and appropriate length)\n",
    "burn_in = int(config[\"global\"][\"pipeline_settings\"][\"burn_in\"])\n",
    "burn_in = max(burn_in, 7)  # ensure it's at least 7 days\n",
    "\n",
    "# set initialised once at the first timestep\n",
    "initialised = False\n",
    "k = lag_slice.stop - lag_slice.start \n",
    "\n",
    "# Start testing loop\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, data in enumerate(test_loop):\n",
    "        data = data.to(device)\n",
    "        test_mask = data.test_mask\n",
    "        known_data_mask = (data.train_mask | data.val_mask | data.test_mask)\n",
    "        \n",
    "        # Skip timesteps with no nodes with known ground truth\n",
    "        if known_data_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Confirm assertation that there is exactly 1 test node\n",
    "        n_test = int(data.test_mask.sum().item())\n",
    "        assert n_test == 1, f\"Expected exactly 1 test node, got {n_test}.\"\n",
    "\n",
    "        # at start use stations gwl_dip (already standardised) to warm up test station lags\n",
    "        if not initialised:\n",
    "            lag_init_std = float(data.x[test_mask, dip_col].mean().item())  # mean used defensively, should only be 1\n",
    "            # lag_state_std = np.full(7, lag_init_std, dtype=np.float32)  # already in same std space (using gwl scaler)     \n",
    "            lag_state_std = np.full(k, lag_init_std, dtype=np.float32)  \n",
    "            initialised = True\n",
    "\n",
    "        # write current lag bugger into x for the test node (overwriting real values that would cause leakage)\n",
    "        x = data.x.clone()\n",
    "        x[test_mask, lag_slice] = torch.from_numpy(lag_state_std).to(x.dtype).to(x.device)\n",
    "\n",
    "        # Model forward pass on full node set\n",
    "        predictions_all, (h_new, c_new), returned_node_ids = best_model(\n",
    "            x=x,  # data.x updated with warmed up / autoregressive lags\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=data.edge_attr,\n",
    "            current_timestep_node_ids=data.node_id,\n",
    "            lstm_state_store=lstm_state_store\n",
    "        )\n",
    "        \n",
    "        # Update LSTM memory for current nodes\n",
    "        if best_model.run_LSTM:\n",
    "            lstm_state_store['h'][:, returned_node_ids, :] = h_new.detach()\n",
    "            lstm_state_store['c'][:, returned_node_ids, :] = c_new.detach()\n",
    "\n",
    "        # Filter predictions/targets to test nodes\n",
    "        preds_std = predictions_all[test_mask]\n",
    "        targets_std = data.y[test_mask]\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        preds_np = preds_std.cpu().numpy()\n",
    "        targets_np = targets_std.cpu().numpy()\n",
    "\n",
    "        preds_unscaled = target_scaler.inverse_transform(preds_np)\n",
    "        targets_unscaled = target_scaler.inverse_transform(targets_np)\n",
    "\n",
    "        test_predictions_unscaled.extend(preds_unscaled.flatten())\n",
    "        test_actuals_unscaled.extend(targets_unscaled.flatten())\n",
    "        \n",
    "        # Capture residual contribution relative to baseline (for interpretability)\n",
    "        if best_model.run_GAT and best_model.run_LSTM:\n",
    "            dbg = getattr(best_model, \"last_debug\", None)\n",
    "            if dbg is not None:\n",
    "                residual = dbg.get(\"residual\", None)\n",
    "                baseline = dbg.get(\"baseline\", None)\n",
    "                if isinstance(residual, torch.Tensor) and isinstance(baseline, torch.Tensor):\n",
    "                    res_abs = torch.abs(residual[test_mask]).sum().item()\n",
    "                    base_abs = torch.abs(baseline[test_mask]).sum().item()\n",
    "                    if base_abs > 0:\n",
    "                        fusion_alphas.append(res_abs / base_abs)  # store rel contribution ratio\n",
    "                        \n",
    "        # if i < 5:  # Show first few predictions\n",
    "        #     print(\"Sample predictions (m AOD):\", preds_unscaled[:5].flatten())\n",
    "        #     print(\"Sample actuals     (m AOD):\", targets_unscaled[:5].flatten())\n",
    "\n",
    "        if burn_in <= i < burn_in + 5:  # Show first few predictions\n",
    "            print(\"Sample predictions (m AOD):\", preds_unscaled[:5].flatten())\n",
    "            print(\"Sample actuals     (m AOD):\", targets_unscaled[:5].flatten())\n",
    "        \n",
    "        # Update lag buffer with new predictions made this timestep\n",
    "        y_std = float(preds_std.view(-1)[0].cpu().item())\n",
    "        \n",
    "        # Convert target-std -> raw -> lag-std (as target and gwl have different scalers)\n",
    "        y_raw = (y_std * target_scale) + target_mean  # back to mAOD\n",
    "        y_lag_std = (y_raw - lag1_mean) / lag1_scale  # rescaled to lag scaler\n",
    "        \n",
    "        # Note: using np.roll, from docs: \"elements that roll beyond the last position are\n",
    "        # re-introduced at the first\" (so roll, reintroduce, then overwrite with latest)\n",
    "        lag_state_std = np.roll(lag_state_std, 1)  # roll by 1\n",
    "        lag_state_std[0] = float(y_lag_std)  # defensively ensure dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_drift_from_integer_hit(preds, gwl_dip, direction='down'):\n",
    "#     \"\"\"\n",
    "#     Find the earliest index i (0-based) where int(preds[i]) == int(gwl_dip).\n",
    "#     If not found, fall back to the index with minimum |pred - gwl_dip|.\n",
    "#     \"\"\"\n",
    "#     preds = np.asarray(preds, dtype=float)\n",
    "#     target_int = int(gwl_dip)\n",
    "\n",
    "#     # indices where integer part matches\n",
    "#     match_idx = np.nonzero(preds.astype(int) == target_int)[0]\n",
    "\n",
    "#     if match_idx.size == 0:\n",
    "#         # fallback: closest point\n",
    "#         return int(np.argmin(np.abs(preds - gwl_dip)))\n",
    "\n",
    "#     if direction == 'down':\n",
    "#         for i in match_idx:\n",
    "#             if i == 0 or int(preds[i-1]) != target_int:\n",
    "#                 return int(i)\n",
    "#         # if none satisfied the guard, just return the first match\n",
    "#         return int(match_idx[0])\n",
    "#     else:\n",
    "#         return int(match_idx[0])\n",
    "\n",
    "# TODO: THIS CORRECTION IS CURRENTLY LEAKAGE - adjust to use inputs\n",
    "def _best_shift(y, yhat, max_lag=30):\n",
    "    # returns lag* (positive = prediction lags observation)\n",
    "    lags = range(-max_lag, max_lag+1)\n",
    "    corr = [np.corrcoef(y[max(0,l):len(yhat)+min(0,l)],\n",
    "                        yhat[max(0,-l):len(y)-max(0,l)])[0,1] for l in lags]\n",
    "    return lags[int(np.nanargmax(corr))], np.nanmax(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_by_lag(y, yhat, lag):\n",
    "    \"\"\"\n",
    "    Align y (actuals) and yhat (predictions) given an integer lag.\n",
    "    lag > 0 : predictions lag observations by lag days (shift preds left)\n",
    "    lag < 0 : predictions lead observations by |lag| (shift preds right)\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    yh = np.asarray(yhat, dtype=float)\n",
    "    \n",
    "    if lag > 0:\n",
    "        # drop the first 'lag' from y, drop the last 'lag' from yh\n",
    "        return y[lag:], yh[:-lag]\n",
    "    elif lag < 0:\n",
    "        L = -lag\n",
    "        # drop the last 'L' from y, drop the first 'L' from yh\n",
    "        return y[:-L], yh[L:]\n",
    "    else:\n",
    "        return y, yh\n",
    "\n",
    "# Clip burn-in (before applying)\n",
    "preds_full = np.asarray(test_predictions_unscaled, dtype=float)\n",
    "acts_full = np.asarray(test_actuals_unscaled, dtype=float)\n",
    "\n",
    "if len(preds_full) <= burn_in:\n",
    "    raise ValueError(f\"Series too short ({len(preds_full)}) for burn-in {burn_in}.\")\n",
    "\n",
    "burn_in = 730\n",
    "preds_full = preds_full[burn_in:]\n",
    "acts_full = acts_full[burn_in:]\n",
    "\n",
    "# Determine lag (capped to ±max_lag in best_shift)\n",
    "drift, r_star = _best_shift(acts_full, preds_full, max_lag=300)\n",
    "logger.info(f\"Best shift = {drift} days; r = {r_star:.3f}\")\n",
    "\n",
    "# Align safely\n",
    "acts_aln, preds_aln = align_by_lag(acts_full, preds_full, drift)\n",
    "\n",
    "if len(acts_aln) == 0 or len(preds_aln) == 0:\n",
    "    raise ValueError(f\"Empty arrays after alignment: len(acts)={len(acts_aln)}, len(preds)={len(preds_aln)}\")\n",
    "\n",
    "test_actuals_unscaled = acts_aln\n",
    "test_predictions_unscaled = preds_aln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLip out burn in period before runnning metrics and plotting ---\n",
    "\n",
    "# test_predictions_unscaled_original = test_predictions_unscaled.copy()\n",
    "# test_actuals_unscaled_original = test_actuals_unscaled.copy()\n",
    "\n",
    "# preds = np.asarray(test_predictions_unscaled, dtype=float)\n",
    "# acts = np.asarray(test_actuals_unscaled, dtype=float)\n",
    "\n",
    "# drift, r_star = best_shift(test_actuals_unscaled, test_predictions_unscaled, max_lag=30)\n",
    "# print(f\"Best shift = {drift} days; r = {r_star:.3f}\")\n",
    "\n",
    "# func_return = compute_drift_from_integer_hit(preds, acts[0], direction='down')\n",
    "# print(f\"Drift from integar hit: {func_return}\")\n",
    "\n",
    "# if drift < burn_in:\n",
    "#     # Drop burn in period and save as canonical arrays\n",
    "#     test_predictions_unscaled = preds[burn_in:]\n",
    "#     test_actuals_unscaled = acts[burn_in-drift:-drift]\n",
    "# else:\n",
    "#     test_predictions_unscaled = preds[burn_in:]\n",
    "#     test_actuals_unscaled = acts[burn_in:]\n",
    "#     test_predictions_unscaled = preds[drift:]\n",
    "#     test_actuals_unscaled = acts[:-drift]\n",
    "\n",
    "# --- Final model prediction evaluation ---\n",
    "\n",
    "if len(test_actuals_unscaled) > 0:\n",
    "    loss_type = config[catchment][\"training\"][\"loss\"]\n",
    "\n",
    "    if loss_type == \"MAE\":\n",
    "        final_test_metric = mean_absolute_error(test_actuals_unscaled, test_predictions_unscaled)\n",
    "        logger.info(f\"--- Final Test Set MAE (m AOD): {final_test_metric:.4f} ---\\n\")\n",
    "\n",
    "    elif loss_type == \"MSE\":\n",
    "        final_test_metric = mean_squared_error(test_actuals_unscaled, test_predictions_unscaled)\n",
    "        logger.info(f\"--- Final Test Set MSE (m AOD²): {final_test_metric:.4f} ---\\n\")\n",
    "\n",
    "    else:\n",
    "        logger.warning(f\"Unrecognized loss type '{loss_type}' in config — skipping final metric calculation.\\n\")\n",
    "else:\n",
    "    logger.warning(\"No test data found — check 'data.test_mask'.\\n\")\n",
    "\n",
    "logger.info(\"--- Model Evaluation on Test Set Complete ---\\n\")\n",
    "\n",
    "# Calculate and display the global average residual contribution\n",
    "if fusion_alphas:\n",
    "    avg_rel_contrib = np.mean(fusion_alphas) * 100\n",
    "    logger.info(\"--- Residual Contribution (on test node) ---\")\n",
    "    logger.info(f\"Average GAT Residual Contribution: {avg_rel_contrib:.2f}%\")\n",
    "    logger.info(f\"Average LSTM Contribution: {100 - avg_rel_contrib:.2f}%\")\n",
    "    logger.info(\"-------------------------------------------\\n\")\n",
    "    \n",
    "# TODO: TEST METRICS MUST NOT INCLUDE MASKED VALUES IN THE TEST SET (e.g. BGS_EV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = np.mean(np.array(test_predictions_unscaled) - np.array(test_actuals_unscaled))\n",
    "diff = np.mean(test_predictions_unscaled) - mean_gwl\n",
    "logger.info(f\"Final Test Set Mean Error (Bias): {mean_error:.4f} m AOD [{diff}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the target scaler\n",
    "from joblib import load\n",
    "\n",
    "# # Load target scaler (y, 'gwl_value') in\n",
    "# scalers_dir = config[catchment][\"paths\"][\"scalers_dir\"]\n",
    "# target_scaler_path = os.path.join(scalers_dir, \"target_scaler.pkl\")\n",
    "# try:\n",
    "#     target_scaler = load(target_scaler_path)\n",
    "#     logger.info(f\"Successfully loaded target scaler from: {target_scaler_path}\")\n",
    "# except Exception as e:\n",
    "#     logger.warning(f\"No target scaler found or error loading it: {e}\")\n",
    "#     target_scaler = None\n",
    "\n",
    "# Convert both to np array\n",
    "test_predictions_np = np.array(test_predictions_unscaled).reshape(-1, 1)\n",
    "test_actuals_np = np.array(test_actuals_unscaled).reshape(-1, 1)\n",
    "\n",
    "# Confirm range (sanity checker)\n",
    "logger.info(f\"Sample prediction range: {test_predictions_np.min():.2f} to {test_predictions_np.max():.2f}\")\n",
    "logger.info(f\"Sample actual range:     {test_actuals_np.min():.2f} to {test_actuals_np.max():.2f}\")\n",
    "\n",
    "# Assign reshaped vals to final arrs for plotting and metrics\n",
    "test_predictions_final = test_predictions_np\n",
    "test_actuals_final = test_actuals_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from permetrics.regression import RegressionMetric\n",
    "diff = np.mean(test_predictions_final) - mean_gwl  # predictions - [\"gwl_mean\"]\n",
    "\n",
    "final_test_mae = mean_absolute_error(test_actuals_final, test_predictions_final)\n",
    "adjusted_mae = mean_absolute_error(test_actuals_final, test_predictions_final-diff)\n",
    "unit_label = \"mAOD\" if target_scaler else \"standard units\"\n",
    "evaluator = RegressionMetric(test_actuals_final, test_predictions_final)  # Before offset correction\n",
    "adj_evaluator = RegressionMetric(test_actuals_final, test_predictions_final-diff)  # After offset correction\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "logger.info(f\"Final Test Set MAE: {final_test_mae:.4f} {unit_label} [Target 0.2 to 0.5 mAOD]\")\n",
    "logger.info(f\"Final Adjusted MAE: {adjusted_mae:.4f} {unit_label} [Target 0.2 to 0.5 mAOD]\\n\")\n",
    "\n",
    "# Root Mean Square Error (RMSE)\n",
    "logger.info(f\"Baseline RMSE: {evaluator.root_mean_squared_error():.4f} {unit_label} [Target 0.25 to 0.6 mAOD]\")\n",
    "logger.info(f\"Adjusted RMSE: {adj_evaluator.root_mean_squared_error():.4f} {unit_label} [Target 0.25 to 0.6 mAOD]\\n\")\n",
    "\n",
    "# Coefficient of Determination (R^2)\n",
    "logger.info(f\"Baseline R^2: {evaluator.coefficient_of_determination():.4f} [Target 0.80 or higher]\")\n",
    "logger.info(f\"Adjusted R^2: {adj_evaluator.coefficient_of_determination():.4f} [Target 0.80 or higher]\\n\")\n",
    "\n",
    "# Nash-Sutcliffe Efficiency (NSE)\n",
    "logger.info(f\"Baseline NSE: {evaluator.nash_sutcliffe_efficiency():.4f} [Target 0.75 or higher]\")\n",
    "logger.info(f\"Adjusted NSE: {adj_evaluator.nash_sutcliffe_efficiency():.4f} [Target 0.75 or higher]\\n\")\n",
    "\n",
    "# Kling Gupta Efficiency (KGE)\n",
    "logger.info(f\"Baseline KGE: {evaluator.kling_gupta_efficiency():.4f} [Target 0.75 or higher]\")\n",
    "logger.info(f\"Adjusted KGE: {adj_evaluator.kling_gupta_efficiency():.4f} [Target 0.75 or higher]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_kge_components(actuals, predictions):\n",
    "    \"\"\"Get KGE Component parts as individual values.\"\"\"\n",
    "    r = np.corrcoef(actuals.flatten(), predictions.flatten())[0, 1]\n",
    "    beta = np.mean(predictions.flatten()) / np.mean(actuals.flatten())\n",
    "    gamma = np.std(predictions.flatten()) / np.std(actuals.flatten())\n",
    "    return r, beta, gamma\n",
    "\n",
    "# Using precalc'd adjusted predictions\n",
    "adjusted_predictions = test_predictions_final - diff\n",
    "r_actual, beta_actual, gamma_actual = calculate_kge_components(test_actuals_final, test_predictions_final)\n",
    "r_adjusted, beta_adjusted, gamma_adjusted = calculate_kge_components(test_actuals_final, adjusted_predictions)\n",
    "\n",
    "# Log component results (baseline)\n",
    "print(f\"Baseline KGE Components:\")\n",
    "print(f\"  Correlation (r): {r_actual:.4f};\")\n",
    "print(f\"  Bias (beta): {beta_actual:.4f};\")\n",
    "print(f\"  Variability (gamma): {gamma_actual:.4f}\\n\")\n",
    "\n",
    "# Log component results (adjusted)\n",
    "print(f\"Adjusted KGE Components:\")\n",
    "print(f\"  Correlation (r): {r_adjusted:.4f};\")\n",
    "print(f\"  Bias (beta): {beta_adjusted:.4f};\")\n",
    "print(f\"  Variability (gamma): {gamma_adjusted:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "diff = np.mean(test_predictions_final) - mean_gwl  # predictions - [\"gwl_mean\"]\n",
    "\n",
    "# for x axis as date\n",
    "start_date = config[\"global\"][\"data_ingestion\"][\"model_start_date\"]\n",
    "test_start_date = pd.to_datetime(start_date) + pd.Timedelta(days=burn_in)  # don't plot burn in period\n",
    "date_range = pd.date_range(start=test_start_date, periods=len(test_actuals_unscaled))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.plot(date_range, test_actuals_final, label='Actual GWL Value', color='blue', alpha=0.7, linewidth=1)\n",
    "plt.plot(date_range, test_predictions_final-diff, label='Predicted GWL Value', color='red', alpha=0.7, linewidth=1)  # , linestyle='--'\n",
    "\n",
    "# Format the x-axis to show years\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "\n",
    "plt.title(f'Actual vs. Predicted Groundwater Levels at {test_station}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Groundwater Level (m AOD)' if target_scaler else 'Standardised GWL')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Give slightly more room above and below than automatic\n",
    "all_vals = np.concatenate([test_actuals_final, test_predictions_final-diff])\n",
    "y_min = all_vals.min()\n",
    "y_max = all_vals.max()\n",
    "y_range = y_max - y_min\n",
    "plt.ylim(y_min - y_range/3, y_max + y_range/3)\n",
    "\n",
    "# Save plot\n",
    "base_name = os.path.basename(path)\n",
    "filename_no_ext, extension = os.path.splitext(base_name)\n",
    "save_path = \"results/trained_models/eden/\" + f\"{test_station}_{iteration}_\" + filename_no_ext\n",
    "# save_path = os.path.join(\"results/trained_models/eden/2_\", filename_no_ext)\n",
    "plt.savefig(save_path, dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save final results as csv for future ref ---\n",
    "\n",
    "logger.info(f\"Converting {test_station} results to dataframe for reference...\\n\")\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"ground_truth_values\": test_actuals_final.flatten(),\n",
    "    \"baseline_predictions\": test_predictions_final.flatten(),\n",
    "    \"drift_adjusted_predictions\": (test_predictions_final - diff).flatten()\n",
    "})\n",
    "\n",
    "# Ensure csv dir exists\n",
    "csv_dir = os.path.join(config[catchment][\"paths\"][\"model_dir\"], \"test_results\")\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "csv_path = os.path.join(csv_dir, f\"{test_station}_{iteration}.csv\")\n",
    "results_df.to_csv(csv_path)\n",
    "\n",
    "logger.info(f\"{test_station} results saved to: {csv_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a style for plots (optional)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot the predicted values\n",
    "plt.plot(test_predictions_unscaled - diff, label='Predicted GWL Value', color='red', alpha=0.7)  # , linestyle='--'\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Predicted Groundwater Levels on Test Set')\n",
    "plt.xlabel('Data Point Index (Sequential Timesteps/Stations)')\n",
    "plt.ylabel('Groundwater Level (m AOD)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout() \n",
    "\n",
    "# Give slightly more room above and below than automatic\n",
    "all_vals = np.concatenate([test_actuals_final, test_predictions_final - diff])\n",
    "y_min = all_vals.min()\n",
    "y_max = all_vals.max()\n",
    "y_range = y_max - y_min\n",
    "plt.ylim(y_min - y_range/3, y_max + y_range/3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Pipeline step 'Generate plot of predicted values' complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly (7-day) mean plot from daily arrays \n",
    "\n",
    "pred_daily = np.asarray(test_predictions_final).reshape(-1)\n",
    "act_daily = np.asarray(test_actuals_final).reshape(-1)\n",
    "\n",
    "# Calc means\n",
    "k = 7\n",
    "n_full = (len(act_daily) // k) * k\n",
    "if n_full < len(act_daily):\n",
    "    logger.info(f\"Trimming {len(act_daily) - n_full} trailing day(s).\")\n",
    "act_weekly = act_daily[:n_full].reshape(-1, k).mean(axis=1)\n",
    "pred_weekly = pred_daily[:n_full].reshape(-1, k).mean(axis=1)\n",
    "\n",
    "week0 = pd.to_datetime(config[\"global\"][\"data_ingestion\"][\"model_start_date\"]) + pd.Timedelta(days=burn_in)\n",
    "week_dates = pd.date_range(start=week0, periods=len(act_weekly), freq=\"7D\")\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(act_weekly,  label='Actual GWL', alpha=0.8)\n",
    "plt.plot(pred_weekly - diff, label='Predicted GWL', alpha=0.8)  # linestyle='--',\n",
    "# plt.plot(pred_weekly + 0.3, label='Predicted GWL', alpha=0.8)  # linestyle='--',\n",
    "plt.title('Actual vs. Predicted Groundwater Levels (Weekly Means)')\n",
    "plt.xlabel('Week Index')\n",
    "plt.ylabel('Groundwater Level (m AOD)' if target_scaler else 'Standardised GWL')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Set plot limits\n",
    "all_vals = np.concatenate([act_weekly, pred_weekly - diff])\n",
    "y_min = all_vals.min()\n",
    "y_max = all_vals.max()\n",
    "y_range = y_max - y_min\n",
    "plt.ylim(y_min - y_range/2, y_max + y_range/2)\n",
    "\n",
    "# Save next to the daily figure with _weekly\n",
    "# weekly_save_path = save_path + \"_weekly\"\n",
    "# plt.savefig(weekly_save_path, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np, pandas as pd\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# # Reload if needed\n",
    "# scaler_path = \"data/03_graph/eden/scalers/croglin_20250818_180550/target_scaler.pkl\"\n",
    "# target_scaler = joblib.load(scaler_path)\n",
    "# logger.info(f\"Loaded target scaler from: {scaler_path}\")\n",
    "# scale = torch.tensor(target_scaler.scale_, device=device)\n",
    "# mean = torch.tensor(target_scaler.mean_, device=device)\n",
    "\n",
    "\n",
    "# def inv_std(arr_std, scaler):\n",
    "#     arr_std = np.asarray(arr_std).reshape(-1,1)\n",
    "#     if scaler is None:\n",
    "#         return arr_std.ravel()\n",
    "#     return scaler.inverse_transform(arr_std).ravel()\n",
    "\n",
    "# def nse(y, yhat):\n",
    "#     y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "#     den = np.sum((y - y.mean())**2)\n",
    "#     return 1.0 - (np.sum((y - yhat)**2) / den if den > 0 else np.inf)\n",
    "\n",
    "# def kge(y, yhat):\n",
    "#     y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "#     r = np.corrcoef(y, yhat)[0,1] if y.size > 1 else np.nan\n",
    "#     beta = yhat.mean()/y.mean() if y.mean()!=0 else np.nan\n",
    "#     gamma = (yhat.std(ddof=1)/y.std(ddof=1)) if y.std(ddof=1)>0 else np.nan\n",
    "#     return 1 - np.sqrt((r-1)**2 + (beta-1)**2 + (gamma-1)**2), r, beta, gamma\n",
    "\n",
    "# # ---------- Build climatology from TRAINING ONLY ----------\n",
    "# from collections import defaultdict\n",
    "# clim_sum = defaultdict(lambda: np.zeros(366, dtype=float))\n",
    "# clim_cnt = defaultdict(lambda: np.zeros(366, dtype=int))\n",
    "\n",
    "# for data in all_timesteps_list:\n",
    "#     train_mask = data.train_mask\n",
    "#     if train_mask.sum() == 0:\n",
    "#         continue\n",
    "#     doy = int(pd.to_datetime(str(data.timestep)).dayofyear) - 1\n",
    "#     y_std = data.y[train_mask].detach().cpu().numpy().ravel()\n",
    "#     y_raw = inv_std(y_std, target_scaler)\n",
    "#     nids  = data.node_id[train_mask].detach().cpu().numpy().ravel().astype(int)\n",
    "#     for nid, val in zip(nids, y_raw):\n",
    "#         clim_sum[nid][doy] += val\n",
    "#         clim_cnt[nid][doy] += 1\n",
    "\n",
    "# climatology = {}\n",
    "# node_annual = {}\n",
    "# for nid in clim_sum.keys():\n",
    "#     s, c = clim_sum[nid], clim_cnt[nid]\n",
    "#     with np.errstate(divide='ignore', invalid='ignore'):\n",
    "#         m = np.divide(s, np.where(c==0, 1, c))\n",
    "#     annual = s.sum() / max(c.sum(), 1) if c.sum()>0 else 0.0\n",
    "#     m[c==0] = annual\n",
    "#     climatology[nid] = m            # per-node DoY mean (366)\n",
    "#     node_annual[nid] = annual       # per-node annual mean\n",
    "\n",
    "# # Global fallbacks (across all training observations)\n",
    "# all_train_vals = []\n",
    "# for nid in clim_sum.keys():\n",
    "#     s, c = clim_sum[nid], clim_cnt[nid]\n",
    "#     if c.sum() > 0:\n",
    "#         # expand s/c into list of observed values is overkill; use annual means as proxy\n",
    "#         all_train_vals.append(node_annual[nid])\n",
    "# global_mean = float(np.mean(all_train_vals)) if len(all_train_vals) else 0.0\n",
    "# global_doy = np.zeros(366, dtype=float)\n",
    "# # simple global DoY baseline = global mean (you could compute true global DoY means if needed)\n",
    "# global_doy[:] = global_mean\n",
    "\n",
    "# # ---------- Evaluate on TEST timesteps ----------\n",
    "# y_true, y_persist, y_clim = [], [], []\n",
    "# prev_obs = {}  # nid -> last RAW y\n",
    "\n",
    "# for data in all_timesteps_list:\n",
    "#     doy = int(pd.to_datetime(str(data.timestep)).dayofyear) - 1\n",
    "\n",
    "#     # 1) make predictions FIRST using previous observations\n",
    "#     test_mask = data.test_mask\n",
    "#     if test_mask.any():\n",
    "#         y_std = data.y[test_mask].detach().cpu().numpy().ravel()\n",
    "#         y_raw = inv_std(y_std, target_scaler)\n",
    "#         nids  = data.node_id[test_mask].detach().cpu().numpy().ravel().astype(int)\n",
    "\n",
    "#         for nid, yt in zip(nids, y_raw):\n",
    "#             # persistence (t-1), fallback to node climatology, then global day-of-year, then global mean\n",
    "#             yp = prev_obs.get(nid, None)\n",
    "#             if yp is None:\n",
    "#                 yp = climatology.get(nid, global_doy)[doy] if nid in climatology else global_doy[doy]\n",
    "#             yc = climatology.get(nid, global_doy)[doy] if nid in climatology else global_doy[doy]\n",
    "\n",
    "#             y_true.append(yt)\n",
    "#             y_persist.append(yp)\n",
    "#             y_clim.append(yc)\n",
    "\n",
    "#     # 2) AFTER predicting, update prev_obs with *current* observed values\n",
    "#     known_mask = (data.train_mask | data.val_mask | data.test_mask)\n",
    "#     if known_mask.any():\n",
    "#         ys_std = data.y[known_mask].detach().cpu().numpy().ravel()\n",
    "#         ys_raw = inv_std(ys_std, target_scaler)\n",
    "#         nids   = data.node_id[known_mask].detach().cpu().numpy().ravel().astype(int)\n",
    "#         for nid, val in zip(nids, ys_raw):\n",
    "#             prev_obs[nid] = val\n",
    "\n",
    "# # ---------- Metrics ----------\n",
    "# y_true = np.asarray(y_true); y_persist = np.asarray(y_persist); y_clim = np.asarray(y_clim)\n",
    "\n",
    "# # ensure no NaNs remain\n",
    "# for name, arr in [(\"y_true\", y_true), (\"y_persist\", y_persist), (\"y_clim\", y_clim)]:\n",
    "#     if np.isnan(arr).any():\n",
    "#         n = int(np.isnan(arr).sum())\n",
    "#         raise ValueError(f\"{name} contains {n} NaNs after baseline construction.\")\n",
    "\n",
    "# def report(name, yhat):\n",
    "#     mae  = mean_absolute_error(y_true, yhat)\n",
    "#     rmse = np.sqrt(mean_squared_error(y_true, yhat))\n",
    "#     _nse = nse(y_true, yhat)\n",
    "#     _kge, r, beta, gamma = kge(y_true, yhat)\n",
    "#     print(f\"{name}: MAE={mae:.3f} m, RMSE={rmse:.3f} m, NSE={_nse:.3f}, KGE={_kge:.3f} (r={r:.3f}, β={beta:.3f}, γ={gamma:.3f})\")\n",
    "\n",
    "# if y_true.size == 0:\n",
    "#     print(\"No test data found — check test masks.\")\n",
    "# else:\n",
    "#     report(\"Persistence\", y_persist)\n",
    "#     report(\"Seasonal climatology (DoY mean)\", y_clim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assume you already have:\n",
    "# # y_true = np.array([...])   # test targets (unscaled, m AOD)\n",
    "# # y_pred = np.array([...])   # model predictions (unscaled, m AOD)\n",
    "\n",
    "# # --- 1. Hydrograph with persistence baseline ---\n",
    "# y_persist = np.roll(y_true, 1)   # simple lag-1 baseline\n",
    "# y_persist[0] = y_true[0]         # first value no lag\n",
    "\n",
    "# plt.figure(figsize=(12,4))\n",
    "# plt.plot(y_true, label=\"Observed\", lw=1)\n",
    "# plt.plot(test_predictions_final, label=\"Model\", alpha=0.7, lw=1)\n",
    "# plt.plot(y_persist, label=\"Persistence\", alpha=0.7, lw=1)\n",
    "# plt.legend(); plt.title(\"Hydrograph Comparison\")\n",
    "# plt.show()\n",
    "\n",
    "# # --- 2. Autocorrelation (ACF) of observed series ---\n",
    "# from statsmodels.graphics.tsaplots import plot_acf\n",
    "# plot_acf(y_true, lags=50)\n",
    "# plt.title(\"Autocorrelation of Observed GWL\")\n",
    "# plt.show()\n",
    "\n",
    "# # --- 3. Lag plot: y(t) vs y(t-1) ---\n",
    "# plt.figure(figsize=(4,4))\n",
    "# plt.scatter(y_true[:-1], y_true[1:], alpha=0.5)\n",
    "# plt.xlabel(\"y(t-1)\")\n",
    "# plt.ylabel(\"y(t)\")\n",
    "# plt.title(\"Lag-1 Plot (Observed)\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Plot at a weekly resolution (using daily predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ray.tune import ExperimentAnalysis\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ray_tune_dir = \"data/04_model/eden/model/ray_tune_gwl\"\n",
    "\n",
    "if not os.path.isdir(ray_tune_dir):\n",
    "    logger.error(f\"Error: The specified directory does not exist or is not a directory: {ray_tune_dir}\")\n",
    "else:\n",
    "    try:\n",
    "        abs_dir = os.path.abspath(ray_tune_dir)\n",
    "        uri = f\"file://{abs_dir}\"\n",
    "\n",
    "        analysis = ExperimentAnalysis(uri)\n",
    "        df = analysis.dataframe()\n",
    "\n",
    "        # Coerce numeric metrics (in case anything logged as strings)\n",
    "        for col in ['val_loss', 'train_loss', 'epoch']:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # Keep only rows with a valid trial_id\n",
    "        if 'trial_id' not in df.columns:\n",
    "            raise KeyError(\"Expected 'trial_id' column not found in Ray Tune results DataFrame.\")\n",
    "        df = df[df['trial_id'].notna()].copy()\n",
    "\n",
    "        # filter out errored trials (errored due to timeouts)\n",
    "        if 'error' in df.columns:\n",
    "            df = df[df['error'].isna()].copy()\n",
    "\n",
    "        logger.info(f\"Total rows (iterations) loaded: {len(df)}\")\n",
    "        n_trials = df['trial_id'].nunique()\n",
    "        logger.info(f\"Trials represented: {n_trials}\")\n",
    "\n",
    "        # Compute per-trial aggregates\n",
    "        config_cols = [c for c in df.columns if c.startswith('config/')]\n",
    "        keep_cols = ['trial_id', 'logdir'] + config_cols\n",
    "        meta_first = (df[keep_cols]\n",
    "                      .sort_values(['trial_id']) \n",
    "                      .groupby('trial_id', as_index=False)\n",
    "                      .first())\n",
    "        \n",
    "        # Trials that never logged val_loss/train_los (timed out) become NaN -> drop for ranking\n",
    "        agg = (df.groupby('trial_id')\n",
    "                 .agg(min_val_loss=('val_loss', 'min'),\n",
    "                      mean_val_loss=('val_loss', 'mean'),\n",
    "                      min_train_loss=('train_loss', 'min'),\n",
    "                      mean_train_loss=('train_loss', 'mean'),\n",
    "                      last_epoch=('epoch', 'max'),\n",
    "                      last_iter=('training_iteration', 'max'))\n",
    "                 .reset_index())\n",
    "\n",
    "        # Merge configs back in\n",
    "        per_trial = meta_first.merge(agg, on='trial_id', how='left')\n",
    "\n",
    "        # Keep only trials with at least some val_loss signal\n",
    "        ranked = per_trial[per_trial['min_val_loss'].notna()].copy()\n",
    "        if ranked.empty:\n",
    "            raise RuntimeError(\"No trials have non-NaN 'val_loss'. Cannot rank.\")\n",
    "\n",
    "        ranked = ranked.sort_values(['min_val_loss', 'mean_val_loss'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "        best = ranked.iloc[0]\n",
    "        trial_id = best['trial_id']\n",
    "\n",
    "        # Pack nice dicts for logging\n",
    "        best_config = {k: best[k] for k in config_cols if k in ranked.columns}\n",
    "        best_metrics = {k: best[k] for k in ['min_val_loss', 'mean_val_loss', 'min_train_loss', 'mean_train_loss',\n",
    "                                             'last_epoch', 'last_iter'] if k in ranked.columns}\n",
    "\n",
    "        logger.info(\"\\n--- Best Trial (by min_val_loss, then mean_val_loss) ---\")\n",
    "        logger.info(f\"Trial ID: {trial_id}\")\n",
    "        logger.info(f\"Best metrics: {best_metrics}\")\n",
    "        logger.info(f\"Best hyperparameters: {best_config}\")\n",
    "\n",
    "        # Locate the corresponding PT model for this trial_id\n",
    "        pt_root = os.path.abspath(\"data/04_model/eden/model/pt_model\")\n",
    "        pattern = os.path.join(pt_root, f\"trial_{trial_id}\", \"pt_model\", \"*.pt\")\n",
    "        pt_candidates = sorted(glob.glob(pattern))\n",
    "        if pt_candidates:\n",
    "            best_pt_model_path = pt_candidates[-1]  # choose last\n",
    "            logger.info(f\"Best trial PT model: {best_pt_model_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"No .pt model files found for trial_id={trial_id} with pattern: {pattern}\")\n",
    "\n",
    "        # Show a compact table of the top 10 trials\n",
    "        display_cols = (['trial_id'] + config_cols +\n",
    "                        ['min_val_loss', 'mean_val_loss', 'min_train_loss', 'mean_train_loss', 'last_epoch'])\n",
    "        display_cols = [c for c in display_cols if c in ranked.columns]\n",
    "        top10 = ranked[display_cols].head(10)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', 160)\n",
    "        print(\"\\nTop 10 trials by validation loss:\")\n",
    "        print(top10.to_string(index=False))\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during analysis: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
