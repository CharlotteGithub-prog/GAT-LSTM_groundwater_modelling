{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f7bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library imports\n",
    "import sys\n",
    "import joblib\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Load project Imports\n",
    "from src.utils.config_loader import load_project_config\n",
    "from src.data_ingestion.gwl_data_ingestion import process_station_coordinates, \\\n",
    "    fetch_and_process_station_data, download_and_save_station_readings\n",
    "from src.preprocessing.gwl_preprocessing import load_timeseries_to_dict, outlier_detection, \\\n",
    "    resample_daily_average, remove_spurious_data, interpolate_short_gaps, handle_large_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0057d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Loading configuration from: /Users/charlotte/Desktop/Dissertation_Code/config/project_config.yaml\n",
      "INFO - Notebook Demo Catchment: eden\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up logging config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "   format='%(levelname)s - %(message)s',\n",
    "#    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Set up logger for file and load config file for paths and params\n",
    "logger = logging.getLogger(__name__)\n",
    "config = load_project_config(config_path=\"config/project_config.yaml\")\n",
    "notebook = False\n",
    "\n",
    "# Define notebook demo catchment\n",
    "catchments_to_process = config[\"global\"][\"pipeline_settings\"][\"catchments_to_process\"]\n",
    "catchment = catchments_to_process[0]\n",
    "run_defra_API_calls = config[\"global\"][\"pipeline_settings\"][\"run_defra_api\"]  # True to run API calls\n",
    "\n",
    "logging.info(f\"Notebook Demo Catchment: {catchment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d049b3",
   "metadata": {},
   "source": [
    "### DATA INGESTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1e6a9",
   "metadata": {},
   "source": [
    "Load gwl station list with grid references and convert grid references to easting, northing, longitude and latitude form for plotting and data alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a6111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - [eden] Starting coordinate processing for station list...\n",
      "\n",
      "INFO - [eden] Loaded OS grid squares lookup from: data/01_raw/global/os_grid_squares.csv\n",
      "INFO - [eden] Loaded station list from: data/01_raw/eden/gwl_stations/station_list.csv\n",
      "INFO - [eden] Converted OS grid references to coordinates for 17 stations.\n",
      "\n",
      "INFO - [eden] Saved processed station list to: data/02_processed/eden/gwl_station_data/station_list_with_coords.csv\n",
      "INFO - Station location reference table head:\n",
      "\n",
      "  station_id    station_name    grid_ref   easting  northing        lat  \\\n",
      "0     NY36_2        longtown  NY39146790  339140.0  567900.0  55.001931   \n",
      "1     NY46_3         scaleby  NY46526426  346520.0  564260.0  54.970074   \n",
      "2    NY55_71  castle_carrock  NY53825337  353820.0  553370.0  54.872953   \n",
      "3    NY54_54         croglin  NY56354839  356350.0  548390.0  54.828432   \n",
      "4    NY54_10       ainstable  NY52574645  352570.0  546450.0  54.810655   \n",
      "\n",
      "        lon  \n",
      "0 -2.952971  \n",
      "1 -2.836931  \n",
      "2 -2.721158  \n",
      "3 -2.680981  \n",
      "4 -2.739497  \n",
      "\n",
      "INFO - Total Stations: 17\n",
      "INFO - [eden] Coordinate processing for station list complete.\n",
      "\n",
      "INFO - Pipeline step 'Process Station Coordinates for eden' complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Process Catchment Stations List ----\n",
    "stations_with_coords_df = process_station_coordinates(\n",
    "    os_grid_squares=config[\"global\"][\"paths\"][\"gis_os_grid_squares\"],\n",
    "    station_list_input=config[catchment][\"paths\"][\"gwl_station_list\"],\n",
    "    station_list_output=config[catchment][\"paths\"][\"gwl_station_list_with_coords\"],\n",
    "    catchment=catchment\n",
    ")\n",
    "\n",
    "logger.info(f\"Pipeline step 'Process Station Coordinates for {catchment}' complete.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98618322",
   "metadata": {},
   "source": [
    "**API Documentation notes:**\n",
    "\n",
    "1. The API calls that return readings data have a soft limit of 100,000 rows per-call which can be overridden by setting a _limit parameter. There is a hard limit of 2,000,000 rows, which cannot be overridden.\n",
    "2. The primary identifier for most stations uses a GUID style identifier called an SUID. These are used in the URL for the station and given as the value of the notation property in the station metadata.  \n",
    "    a. Wiski identifier (wiskiID) is also available for my subset of stations and data type  \n",
    "3. All monitoring stations can be filtered by name, location and other parameters. See https://environment.data.gov.uk/hydrology/doc/reference#stations-summary for full metadata details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3a28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_defra_API_calls:\n",
    "    # Retrieve gwl monitoring station metadata and measures from DEFRA API\n",
    "    stations_with_metadata_measures = fetch_and_process_station_data(\n",
    "        stations_df=stations_with_coords_df,\n",
    "        base_url=config[\"global\"][\"paths\"][\"defra_station_base_url\"],\n",
    "        output_path=config[catchment][\"paths\"][\"gwl_station_metadata_measures\"]\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Pipeline step 'Pull Hydrological Station Metadata for {catchment}' complete.\\n\")\n",
    "\n",
    "    stations_with_metadata_measures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8d66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_defra_API_calls:\n",
    "    download_and_save_station_readings(\n",
    "        stations_df=stations_with_metadata_measures,\n",
    "        start_date=config[\"global\"][\"data_ingestion\"][\"api_start_date\"],\n",
    "        end_date=config[\"global\"][\"data_ingestion\"][\"api_end_date\"],\n",
    "        gwl_data_output_dir=config[catchment][\"paths\"][\"gwl_data_output_dir\"]\n",
    "    )\n",
    "\n",
    "    logger.info(f\"All timeseries groundwater level data saved for {catchment} catchment.\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    loaded_csv_path = config[catchment][\"paths\"][\"gwl_station_metadata_measures\"]\n",
    "    stations_with_metadata_measures = pd.read_csv(loaded_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f7ecc",
   "metadata": {},
   "source": [
    "### PREPROCESSING ###\n",
    "\n",
    "Remove stations with insufficient data and clean ts data from outliers and incorrect measurements. Interpolate between small data gaps using rational spline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0968cf",
   "metadata": {},
   "source": [
    "1. Load station df's into dict, dropping catchments with insufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deef6250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Converting API csv data to reference dict...\n",
      "\n",
      "INFO - Longtown successfully saved to dict.\n",
      "INFO - Scaleby successfully saved to dict.\n",
      "INFO - Castle_Carrock successfully saved to dict.\n",
      "INFO - Croglin successfully saved to dict.\n",
      "INFO - Ainstable successfully saved to dict.\n",
      "INFO - Baronwood successfully saved to dict.\n",
      "INFO - Renwick successfully saved to dict.\n",
      "INFO - East_Brownrigg successfully saved to dict.\n",
      "INFO - Bgs_Ev2 successfully saved to dict.\n",
      "INFO - Station Penrith_North contained insufficient data -> dropping dataframe.(40 < 5000)\n",
      "INFO - Skirwith successfully saved to dict.\n",
      "INFO - Cliburn_Town_Bridge_1 successfully saved to dict.\n",
      "INFO - Cliburn_Town_Bridge_2 successfully saved to dict.\n",
      "INFO - Hilton successfully saved to dict.\n",
      "INFO - Coupland successfully saved to dict.\n",
      "INFO - Great_Musgrave successfully saved to dict.\n",
      "INFO - East_Curthwaite successfully saved to dict.\n",
      "INFO - 16 stations saved to dict.\n",
      "\n",
      "INFO - All timeseries data converted to dict for eden catchment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load timeseries CSVs from API into reference dict\n",
    "gwl_time_series_dict = load_timeseries_to_dict(\n",
    "    stations_df=stations_with_metadata_measures,\n",
    "    col_order=config[\"global\"][\"data_ingestion\"][\"col_order\"],\n",
    "    data_dir=config[catchment][\"paths\"][\"gwl_data_output_dir\"],\n",
    "    inclusion_threshold=config[catchment][\"preprocessing\"][\"inclusion_threshold\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"All timeseries data converted to dict for {catchment} catchment.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee5321",
   "metadata": {},
   "source": [
    "2. Remove outlying and incorrect data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5a296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Station Ainstable: Shifted values by +0.4654 between 2021-06-16 and 2021-11-17.\n",
      "INFO - Station Ainstable: Inverted segment between 2022-08-01 and 2023-04-18 around mean = 84.7347\n",
      "INFO - Station Ainstable: Shifted values by +-0.5928 between 2022-08-01 and 2023-04-18.\n",
      "Removed data for range: 2021-06-13 to 2021-06-17\n",
      "Removed data for range: 2021-11-15 to 2021-11-19\n",
      "Removed data for range: 2023-04-16 to 2023-04-20\n",
      "INFO - Ainstable time series data in daily timestep saved to results/figures/eden/time_series/Ainstable_aggregated_daily.png.\n",
      "\n",
      "INFO - Station Renwick: Removed 6961 data points between 2023-01-06 and 2023-10-23.\n"
     ]
    }
   ],
   "source": [
    "for station_name, df in gwl_time_series_dict.items():\n",
    "    gwl_time_series_dict[station_name] = remove_spurious_data(\n",
    "        target_df=df,\n",
    "        station_name=station_name,\n",
    "        path=config[catchment][\"visualisations\"][\"ts_plots\"][\"time_series_gwl_output\"],\n",
    "        notebook=notebook\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_outlier_processing = False\n",
    "\n",
    "if run_outlier_processing:\n",
    "    # run outlier detection and processing\n",
    "    processed_gwl_time_series_dict = outlier_detection(\n",
    "        gwl_time_series_dict=gwl_time_series_dict,\n",
    "        output_path=config[catchment][\"visualisations\"][\"ts_plots\"][\"time_series_gwl_output\"],\n",
    "        dpi=config[catchment][\"visualisations\"][\"ts_plots\"][\"dpi_save\"],\n",
    "        dict_output=config[catchment][\"paths\"][\"gwl_outlier_dict\"],\n",
    "        notebook=notebook\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948a6c5",
   "metadata": {},
   "source": [
    "3. Aggregate to daily time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f90e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_outlier_processing:\n",
    "    input_dict = config[catchment][\"paths\"][\"gwl_outlier_dict\"]\n",
    "    processed_gwl_time_series_dict = joblib.load(input_dict)\n",
    "\n",
    "daily_data = resample_daily_average(\n",
    "    dict=processed_gwl_time_series_dict,\n",
    "    start_date=config[\"global\"][\"data_ingestion\"][\"api_start_date\"],\n",
    "    end_date=config[\"global\"][\"data_ingestion\"][\"api_end_date\"],\n",
    "    path=config[catchment][\"visualisations\"][\"ts_plots\"][\"time_series_gwl_output\"],\n",
    "    notebook=notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be08af9",
   "metadata": {},
   "source": [
    "4. Interpolate across small gaps in the ts data using rational spline or PCHIP - try both (& define threshold n/o missing time steps for interpolation eligibility) + Add binary interpolation flag column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76edee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_list = []\n",
    "\n",
    "for station_name, df in daily_data.items():\n",
    "    gaps, daily_data[station_name] = interpolate_short_gaps(\n",
    "        df=df,\n",
    "        station_name=station_name,\n",
    "        path=config[catchment][\"visualisations\"][\"ts_plots\"][\"time_series_gwl_output\"],\n",
    "        max_steps=config[\"global\"][\"data_ingestion\"][\"max_interp_length\"],\n",
    "        notebook=notebook\n",
    "    )\n",
    "    \n",
    "    if gaps:\n",
    "        gaps_list.append(station_name)\n",
    "        \n",
    "logging.info(f\"Stations still needing interpolation: {gaps_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ad895",
   "metadata": {},
   "source": [
    "Handle large gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_large_gaps(\n",
    "    df=daily_data,\n",
    "    gaps_list=gaps_list,\n",
    "    catchment=catchment,\n",
    "    spatial_path=config[catchment][\"paths\"][\"gwl_station_list_with_coords\"],\n",
    "    path=config[catchment][\"visualisations\"][\"ts_plots\"][\"time_series_gwl_output\"],\n",
    "    threshold_m=config[catchment][\"preprocessing\"][\"large_catchment_threshold_m\"],\n",
    "    radius=config[\"global\"][\"preprocessing\"][\"radius\"],\n",
    "    notebook=notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f6579",
   "metadata": {},
   "source": [
    "5. Lagged: Add lagged features (by timestep across 7 days?) + potentially rolling averages (3-day/7-day?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702346e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eceea5eb",
   "metadata": {},
   "source": [
    "6. Temporal Encoding: Define sinasoidal features for seasonality (both sine and cosine for performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71caa59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5eccabf",
   "metadata": {},
   "source": [
    "To zoom in on an area of a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.dates as mdates\n",
    "# import pandas as pd\n",
    "\n",
    "# start_date = '2022-12-01'\n",
    "# end_date = '2023-12-31'\n",
    "\n",
    "# # Filter the DataFrame\n",
    "# df_renwick = daily_data['Ainstable']\n",
    "# df_renwick['dateTime'] = pd.to_datetime(df_renwick['dateTime'], errors='coerce')\n",
    "# filtered_df_renwick = df_renwick[(df_renwick['dateTime'] >= start_date) & (df_renwick['dateTime'] <= end_date)]\n",
    "\n",
    "# # print(filtered_df_renwick.head(20))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 6)) # Use fig, ax for more control\n",
    "\n",
    "# ax.plot(filtered_df_renwick['dateTime'], filtered_df_renwick['value'])\n",
    "# ax.set_title(f\"Renwick Groundwater Level: {start_date} to {end_date}\")\n",
    "# ax.set_xlabel(\"Date\")\n",
    "# ax.set_ylabel(\"Groundwater Level (mAOD)\")\n",
    "\n",
    "# ax.xaxis.set_minor_locator(mdates.WeekdayLocator(interval=1))\n",
    "# ax.yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
    "# ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n",
    "# ax.grid(True, which='minor', linestyle=':', linewidth=0.2) # Finer, dashed minor grid\n",
    "\n",
    "\n",
    "# fig.autofmt_xdate() # Auto-formats date labels for readability\n",
    "# plt.show()\n",
    "\n",
    "# # print(filtered_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
